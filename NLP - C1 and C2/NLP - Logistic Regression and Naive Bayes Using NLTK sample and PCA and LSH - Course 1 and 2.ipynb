{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import getcwd\n",
    "\n",
    "import pdb\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "nltk.data.path.append('.')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='svg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /Users/diegobatalhacunhadasilva/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/diegobatalhacunhadasilva/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "#split the data\n",
    "train_pos=all_positive_tweets[:4000]\n",
    "test_pos=all_positive_tweets[4000:] #from 4000-5000\n",
    "train_neg=all_negative_tweets[:4000]\n",
    "test_neg=all_negative_tweets[4000:] #from 4000-5000\n",
    "\n",
    "train_x=train_pos+train_neg\n",
    "test_x=test_pos+test_neg\n",
    "\n",
    "train_y=np.append(np.ones((len(train_pos),1)),np.zeros((len(train_neg),1)),axis=0)\n",
    "test_y=np.append(np.ones((len(test_pos),1)),np.zeros((len(test_neg),1)),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    stemmer=PorterStemmer()\n",
    "    stopwords_english=stopwords.words('english')\n",
    "    \n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    \n",
    "    tokenizer=TweetTokenizer(preserve_case=False,strip_handles=True,\n",
    "                             reduce_len=True)\n",
    "    \n",
    "    tweet_tokens=tokenizer.tokenize(tweet)\n",
    "    \n",
    "    tweets_clean=[]\n",
    "    # remove stopwords\n",
    "    # remove punctuation\n",
    "    for word in tweet_tokens:\n",
    "        if word not in stopwords_english and\\\n",
    "            word not in string.punctuation:\n",
    "            stem_word=stemmer.stem(word)\n",
    "            tweets_clean.append(stem_word)\n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_freqs(tweets,ys):\n",
    "    yslist = np.squeeze(ys).tolist()\n",
    "    freqs={}\n",
    "    for y,tweet in zip(yslist,tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            pair=(word,y)\n",
    "            \n",
    "            if pair in freqs:\n",
    "                freqs[pair]+=1\n",
    "            else:\n",
    "                freqs[pair]=1\n",
    "                \n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('followfriday', 1.0): 23,\n",
       " ('top', 1.0): 30,\n",
       " ('engag', 1.0): 7,\n",
       " ('member', 1.0): 14,\n",
       " ('commun', 1.0): 27,\n",
       " ('week', 1.0): 72,\n",
       " (':)', 1.0): 2847,\n",
       " ('hey', 1.0): 60,\n",
       " ('jame', 1.0): 7,\n",
       " ('odd', 1.0): 2,\n",
       " (':/', 1.0): 5,\n",
       " ('pleas', 1.0): 80,\n",
       " ('call', 1.0): 27,\n",
       " ('contact', 1.0): 4,\n",
       " ('centr', 1.0): 1,\n",
       " ('02392441234', 1.0): 1,\n",
       " ('abl', 1.0): 6,\n",
       " ('assist', 1.0): 1,\n",
       " ('mani', 1.0): 28,\n",
       " ('thank', 1.0): 504,\n",
       " ('listen', 1.0): 14,\n",
       " ('last', 1.0): 39,\n",
       " ('night', 1.0): 55,\n",
       " ('bleed', 1.0): 2,\n",
       " ('amaz', 1.0): 41,\n",
       " ('track', 1.0): 5,\n",
       " ('scotland', 1.0): 2,\n",
       " ('congrat', 1.0): 15,\n",
       " ('yeaaah', 1.0): 1,\n",
       " ('yipppi', 1.0): 1,\n",
       " ('accnt', 1.0): 2,\n",
       " ('verifi', 1.0): 2,\n",
       " ('rqst', 1.0): 1,\n",
       " ('succeed', 1.0): 1,\n",
       " ('got', 1.0): 57,\n",
       " ('blue', 1.0): 8,\n",
       " ('tick', 1.0): 1,\n",
       " ('mark', 1.0): 1,\n",
       " ('fb', 1.0): 4,\n",
       " ('profil', 1.0): 2,\n",
       " ('15', 1.0): 4,\n",
       " ('day', 1.0): 187,\n",
       " ('one', 1.0): 90,\n",
       " ('irresist', 1.0): 2,\n",
       " ('flipkartfashionfriday', 1.0): 16,\n",
       " ('like', 1.0): 187,\n",
       " ('keep', 1.0): 55,\n",
       " ('love', 1.0): 336,\n",
       " ('custom', 1.0): 4,\n",
       " ('wait', 1.0): 55,\n",
       " ('long', 1.0): 27,\n",
       " ('hope', 1.0): 113,\n",
       " ('enjoy', 1.0): 57,\n",
       " ('happi', 1.0): 161,\n",
       " ('friday', 1.0): 91,\n",
       " ('lwwf', 1.0): 1,\n",
       " ('second', 1.0): 8,\n",
       " ('thought', 1.0): 21,\n",
       " ('’', 1.0): 17,\n",
       " ('enough', 1.0): 16,\n",
       " ('time', 1.0): 100,\n",
       " ('dd', 1.0): 1,\n",
       " ('new', 1.0): 111,\n",
       " ('short', 1.0): 6,\n",
       " ('enter', 1.0): 9,\n",
       " ('system', 1.0): 2,\n",
       " ('sheep', 1.0): 1,\n",
       " ('must', 1.0): 14,\n",
       " ('buy', 1.0): 9,\n",
       " ('jgh', 1.0): 4,\n",
       " ('go', 1.0): 120,\n",
       " ('bayan', 1.0): 1,\n",
       " (':D', 1.0): 498,\n",
       " ('bye', 1.0): 5,\n",
       " ('act', 1.0): 6,\n",
       " ('mischiev', 1.0): 1,\n",
       " ('etl', 1.0): 1,\n",
       " ('layer', 1.0): 1,\n",
       " ('in-hous', 1.0): 1,\n",
       " ('wareh', 1.0): 1,\n",
       " ('app', 1.0): 12,\n",
       " ('katamari', 1.0): 1,\n",
       " ('well', 1.0): 66,\n",
       " ('…', 1.0): 31,\n",
       " ('name', 1.0): 12,\n",
       " ('impli', 1.0): 1,\n",
       " (':p', 1.0): 103,\n",
       " ('influenc', 1.0): 16,\n",
       " ('big', 1.0): 27,\n",
       " ('...', 1.0): 227,\n",
       " ('juici', 1.0): 3,\n",
       " ('selfi', 1.0): 11,\n",
       " ('follow', 1.0): 319,\n",
       " ('perfect', 1.0): 17,\n",
       " ('alreadi', 1.0): 19,\n",
       " ('know', 1.0): 120,\n",
       " (\"what'\", 1.0): 14,\n",
       " ('great', 1.0): 134,\n",
       " ('opportun', 1.0): 17,\n",
       " ('junior', 1.0): 2,\n",
       " ('triathlet', 1.0): 1,\n",
       " ('age', 1.0): 2,\n",
       " ('12', 1.0): 5,\n",
       " ('13', 1.0): 5,\n",
       " ('gatorad', 1.0): 1,\n",
       " ('seri', 1.0): 4,\n",
       " ('get', 1.0): 164,\n",
       " ('entri', 1.0): 3,\n",
       " ('lay', 1.0): 3,\n",
       " ('greet', 1.0): 4,\n",
       " ('card', 1.0): 6,\n",
       " ('rang', 1.0): 2,\n",
       " ('print', 1.0): 3,\n",
       " ('today', 1.0): 86,\n",
       " ('job', 1.0): 34,\n",
       " (':-)', 1.0): 543,\n",
       " (\"friend'\", 1.0): 3,\n",
       " ('lunch', 1.0): 3,\n",
       " ('yummm', 1.0): 1,\n",
       " ('nostalgia', 1.0): 1,\n",
       " ('tb', 1.0): 1,\n",
       " ('ku', 1.0): 1,\n",
       " ('id', 1.0): 8,\n",
       " ('conflict', 1.0): 1,\n",
       " ('help', 1.0): 37,\n",
       " (\"here'\", 1.0): 20,\n",
       " ('screenshot', 1.0): 2,\n",
       " ('work', 1.0): 88,\n",
       " ('hi', 1.0): 154,\n",
       " ('liv', 1.0): 2,\n",
       " ('hello', 1.0): 49,\n",
       " ('need', 1.0): 62,\n",
       " ('someth', 1.0): 25,\n",
       " ('u', 1.0): 136,\n",
       " ('fm', 1.0): 2,\n",
       " ('twitter', 1.0): 25,\n",
       " ('—', 1.0): 22,\n",
       " ('sure', 1.0): 37,\n",
       " ('thing', 1.0): 48,\n",
       " ('dm', 1.0): 34,\n",
       " ('x', 1.0): 50,\n",
       " (\"i'v\", 1.0): 25,\n",
       " ('heard', 1.0): 9,\n",
       " ('four', 1.0): 5,\n",
       " ('season', 1.0): 5,\n",
       " ('pretti', 1.0): 17,\n",
       " ('dope', 1.0): 2,\n",
       " ('penthous', 1.0): 1,\n",
       " ('obv', 1.0): 1,\n",
       " ('gobigorgohom', 1.0): 1,\n",
       " ('fun', 1.0): 45,\n",
       " (\"y'all\", 1.0): 3,\n",
       " ('yeah', 1.0): 30,\n",
       " ('suppos', 1.0): 6,\n",
       " ('lol', 1.0): 48,\n",
       " ('chat', 1.0): 9,\n",
       " ('bit', 1.0): 16,\n",
       " ('youth', 1.0): 14,\n",
       " ('💅', 1.0): 1,\n",
       " ('🏽', 1.0): 1,\n",
       " ('💋', 1.0): 2,\n",
       " ('seen', 1.0): 6,\n",
       " ('year', 1.0): 33,\n",
       " ('rest', 1.0): 9,\n",
       " ('goe', 1.0): 4,\n",
       " ('quickli', 1.0): 3,\n",
       " ('bed', 1.0): 8,\n",
       " ('music', 1.0): 15,\n",
       " ('fix', 1.0): 6,\n",
       " ('dream', 1.0): 17,\n",
       " ('spiritu', 1.0): 1,\n",
       " ('ritual', 1.0): 1,\n",
       " ('festiv', 1.0): 7,\n",
       " ('népal', 1.0): 1,\n",
       " ('begin', 1.0): 4,\n",
       " ('line-up', 1.0): 4,\n",
       " ('left', 1.0): 10,\n",
       " ('see', 1.0): 156,\n",
       " ('sarah', 1.0): 4,\n",
       " ('send', 1.0): 17,\n",
       " ('us', 1.0): 91,\n",
       " ('email', 1.0): 22,\n",
       " ('bitsy@bitdefender.com', 1.0): 1,\n",
       " (\"we'll\", 1.0): 12,\n",
       " ('asap', 1.0): 5,\n",
       " ('kik', 1.0): 16,\n",
       " ('hatessuc', 1.0): 1,\n",
       " ('32429', 1.0): 1,\n",
       " ('kikm', 1.0): 1,\n",
       " ('lgbt', 1.0): 2,\n",
       " ('tinder', 1.0): 1,\n",
       " ('nsfw', 1.0): 1,\n",
       " ('akua', 1.0): 1,\n",
       " ('cumshot', 1.0): 1,\n",
       " ('come', 1.0): 63,\n",
       " ('hous', 1.0): 5,\n",
       " ('nsn_supplement', 1.0): 1,\n",
       " ('effect', 1.0): 2,\n",
       " ('press', 1.0): 1,\n",
       " ('releas', 1.0): 11,\n",
       " ('distribut', 1.0): 1,\n",
       " ('result', 1.0): 2,\n",
       " ('link', 1.0): 13,\n",
       " ('remov', 1.0): 3,\n",
       " ('pressreleas', 1.0): 1,\n",
       " ('newsdistribut', 1.0): 1,\n",
       " ('bam', 1.0): 44,\n",
       " ('bestfriend', 1.0): 50,\n",
       " ('lot', 1.0): 80,\n",
       " ('warsaw', 1.0): 44,\n",
       " ('<3', 1.0): 118,\n",
       " ('x46', 1.0): 1,\n",
       " ('everyon', 1.0): 45,\n",
       " ('watch', 1.0): 32,\n",
       " ('documentari', 1.0): 1,\n",
       " ('earthl', 1.0): 1,\n",
       " ('youtub', 1.0): 8,\n",
       " ('support', 1.0): 25,\n",
       " ('buuut', 1.0): 1,\n",
       " ('oh', 1.0): 44,\n",
       " ('look', 1.0): 109,\n",
       " ('forward', 1.0): 20,\n",
       " ('visit', 1.0): 25,\n",
       " ('next', 1.0): 37,\n",
       " ('letsgetmessi', 1.0): 1,\n",
       " ('jo', 1.0): 1,\n",
       " ('make', 1.0): 69,\n",
       " ('feel', 1.0): 33,\n",
       " ('better', 1.0): 40,\n",
       " ('never', 1.0): 31,\n",
       " ('anyon', 1.0): 7,\n",
       " ('kpop', 1.0): 1,\n",
       " ('flesh', 1.0): 1,\n",
       " ('good', 1.0): 191,\n",
       " ('girl', 1.0): 34,\n",
       " ('best', 1.0): 49,\n",
       " ('wish', 1.0): 29,\n",
       " ('reason', 1.0): 10,\n",
       " ('epic', 1.0): 1,\n",
       " ('soundtrack', 1.0): 1,\n",
       " ('shout', 1.0): 11,\n",
       " ('ad', 1.0): 10,\n",
       " ('video', 1.0): 29,\n",
       " ('playlist', 1.0): 5,\n",
       " ('would', 1.0): 70,\n",
       " ('dear', 1.0): 15,\n",
       " ('jordan', 1.0): 1,\n",
       " ('okay', 1.0): 31,\n",
       " ('fake', 1.0): 1,\n",
       " ('gameplay', 1.0): 1,\n",
       " (';)', 1.0): 22,\n",
       " ('haha', 1.0): 44,\n",
       " ('im', 1.0): 38,\n",
       " ('kid', 1.0): 13,\n",
       " ('stuff', 1.0): 11,\n",
       " ('exactli', 1.0): 5,\n",
       " ('product', 1.0): 11,\n",
       " ('line', 1.0): 6,\n",
       " ('etsi', 1.0): 1,\n",
       " ('shop', 1.0): 12,\n",
       " ('check', 1.0): 38,\n",
       " ('vacat', 1.0): 5,\n",
       " ('recharg', 1.0): 1,\n",
       " ('normal', 1.0): 5,\n",
       " ('charger', 1.0): 2,\n",
       " ('asleep', 1.0): 7,\n",
       " ('talk', 1.0): 37,\n",
       " ('sooo', 1.0): 6,\n",
       " ('someon', 1.0): 29,\n",
       " ('text', 1.0): 12,\n",
       " ('ye', 1.0): 60,\n",
       " ('bet', 1.0): 6,\n",
       " (\"he'll\", 1.0): 2,\n",
       " ('fit', 1.0): 2,\n",
       " ('hear', 1.0): 24,\n",
       " ('speech', 1.0): 1,\n",
       " ('piti', 1.0): 2,\n",
       " ('green', 1.0): 2,\n",
       " ('garden', 1.0): 5,\n",
       " ('midnight', 1.0): 1,\n",
       " ('sun', 1.0): 6,\n",
       " ('beauti', 1.0): 45,\n",
       " ('canal', 1.0): 1,\n",
       " ('dasvidaniya', 1.0): 1,\n",
       " ('till', 1.0): 16,\n",
       " ('scout', 1.0): 1,\n",
       " ('sg', 1.0): 1,\n",
       " ('futur', 1.0): 9,\n",
       " ('wlan', 1.0): 1,\n",
       " ('pro', 1.0): 4,\n",
       " ('confer', 1.0): 1,\n",
       " ('asia', 1.0): 1,\n",
       " ('chang', 1.0): 20,\n",
       " ('lollipop', 1.0): 1,\n",
       " ('🍭', 1.0): 1,\n",
       " ('nez', 1.0): 1,\n",
       " ('agnezmo', 1.0): 1,\n",
       " ('oley', 1.0): 1,\n",
       " ('mama', 1.0): 1,\n",
       " ('stand', 1.0): 6,\n",
       " ('stronger', 1.0): 1,\n",
       " ('god', 1.0): 14,\n",
       " ('misti', 1.0): 1,\n",
       " ('babi', 1.0): 17,\n",
       " ('cute', 1.0): 21,\n",
       " ('woohoo', 1.0): 3,\n",
       " (\"can't\", 1.0): 31,\n",
       " ('sign', 1.0): 9,\n",
       " ('yet', 1.0): 12,\n",
       " ('still', 1.0): 37,\n",
       " ('think', 1.0): 48,\n",
       " ('mka', 1.0): 5,\n",
       " ('liam', 1.0): 5,\n",
       " ('access', 1.0): 3,\n",
       " ('welcom', 1.0): 54,\n",
       " ('stat', 1.0): 51,\n",
       " ('arriv', 1.0): 57,\n",
       " ('1', 1.0): 60,\n",
       " ('unfollow', 1.0): 53,\n",
       " ('via', 1.0): 60,\n",
       " ('surpris', 1.0): 10,\n",
       " ('figur', 1.0): 5,\n",
       " ('happybirthdayemilybett', 1.0): 1,\n",
       " ('sweet', 1.0): 16,\n",
       " ('talent', 1.0): 4,\n",
       " ('2', 1.0): 41,\n",
       " ('plan', 1.0): 21,\n",
       " ('drain', 1.0): 1,\n",
       " ('gotta', 1.0): 4,\n",
       " ('timezon', 1.0): 1,\n",
       " ('parent', 1.0): 4,\n",
       " ('proud', 1.0): 11,\n",
       " ('least', 1.0): 14,\n",
       " ('mayb', 1.0): 17,\n",
       " ('sometim', 1.0): 11,\n",
       " ('grade', 1.0): 4,\n",
       " ('al', 1.0): 3,\n",
       " ('grand', 1.0): 4,\n",
       " ('manila_bro', 1.0): 1,\n",
       " ('chosen', 1.0): 1,\n",
       " ('let', 1.0): 57,\n",
       " ('around', 1.0): 14,\n",
       " ('..', 1.0): 100,\n",
       " ('side', 1.0): 13,\n",
       " ('world', 1.0): 23,\n",
       " ('eh', 1.0): 2,\n",
       " ('take', 1.0): 30,\n",
       " ('care', 1.0): 12,\n",
       " ('final', 1.0): 24,\n",
       " ('fuck', 1.0): 20,\n",
       " ('weekend', 1.0): 61,\n",
       " ('real', 1.0): 18,\n",
       " ('x45', 1.0): 1,\n",
       " ('join', 1.0): 21,\n",
       " ('hushedcallwithfraydo', 1.0): 1,\n",
       " ('gift', 1.0): 7,\n",
       " ('yeahhh', 1.0): 1,\n",
       " ('hushedpinwithsammi', 1.0): 2,\n",
       " ('event', 1.0): 7,\n",
       " ('might', 1.0): 21,\n",
       " ('luv', 1.0): 4,\n",
       " ('realli', 1.0): 66,\n",
       " ('appreci', 1.0): 28,\n",
       " ('share', 1.0): 41,\n",
       " ('wow', 1.0): 14,\n",
       " ('tom', 1.0): 5,\n",
       " ('gym', 1.0): 3,\n",
       " ('monday', 1.0): 7,\n",
       " ('invit', 1.0): 15,\n",
       " ('scope', 1.0): 5,\n",
       " ('friend', 1.0): 40,\n",
       " ('nude', 1.0): 1,\n",
       " ('sleep', 1.0): 35,\n",
       " ('birthday', 1.0): 53,\n",
       " ('want', 1.0): 69,\n",
       " ('t-shirt', 1.0): 2,\n",
       " ('cool', 1.0): 29,\n",
       " ('haw', 1.0): 1,\n",
       " ('phela', 1.0): 1,\n",
       " ('mom', 1.0): 7,\n",
       " ('obvious', 1.0): 1,\n",
       " ('princ', 1.0): 1,\n",
       " ('charm', 1.0): 1,\n",
       " ('stage', 1.0): 2,\n",
       " ('luck', 1.0): 26,\n",
       " ('tyler', 1.0): 1,\n",
       " ('hipster', 1.0): 1,\n",
       " ('glass', 1.0): 3,\n",
       " ('marti', 1.0): 2,\n",
       " ('glad', 1.0): 41,\n",
       " ('done', 1.0): 40,\n",
       " ('afternoon', 1.0): 7,\n",
       " ('read', 1.0): 27,\n",
       " ('kahfi', 1.0): 1,\n",
       " ('finish', 1.0): 15,\n",
       " ('ohmyg', 1.0): 1,\n",
       " ('yaya', 1.0): 3,\n",
       " ('dub', 1.0): 1,\n",
       " ('stalk', 1.0): 2,\n",
       " ('ig', 1.0): 3,\n",
       " ('gondooo', 1.0): 1,\n",
       " ('moo', 1.0): 2,\n",
       " ('tologooo', 1.0): 1,\n",
       " ('becom', 1.0): 8,\n",
       " ('detail', 1.0): 8,\n",
       " ('zzz', 1.0): 1,\n",
       " ('xx', 1.0): 33,\n",
       " ('physiotherapi', 1.0): 1,\n",
       " ('hashtag', 1.0): 3,\n",
       " ('💪', 1.0): 1,\n",
       " ('monica', 1.0): 1,\n",
       " ('miss', 1.0): 17,\n",
       " ('sound', 1.0): 20,\n",
       " ('morn', 1.0): 68,\n",
       " (\"that'\", 1.0): 49,\n",
       " ('x43', 1.0): 1,\n",
       " ('definit', 1.0): 20,\n",
       " ('tri', 1.0): 34,\n",
       " ('tonight', 1.0): 15,\n",
       " ('took', 1.0): 7,\n",
       " ('advic', 1.0): 6,\n",
       " ('treviso', 1.0): 1,\n",
       " ('concert', 1.0): 23,\n",
       " ('citi', 1.0): 26,\n",
       " ('countri', 1.0): 22,\n",
       " (\"i'll\", 1.0): 73,\n",
       " ('start', 1.0): 56,\n",
       " ('fine', 1.0): 7,\n",
       " ('gorgeou', 1.0): 9,\n",
       " ('xo', 1.0): 2,\n",
       " ('oven', 1.0): 2,\n",
       " ('roast', 1.0): 1,\n",
       " ('garlic', 1.0): 1,\n",
       " ('oliv', 1.0): 1,\n",
       " ('oil', 1.0): 4,\n",
       " ('dri', 1.0): 4,\n",
       " ('tomato', 1.0): 1,\n",
       " ('basil', 1.0): 1,\n",
       " ('centuri', 1.0): 1,\n",
       " ('tuna', 1.0): 1,\n",
       " ('right', 1.0): 38,\n",
       " ('back', 1.0): 74,\n",
       " ('atchya', 1.0): 1,\n",
       " ('even', 1.0): 26,\n",
       " ('almost', 1.0): 8,\n",
       " ('chanc', 1.0): 3,\n",
       " ('cheer', 1.0): 17,\n",
       " ('po', 1.0): 3,\n",
       " ('ice', 1.0): 6,\n",
       " ('cream', 1.0): 6,\n",
       " ('agre', 1.0): 13,\n",
       " ('100', 1.0): 6,\n",
       " ('heheheh', 1.0): 2,\n",
       " ('that', 1.0): 10,\n",
       " ('point', 1.0): 11,\n",
       " ('stay', 1.0): 20,\n",
       " ('home', 1.0): 20,\n",
       " ('soon', 1.0): 38,\n",
       " ('promis', 1.0): 4,\n",
       " ('web', 1.0): 4,\n",
       " ('whatsapp', 1.0): 3,\n",
       " ('volta', 1.0): 1,\n",
       " ('funcionar', 1.0): 1,\n",
       " ('com', 1.0): 2,\n",
       " ('iphon', 1.0): 7,\n",
       " ('jailbroken', 1.0): 1,\n",
       " ('later', 1.0): 11,\n",
       " ('34', 1.0): 3,\n",
       " ('min', 1.0): 7,\n",
       " ('leia', 1.0): 1,\n",
       " ('appear', 1.0): 3,\n",
       " ('hologram', 1.0): 1,\n",
       " ('r2d2', 1.0): 1,\n",
       " ('w', 1.0): 16,\n",
       " ('messag', 1.0): 9,\n",
       " ('obi', 1.0): 1,\n",
       " ('wan', 1.0): 1,\n",
       " ('sit', 1.0): 7,\n",
       " ('luke', 1.0): 4,\n",
       " ('inter', 1.0): 1,\n",
       " ('3', 1.0): 26,\n",
       " ('ucl', 1.0): 1,\n",
       " ('arsen', 1.0): 2,\n",
       " ('small', 1.0): 1,\n",
       " ('team', 1.0): 24,\n",
       " ('pass', 1.0): 10,\n",
       " ('🚂', 1.0): 1,\n",
       " ('dewsburi', 1.0): 2,\n",
       " ('railway', 1.0): 1,\n",
       " ('station', 1.0): 4,\n",
       " ('dew', 1.0): 1,\n",
       " ('west', 1.0): 1,\n",
       " ('yorkshir', 1.0): 2,\n",
       " ('430', 1.0): 1,\n",
       " ('smh', 1.0): 2,\n",
       " ('9:25', 1.0): 1,\n",
       " ('live', 1.0): 21,\n",
       " ('strang', 1.0): 4,\n",
       " ('imagin', 1.0): 5,\n",
       " ('megan', 1.0): 1,\n",
       " ('masaantoday', 1.0): 4,\n",
       " ('a4', 1.0): 3,\n",
       " ('shweta', 1.0): 1,\n",
       " ('tripathi', 1.0): 1,\n",
       " ('5', 1.0): 15,\n",
       " ('20', 1.0): 5,\n",
       " ('kurta', 1.0): 3,\n",
       " ('half', 1.0): 6,\n",
       " ('number', 1.0): 11,\n",
       " ('wsalelov', 1.0): 14,\n",
       " ('ah', 1.0): 12,\n",
       " ('larri', 1.0): 3,\n",
       " ('anyway', 1.0): 14,\n",
       " ('kinda', 1.0): 12,\n",
       " ('goood', 1.0): 1,\n",
       " ('life', 1.0): 36,\n",
       " ('enn', 1.0): 1,\n",
       " ('could', 1.0): 25,\n",
       " ('warmup', 1.0): 1,\n",
       " ('15th', 1.0): 2,\n",
       " ('bath', 1.0): 6,\n",
       " ('dum', 1.0): 2,\n",
       " ('andar', 1.0): 1,\n",
       " ('ram', 1.0): 1,\n",
       " ('sampath', 1.0): 1,\n",
       " ('sona', 1.0): 1,\n",
       " ('mohapatra', 1.0): 1,\n",
       " ('samantha', 1.0): 1,\n",
       " ('edward', 1.0): 1,\n",
       " ('mein', 1.0): 1,\n",
       " ('tulan', 1.0): 1,\n",
       " ('razi', 1.0): 2,\n",
       " ('wah', 1.0): 2,\n",
       " ('josh', 1.0): 1,\n",
       " ('alway', 1.0): 48,\n",
       " ('smile', 1.0): 47,\n",
       " ('pictur', 1.0): 7,\n",
       " ('16.20', 1.0): 1,\n",
       " ('giveitup', 1.0): 1,\n",
       " ('given', 1.0): 3,\n",
       " ('ga', 1.0): 3,\n",
       " ('subsidi', 1.0): 1,\n",
       " ('initi', 1.0): 2,\n",
       " ('propos', 1.0): 3,\n",
       " ('delight', 1.0): 4,\n",
       " ('yesterday', 1.0): 4,\n",
       " ('x42', 1.0): 1,\n",
       " ('lmaoo', 1.0): 2,\n",
       " ('song', 1.0): 16,\n",
       " ('ever', 1.0): 19,\n",
       " ('shall', 1.0): 5,\n",
       " ('littl', 1.0): 29,\n",
       " ('throwback', 1.0): 3,\n",
       " ('outli', 1.0): 1,\n",
       " ('island', 1.0): 2,\n",
       " ('cheung', 1.0): 1,\n",
       " ('chau', 1.0): 1,\n",
       " ('mui', 1.0): 1,\n",
       " ('wo', 1.0): 1,\n",
       " ('total', 1.0): 5,\n",
       " ('differ', 1.0): 10,\n",
       " ('kfckitchentour', 1.0): 2,\n",
       " ('kitchen', 1.0): 3,\n",
       " ('clean', 1.0): 1,\n",
       " (\"i'm\", 1.0): 140,\n",
       " ('cusp', 1.0): 1,\n",
       " ('test', 1.0): 7,\n",
       " ('water', 1.0): 7,\n",
       " ('reward', 1.0): 1,\n",
       " ('arummzz', 1.0): 2,\n",
       " (\"let'\", 1.0): 18,\n",
       " ('drive', 1.0): 9,\n",
       " ('travel', 1.0): 19,\n",
       " ('yogyakarta', 1.0): 3,\n",
       " ('jeep', 1.0): 3,\n",
       " ('indonesia', 1.0): 3,\n",
       " ('instamood', 1.0): 3,\n",
       " ('wanna', 1.0): 23,\n",
       " ('skype', 1.0): 3,\n",
       " ('may', 1.0): 16,\n",
       " ('nice', 1.0): 70,\n",
       " ('friendli', 1.0): 1,\n",
       " ('pretend', 1.0): 2,\n",
       " ('film', 1.0): 8,\n",
       " ('congratul', 1.0): 9,\n",
       " ('winner', 1.0): 3,\n",
       " ('cheesydelight', 1.0): 1,\n",
       " ('contest', 1.0): 5,\n",
       " ('address', 1.0): 8,\n",
       " ('guy', 1.0): 48,\n",
       " ('market', 1.0): 5,\n",
       " ('24/7', 1.0): 1,\n",
       " ('14', 1.0): 1,\n",
       " ('hour', 1.0): 24,\n",
       " ('leav', 1.0): 11,\n",
       " ('without', 1.0): 9,\n",
       " ('delay', 1.0): 1,\n",
       " ('actual', 1.0): 13,\n",
       " ('easi', 1.0): 7,\n",
       " ('guess', 1.0): 8,\n",
       " ('train', 1.0): 7,\n",
       " ('wd', 1.0): 1,\n",
       " ('shift', 1.0): 4,\n",
       " ('engin', 1.0): 1,\n",
       " ('etc', 1.0): 2,\n",
       " ('sunburn', 1.0): 1,\n",
       " ('peel', 1.0): 2,\n",
       " ('blog', 1.0): 27,\n",
       " ('huge', 1.0): 9,\n",
       " ('warm', 1.0): 4,\n",
       " ('☆', 1.0): 3,\n",
       " ('complet', 1.0): 10,\n",
       " ('triangl', 1.0): 2,\n",
       " ('northern', 1.0): 1,\n",
       " ('ireland', 1.0): 2,\n",
       " ('sight', 1.0): 1,\n",
       " ('smthng', 1.0): 2,\n",
       " ('fr', 1.0): 3,\n",
       " ('hug', 1.0): 11,\n",
       " ('xoxo', 1.0): 3,\n",
       " ('uu', 1.0): 1,\n",
       " ('jaann', 1.0): 1,\n",
       " ('topnewfollow', 1.0): 2,\n",
       " ('connect', 1.0): 13,\n",
       " ('wonder', 1.0): 26,\n",
       " ('made', 1.0): 38,\n",
       " ('fluffi', 1.0): 1,\n",
       " ('insid', 1.0): 7,\n",
       " ('pirouett', 1.0): 1,\n",
       " ('moos', 1.0): 1,\n",
       " ('trip', 1.0): 12,\n",
       " ('philli', 1.0): 1,\n",
       " ('decemb', 1.0): 2,\n",
       " (\"i'd\", 1.0): 13,\n",
       " ('dude', 1.0): 6,\n",
       " ('x41', 1.0): 1,\n",
       " ('question', 1.0): 15,\n",
       " ('flaw', 1.0): 1,\n",
       " ('pain', 1.0): 8,\n",
       " ('negat', 1.0): 1,\n",
       " ('strength', 1.0): 2,\n",
       " ('went', 1.0): 10,\n",
       " ('solo', 1.0): 4,\n",
       " ('move', 1.0): 9,\n",
       " ('fav', 1.0): 11,\n",
       " ('nirvana', 1.0): 1,\n",
       " ('smell', 1.0): 2,\n",
       " ('teen', 1.0): 3,\n",
       " ('spirit', 1.0): 1,\n",
       " ('rip', 1.0): 3,\n",
       " ('ami', 1.0): 4,\n",
       " ('winehous', 1.0): 1,\n",
       " ('coupl', 1.0): 5,\n",
       " ('tomhiddleston', 1.0): 1,\n",
       " ('elizabetholsen', 1.0): 1,\n",
       " ('yaytheylookgreat', 1.0): 1,\n",
       " ('goodnight', 1.0): 18,\n",
       " ('vid', 1.0): 8,\n",
       " ('wake', 1.0): 10,\n",
       " ('gonna', 1.0): 16,\n",
       " ('shoot', 1.0): 5,\n",
       " ('itti', 1.0): 2,\n",
       " ('bitti', 1.0): 2,\n",
       " ('teeni', 1.0): 2,\n",
       " ('bikini', 1.0): 3,\n",
       " ('much', 1.0): 73,\n",
       " ('4th', 1.0): 4,\n",
       " ('togeth', 1.0): 6,\n",
       " ('end', 1.0): 13,\n",
       " ('xfile', 1.0): 1,\n",
       " ('content', 1.0): 3,\n",
       " ('rain', 1.0): 18,\n",
       " ('fabul', 1.0): 4,\n",
       " ('fantast', 1.0): 8,\n",
       " ('♡', 1.0): 12,\n",
       " ('jb', 1.0): 1,\n",
       " ('forev', 1.0): 5,\n",
       " ('belieb', 1.0): 3,\n",
       " ('nighti', 1.0): 1,\n",
       " ('bug', 1.0): 2,\n",
       " ('bite', 1.0): 1,\n",
       " ('bracelet', 1.0): 2,\n",
       " ('idea', 1.0): 23,\n",
       " ('foundri', 1.0): 1,\n",
       " ('game', 1.0): 23,\n",
       " ('sens', 1.0): 6,\n",
       " ('pic', 1.0): 21,\n",
       " ('ef', 1.0): 1,\n",
       " ('phone', 1.0): 16,\n",
       " ('woot', 1.0): 2,\n",
       " ('derek', 1.0): 1,\n",
       " ('use', 1.0): 32,\n",
       " ('parkshar', 1.0): 1,\n",
       " ('gloucestershir', 1.0): 1,\n",
       " ('aaaahhh', 1.0): 1,\n",
       " ('man', 1.0): 16,\n",
       " ('traffic', 1.0): 2,\n",
       " ('stress', 1.0): 4,\n",
       " ('reliev', 1.0): 1,\n",
       " (\"how'r\", 1.0): 1,\n",
       " ('arbeloa', 1.0): 1,\n",
       " ('turn', 1.0): 13,\n",
       " ('17', 1.0): 2,\n",
       " ('omg', 1.0): 13,\n",
       " ('say', 1.0): 43,\n",
       " ('europ', 1.0): 1,\n",
       " ('rise', 1.0): 2,\n",
       " ('find', 1.0): 20,\n",
       " ('hard', 1.0): 9,\n",
       " ('believ', 1.0): 7,\n",
       " ('uncount', 1.0): 1,\n",
       " ('coz', 1.0): 2,\n",
       " ('unlimit', 1.0): 1,\n",
       " ('cours', 1.0): 11,\n",
       " ('teamposit', 1.0): 1,\n",
       " ('aldub', 1.0): 2,\n",
       " ('☕', 1.0): 3,\n",
       " ('rita', 1.0): 2,\n",
       " ('info', 1.0): 10,\n",
       " (\"we'd\", 1.0): 4,\n",
       " ('way', 1.0): 34,\n",
       " ('boy', 1.0): 13,\n",
       " ('x40', 1.0): 1,\n",
       " ('true', 1.0): 19,\n",
       " ('sethi', 1.0): 2,\n",
       " ('high', 1.0): 6,\n",
       " ('exe', 1.0): 1,\n",
       " ('skeem', 1.0): 1,\n",
       " ('saam', 1.0): 1,\n",
       " ('peopl', 1.0): 42,\n",
       " ('polit', 1.0): 2,\n",
       " ('izzat', 1.0): 1,\n",
       " ('wese', 1.0): 1,\n",
       " ('trust', 1.0): 7,\n",
       " ('khawateen', 1.0): 1,\n",
       " ('k', 1.0): 8,\n",
       " ('sath', 1.0): 2,\n",
       " ('mana', 1.0): 1,\n",
       " ('kar', 1.0): 1,\n",
       " ('deya', 1.0): 1,\n",
       " ('sort', 1.0): 7,\n",
       " ('smart', 1.0): 5,\n",
       " ('hair', 1.0): 7,\n",
       " ('tbh', 1.0): 5,\n",
       " ('jacob', 1.0): 2,\n",
       " ('g', 1.0): 7,\n",
       " ('upgrad', 1.0): 2,\n",
       " ('tee', 1.0): 2,\n",
       " ('famili', 1.0): 14,\n",
       " ('person', 1.0): 14,\n",
       " ('two', 1.0): 15,\n",
       " ('convers', 1.0): 6,\n",
       " ('onlin', 1.0): 4,\n",
       " ('mclaren', 1.0): 1,\n",
       " ('fridayfeel', 1.0): 5,\n",
       " ('tgif', 1.0): 8,\n",
       " ('squar', 1.0): 1,\n",
       " ('enix', 1.0): 1,\n",
       " ('bissmillah', 1.0): 1,\n",
       " ('ya', 1.0): 19,\n",
       " ('allah', 1.0): 3,\n",
       " (\"we'r\", 1.0): 25,\n",
       " ('socent', 1.0): 1,\n",
       " ('startup', 1.0): 2,\n",
       " ('drop', 1.0): 9,\n",
       " ('your', 1.0): 3,\n",
       " ('arnd', 1.0): 1,\n",
       " ('town', 1.0): 3,\n",
       " ('basic', 1.0): 4,\n",
       " ('piss', 1.0): 2,\n",
       " ('cup', 1.0): 4,\n",
       " ('also', 1.0): 28,\n",
       " ('terribl', 1.0): 2,\n",
       " ('complic', 1.0): 1,\n",
       " ('discuss', 1.0): 2,\n",
       " ('snapchat', 1.0): 31,\n",
       " ('lynettelow', 1.0): 1,\n",
       " ('kikmenow', 1.0): 2,\n",
       " ('snapm', 1.0): 1,\n",
       " ('hot', 1.0): 20,\n",
       " ('amazon', 1.0): 1,\n",
       " ('kikmeguy', 1.0): 2,\n",
       " ('defin', 1.0): 2,\n",
       " ('grow', 1.0): 6,\n",
       " ('sport', 1.0): 4,\n",
       " ('rt', 1.0): 9,\n",
       " ('rakyat', 1.0): 1,\n",
       " ('write', 1.0): 11,\n",
       " ('sinc', 1.0): 11,\n",
       " ('mention', 1.0): 18,\n",
       " ('fli', 1.0): 5,\n",
       " ('fish', 1.0): 3,\n",
       " ('promot', 1.0): 3,\n",
       " ('post', 1.0): 16,\n",
       " ('cyber', 1.0): 1,\n",
       " ('ourdaughtersourprid', 1.0): 3,\n",
       " ('mypapamyprid', 1.0): 2,\n",
       " ('papa', 1.0): 1,\n",
       " ('coach', 1.0): 2,\n",
       " ('posit', 1.0): 3,\n",
       " ('kha', 1.0): 1,\n",
       " ('atleast', 1.0): 2,\n",
       " ('x39', 1.0): 1,\n",
       " ('mango', 1.0): 1,\n",
       " (\"lassi'\", 1.0): 1,\n",
       " (\"monty'\", 1.0): 1,\n",
       " ('marvel', 1.0): 2,\n",
       " ('though', 1.0): 16,\n",
       " ('suspect', 1.0): 3,\n",
       " ('meant', 1.0): 2,\n",
       " ('24', 1.0): 3,\n",
       " ('hr', 1.0): 2,\n",
       " ('touch', 1.0): 7,\n",
       " ('kepler', 1.0): 3,\n",
       " ('452b', 1.0): 4,\n",
       " ('chalna', 1.0): 1,\n",
       " ('hai', 1.0): 7,\n",
       " ('thankyou', 1.0): 12,\n",
       " ('hazel', 1.0): 1,\n",
       " ('food', 1.0): 6,\n",
       " ('brooklyn', 1.0): 1,\n",
       " ('pta', 1.0): 2,\n",
       " ('awak', 1.0): 8,\n",
       " ('okayi', 1.0): 2,\n",
       " ('awww', 1.0): 12,\n",
       " ('ha', 1.0): 18,\n",
       " ('doc', 1.0): 1,\n",
       " ('splendid', 1.0): 1,\n",
       " ('spam', 1.0): 1,\n",
       " ('folder', 1.0): 1,\n",
       " ('amount', 1.0): 1,\n",
       " ('nigeria', 1.0): 1,\n",
       " ('claim', 1.0): 1,\n",
       " ('rted', 1.0): 1,\n",
       " ('leg', 1.0): 3,\n",
       " ('hurt', 1.0): 4,\n",
       " ('bad', 1.0): 14,\n",
       " ('mine', 1.0): 11,\n",
       " ('saturday', 1.0): 5,\n",
       " ('thaaank', 1.0): 1,\n",
       " ('puhon', 1.0): 1,\n",
       " ('happinesss', 1.0): 1,\n",
       " ('tnc', 1.0): 1,\n",
       " ('prior', 1.0): 1,\n",
       " ('notif', 1.0): 2,\n",
       " ('fat', 1.0): 1,\n",
       " ('co', 1.0): 1,\n",
       " ('probabl', 1.0): 7,\n",
       " ('ate', 1.0): 4,\n",
       " ('yuna', 1.0): 2,\n",
       " ('tamesid', 1.0): 1,\n",
       " ('´', 1.0): 3,\n",
       " ('googl', 1.0): 5,\n",
       " ('account', 1.0): 17,\n",
       " ('scouser', 1.0): 1,\n",
       " ('everyth', 1.0): 10,\n",
       " ('zoe', 1.0): 1,\n",
       " ('mate', 1.0): 5,\n",
       " ('liter', 1.0): 5,\n",
       " (\"they'r\", 1.0): 10,\n",
       " ('samee', 1.0): 1,\n",
       " ('edgar', 1.0): 1,\n",
       " ('updat', 1.0): 12,\n",
       " ('log', 1.0): 3,\n",
       " ('bring', 1.0): 12,\n",
       " ('abe', 1.0): 1,\n",
       " ('meet', 1.0): 26,\n",
       " ('x38', 1.0): 1,\n",
       " ('sigh', 1.0): 3,\n",
       " ('dreamili', 1.0): 1,\n",
       " ('pout', 1.0): 1,\n",
       " ('eye', 1.0): 12,\n",
       " ('quacketyquack', 1.0): 6,\n",
       " ('funni', 1.0): 15,\n",
       " ('happen', 1.0): 13,\n",
       " ('phil', 1.0): 1,\n",
       " ('em', 1.0): 2,\n",
       " ('del', 1.0): 1,\n",
       " ('rodder', 1.0): 1,\n",
       " ('els', 1.0): 8,\n",
       " ('play', 1.0): 37,\n",
       " ('newest', 1.0): 1,\n",
       " ('gamejam', 1.0): 1,\n",
       " ('irish', 1.0): 2,\n",
       " ('literatur', 1.0): 2,\n",
       " ('inaccess', 1.0): 2,\n",
       " (\"kareena'\", 1.0): 2,\n",
       " ('fan', 1.0): 21,\n",
       " ('brain', 1.0): 10,\n",
       " ('dot', 1.0): 8,\n",
       " ('braindot', 1.0): 8,\n",
       " ('fair', 1.0): 4,\n",
       " ('rush', 1.0): 1,\n",
       " ('either', 1.0): 10,\n",
       " ('brandi', 1.0): 1,\n",
       " ('18', 1.0): 5,\n",
       " ('carniv', 1.0): 1,\n",
       " ('men', 1.0): 8,\n",
       " ('put', 1.0): 11,\n",
       " ('mask', 1.0): 2,\n",
       " ('xavier', 1.0): 1,\n",
       " ('forneret', 1.0): 1,\n",
       " ('jennif', 1.0): 1,\n",
       " ('site', 1.0): 7,\n",
       " ('free', 1.0): 31,\n",
       " ('50.000', 1.0): 3,\n",
       " ('8', 1.0): 10,\n",
       " ('ball', 1.0): 7,\n",
       " ('pool', 1.0): 5,\n",
       " ('coin', 1.0): 5,\n",
       " ('edit', 1.0): 6,\n",
       " ('trish', 1.0): 1,\n",
       " ('♥', 1.0): 13,\n",
       " ('grate', 1.0): 5,\n",
       " ('three', 1.0): 8,\n",
       " ('comment', 1.0): 7,\n",
       " ('wakeup', 1.0): 1,\n",
       " ('besid', 1.0): 2,\n",
       " ('dirti', 1.0): 2,\n",
       " ('sex', 1.0): 4,\n",
       " ('lmaooo', 1.0): 1,\n",
       " ('😤', 1.0): 2,\n",
       " ('loui', 1.0): 4,\n",
       " (\"he'\", 1.0): 10,\n",
       " ('throw', 1.0): 3,\n",
       " ('caus', 1.0): 11,\n",
       " ('inspir', 1.0): 6,\n",
       " ('ff', 1.0): 40,\n",
       " ('twoof', 1.0): 3,\n",
       " ('gr8', 1.0): 1,\n",
       " ('wkend', 1.0): 3,\n",
       " ('kind', 1.0): 22,\n",
       " ('exhaust', 1.0): 2,\n",
       " ('word', 1.0): 17,\n",
       " ('cheltenham', 1.0): 1,\n",
       " ('area', 1.0): 4,\n",
       " ('kale', 1.0): 1,\n",
       " ('crisp', 1.0): 1,\n",
       " ('ruin', 1.0): 5,\n",
       " ('x37', 1.0): 1,\n",
       " ('open', 1.0): 12,\n",
       " ('worldwid', 1.0): 2,\n",
       " ('outta', 1.0): 1,\n",
       " ('sfvbeta', 1.0): 1,\n",
       " ('vantast', 1.0): 1,\n",
       " ('xcylin', 1.0): 1,\n",
       " ('bundl', 1.0): 1,\n",
       " ('show', 1.0): 20,\n",
       " ('internet', 1.0): 2,\n",
       " ('price', 1.0): 3,\n",
       " ('realisticli', 1.0): 1,\n",
       " ('pay', 1.0): 8,\n",
       " ('net', 1.0): 1,\n",
       " ('educ', 1.0): 1,\n",
       " ('power', 1.0): 6,\n",
       " ('weapon', 1.0): 1,\n",
       " ('nelson', 1.0): 1,\n",
       " ('mandela', 1.0): 1,\n",
       " ('recent', 1.0): 8,\n",
       " ('j', 1.0): 2,\n",
       " ('chenab', 1.0): 1,\n",
       " ('flow', 1.0): 5,\n",
       " ('pakistan', 1.0): 1,\n",
       " ('incredibleindia', 1.0): 1,\n",
       " ('teenchoic', 1.0): 7,\n",
       " ('choiceinternationalartist', 1.0): 7,\n",
       " ('superjunior', 1.0): 7,\n",
       " ('caught', 1.0): 4,\n",
       " ('first', 1.0): 41,\n",
       " ('salmon', 1.0): 1,\n",
       " ('super-blend', 1.0): 1,\n",
       " ('project', 1.0): 6,\n",
       " ('youth@bipolaruk.org.uk', 1.0): 1,\n",
       " ('awesom', 1.0): 35,\n",
       " ('stream', 1.0): 12,\n",
       " ('alma', 1.0): 1,\n",
       " ('mater', 1.0): 1,\n",
       " ('highschoolday', 1.0): 1,\n",
       " ('clientvisit', 1.0): 1,\n",
       " ('faith', 1.0): 3,\n",
       " ('christian', 1.0): 1,\n",
       " ('school', 1.0): 9,\n",
       " ('lizaminnelli', 1.0): 1,\n",
       " ('upcom', 1.0): 2,\n",
       " ('uk', 1.0): 4,\n",
       " ('😄', 1.0): 3,\n",
       " ('singl', 1.0): 4,\n",
       " ('hill', 1.0): 4,\n",
       " ('everi', 1.0): 23,\n",
       " ('beat', 1.0): 7,\n",
       " ('wrong', 1.0): 9,\n",
       " ('readi', 1.0): 22,\n",
       " ('natur', 1.0): 1,\n",
       " ('pefumeri', 1.0): 1,\n",
       " ('workshop', 1.0): 2,\n",
       " ('neal', 1.0): 1,\n",
       " ('yard', 1.0): 1,\n",
       " ('covent', 1.0): 1,\n",
       " ('tomorrow', 1.0): 31,\n",
       " ('fback', 1.0): 26,\n",
       " ...}"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs = build_freqs(train_x, train_y)\n",
    "freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):   \n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient(x,y,theta,alpha,num_iters):\n",
    "    m=x.shape[0]\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        z=np.dot(x,theta)\n",
    "        h=sigmoid(z)\n",
    "        J = -1./m * (np.dot(y.transpose(), np.log(h)) + np.dot((1-y).transpose(),np.log(1-h)))                                                    \n",
    "        theta=theta-(alpha/m)*np.dot(x.transpose(),(h-y))\n",
    "        \n",
    "    J=float(J)\n",
    "    \n",
    "    return J,theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost after training is 0.67094970.\n",
      "The resulting vector of weights is [4.1e-07, 0.00035658, 7.309e-05]\n"
     ]
    }
   ],
   "source": [
    "# Check the function\n",
    "# Construct a synthetic test case using numpy PRNG functions\n",
    "np.random.seed(1)\n",
    "# X input is 10 x 3 with ones for the bias terms\n",
    "tmp_X = np.append(np.ones((10, 1)), np.random.rand(10, 2) * 2000, axis=1)\n",
    "# Y Labels are 10 x 1\n",
    "tmp_Y = (np.random.rand(10, 1) > 0.35).astype(float)\n",
    "\n",
    "# Apply gradient descent\n",
    "tmp_J, tmp_theta = gradient(tmp_X, tmp_Y, np.zeros((3, 1)), 1e-8, 700)\n",
    "print(f\"The cost after training is {tmp_J:.8f}.\")\n",
    "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(tmp_theta)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extracting the Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(tweet,freqs):\n",
    "    word_1=process_tweet(tweet)\n",
    "    x=np.zeros((1,3))\n",
    "    \n",
    "    x[0,0]=1\n",
    "    \n",
    "    for word in word_1:\n",
    "        x[0,1]+=freqs.get((word,1.),0)\n",
    "        x[0,2]+=freqs.get((word,0.),0)\n",
    "    \n",
    "    assert(x.shape==(1,3))\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00e+00 3.02e+03 6.10e+01]]\n"
     ]
    }
   ],
   "source": [
    "# Check your function\n",
    "tmp1 = extract_features(train_x[0], freqs)\n",
    "print(tmp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Check for when the words are not in the freqs dictionary\n",
    "tmp2 = extract_features('blorb bleeeeb bloooob', freqs)\n",
    "print(tmp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-277-67dfe33f5081>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfreqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_x' is not defined"
     ]
    }
   ],
   "source": [
    "X=np.zeros((len(train_x),3))\n",
    "for i in range(len(train_x)):\n",
    "    X[i,:]=extract_features(train_x[i],freqs)\n",
    "    \n",
    "Y=train_y\n",
    "\n",
    "J,theta=gradient(X,Y,np.zeros((3,1)),1e-9,1500)\n",
    "print(f\"The cost after training is {J:.8f}.\")\n",
    "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Test Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_tweet(tweet,freqs,theta):\n",
    "    x=extract_features(tweet,freqs)\n",
    "    y_pred=sigmoid(np.dot(x,theta))\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentiment(x):\n",
    "    return str('Good') if x>0.5 else str('Bad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am happy -> 0.5185801561975211 -> Good\n",
      "I am bad -> 0.4943391042897527 -> Bad\n",
      "this movie should have been great. -> 0.515331061366408 -> Good\n",
      "great -> 0.5154638369272457 -> Good\n",
      "great great -> 0.5308981010928904 -> Good\n",
      "great great great -> 0.5462734815407703 -> Good\n",
      "great great great great -> 0.5615610965558285 -> Good\n"
     ]
    }
   ],
   "source": [
    "for tweet in ['I am happy', 'I am bad', 'this movie should have been great.',\\\n",
    "              'great', 'great great', 'great great great', 'great great great great']:\n",
    "    print( '{} -> {} -> {}' .format(tweet,predict_tweet(tweet,freqs,theta)[0][0] ,sentiment(predict_tweet(tweet, freqs, theta)[0][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.81636482]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tweet = 'I am learning :)'\n",
    "predict_tweet(my_tweet, freqs, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_lr(test_x,test_y,freqs,theta):\n",
    "    y_hat=[]\n",
    "    \n",
    "    for tweet in test_x:\n",
    "        y_pred=predict_tweet(tweet,freqs,theta)\n",
    "        y_hat.append(1) if y_pred>0.5 else y_hat.append(0)\n",
    "            \n",
    "    accuracy=(y_hat==np.squeeze(test_y)).sum()/len(test_x)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression model's accuracy = 0.9950\n"
     ]
    }
   ],
   "source": [
    "tmp_accuracy = test_lr(test_x, test_y, freqs, theta)\n",
    "print(f\"Logistic regression model's accuracy = {tmp_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Predicted Tweet\n",
      "THE TWEET IS: @jaredNOTsubway @iluvmariah @Bravotv Then that truly is a LATERAL move! Now, we all know the Queen Bee is UPWARD BOUND : ) #MovingOnUp\n",
      "THE PROCESSED TWEET IS: ['truli', 'later', 'move', 'know', 'queen', 'bee', 'upward', 'bound', 'movingonup']\n",
      "1\t0.49996897\tb'truli later move know queen bee upward bound movingonup'\n",
      "THE TWEET IS: @MarkBreech Not sure it would be good thing 4 my bottom daring 2 say 2 Miss B but Im gonna be so stubborn on mouth soaping ! #NotHavingit :p\n",
      "THE PROCESSED TWEET IS: ['sure', 'would', 'good', 'thing', '4', 'bottom', 'dare', '2', 'say', '2', 'miss', 'b', 'im', 'gonna', 'stubborn', 'mouth', 'soap', 'nothavingit', ':p']\n",
      "1\t0.48650628\tb'sure would good thing 4 bottom dare 2 say 2 miss b im gonna stubborn mouth soap nothavingit :p'\n",
      "THE TWEET IS: I'm playing Brain Dots : ) #BrainDots\n",
      "http://t.co/UGQzOx0huu\n",
      "THE PROCESSED TWEET IS: [\"i'm\", 'play', 'brain', 'dot', 'braindot']\n",
      "1\t0.48370676\tb\"i'm play brain dot braindot\"\n",
      "THE TWEET IS: I'm playing Brain Dots : ) #BrainDots http://t.co/aOKldo3GMj http://t.co/xWCM9qyRG5\n",
      "THE PROCESSED TWEET IS: [\"i'm\", 'play', 'brain', 'dot', 'braindot']\n",
      "1\t0.48370676\tb\"i'm play brain dot braindot\"\n",
      "THE TWEET IS: I'm playing Brain Dots : ) #BrainDots http://t.co/R2JBO8iNww http://t.co/ow5BBwdEMY\n",
      "THE PROCESSED TWEET IS: [\"i'm\", 'play', 'brain', 'dot', 'braindot']\n",
      "1\t0.48370676\tb\"i'm play brain dot braindot\"\n",
      "THE TWEET IS: off to the park to get some sunlight : )\n",
      "THE PROCESSED TWEET IS: ['park', 'get', 'sunlight']\n",
      "1\t0.49578773\tb'park get sunlight'\n",
      "THE TWEET IS: @msarosh Uff Itna Miss karhy thy ap :p\n",
      "THE PROCESSED TWEET IS: ['uff', 'itna', 'miss', 'karhi', 'thi', 'ap', ':p']\n",
      "1\t0.48199817\tb'uff itna miss karhi thi ap :p'\n",
      "THE TWEET IS: @phenomyoutube u probs had more fun with david than me : (\n",
      "THE PROCESSED TWEET IS: ['u', 'prob', 'fun', 'david']\n",
      "0\t0.50020361\tb'u prob fun david'\n",
      "THE TWEET IS: pats jay : (\n",
      "THE PROCESSED TWEET IS: ['pat', 'jay']\n",
      "0\t0.50039294\tb'pat jay'\n",
      "THE TWEET IS: my beloved grandmother : ( https://t.co/wt4oXq5xCf\n",
      "THE PROCESSED TWEET IS: ['belov', 'grandmoth']\n",
      "0\t0.50000002\tb'belov grandmoth'\n"
     ]
    }
   ],
   "source": [
    "# Some error analysis done for you\n",
    "print('Label Predicted Tweet')\n",
    "for x,y in zip(test_x,test_y):\n",
    "    y_hat = predict_tweet(x, freqs, theta)\n",
    "\n",
    "    if np.abs(y - (y_hat > 0.5)) > 0:\n",
    "        print('THE TWEET IS:', x)\n",
    "        print('THE PROCESSED TWEET IS:', process_tweet(x))\n",
    "        print('%d\\t%0.8f\\t%s' % (y, y_hat, ' '.join(process_tweet(x)).encode('ascii', 'ignore')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# With a New Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['awesom', 'movi', 'incred']\n",
      "[[0.50441204]]\n",
      "Positive sentiment\n"
     ]
    }
   ],
   "source": [
    "# Feel free to change the tweet below\n",
    "my_tweet = 'This is an awesome movie. Incredible!'\n",
    "print(process_tweet(my_tweet))\n",
    "y_hat = predict_tweet(my_tweet, freqs, theta)\n",
    "print(y_hat)\n",
    "if y_hat > 0.5:\n",
    "    print('Positive sentiment')\n",
    "else: \n",
    "    print('Negative sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ENDS Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Starting Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train Model Using Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_positive_tweets=twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets=twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "# Split the data\n",
    "train_pos=all_positive_tweets[:4000]\n",
    "test_pos=all_positive_tweets[4000:]\n",
    "train_neg=all_negative_tweets[:4000]\n",
    "test_neg=all_negative_tweets[4000:]\n",
    "\n",
    "train_x=train_pos+train_neg\n",
    "test_x=test_pos+test_neg\n",
    "\n",
    "train_y=np.append(np.ones(len(train_pos)),np.zeros(len(train_neg)))\n",
    "test_y=np.append(np.ones(len(test_pos)),np.zeros(len(test_neg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_tweets(result,tweets,ys):\n",
    "    for y,tweet in zip(ys,tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            pair=(word,y)\n",
    "            \n",
    "            if pair in result:\n",
    "                result[pair]+=1\n",
    "            else:\n",
    "                result[pair]=1\n",
    "                \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lookup(freqs,word,label):\n",
    "    n=0\n",
    "    pair=(word,label)\n",
    "    \n",
    "    if pair in freqs:\n",
    "        n=freqs[pair]\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_naive_bayes(freqs,train_x,train_y):\n",
    "    loglikelihood={}\n",
    "    logprior=0\n",
    "    \n",
    "    # Calculate V,the number of unique words in the vocab\n",
    "    vocab=set([pair[0] for pair in freqs.keys()])\n",
    "    V=len(vocab)\n",
    "    \n",
    "    # Calculate N_pos,N_neg,V_pos,V_neg\n",
    "    N_pos=N_neg=0\n",
    "#     V_pos=V_neg=0\n",
    "    \n",
    "    for pair in freqs.keys():\n",
    "        if pair[1]>0:\n",
    "#             V_pos+=1\n",
    "            N_pos+=freqs[pair]\n",
    "        else:\n",
    "#             V_neg+=1\n",
    "            N_neg+=freqs[pair]\n",
    "    \n",
    "#     D=len(train_y)\n",
    "\n",
    "    # Calculate D_pos,the number of positive documents\n",
    "    D_pos=(len(list(filter(lambda x:x>0,train_y))))\n",
    "         \n",
    "    # Calculate D_neg,the number of negative documents\n",
    "    D_neg=(len(list(filter(lambda x:x<=0,train_y))))\n",
    "\n",
    "    # Calculate logprior\n",
    "    logprior=np.log(D_pos)-np.log(D_neg)\n",
    "    \n",
    "    for word in vocab:\n",
    "        # Calculate the frequency of positive/negative word\n",
    "        freq_pos=lookup(freqs,word,1)\n",
    "        freq_neg=lookup(freqs,word,0)\n",
    "        \n",
    "        # Calculate the probability that each word is positice/negative\n",
    "        p_w_pos=(freq_pos+1)/(N_pos+V)\n",
    "        p_w_neg=(freq_neg+1)/(N_neg+V)\n",
    "        \n",
    "        # Calculate the log likelihood of the word\n",
    "        loglikelihood[word]=np.log(p_w_pos/p_w_neg)\n",
    "        \n",
    "    return logprior,loglikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "9085\n"
     ]
    }
   ],
   "source": [
    "logprior, loglikelihood = train_naive_bayes(freqs, train_x, train_y)\n",
    "print(logprior)\n",
    "print(len(loglikelihood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def naive_bayes_predict(tweet,logprior,loglikelihood):\n",
    "    word_1=process_tweet(tweet)\n",
    "    \n",
    "    p=0\n",
    "    \n",
    "    p+=logprior\n",
    "    \n",
    "    for word in word_1:\n",
    "        if word in loglikelihood:\n",
    "            p+=loglikelihood[word]\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_naive_bayes(test_x,test_y,logprior,loglikelihood):\n",
    "    accuracy=0\n",
    "    y_hats=[]\n",
    "    \n",
    "    for tweet in test_x:\n",
    "        if naive_bayes_predict(tweet,logprior,loglikelihood)>0:\n",
    "            y_hat_i=1\n",
    "        else:\n",
    "            y_hat_i=0\n",
    "        y_hats.append(y_hat_i)\n",
    "    \n",
    "    error=np.mean(np.absolute(y_hats-test_y))\n",
    "    \n",
    "    accuracy=1-error\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filter words by Ratio of positive to negative counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ratio(freqs,word):\n",
    "    pos_neg_ratio = {'positive': 0, 'negative': 0, 'ratio': 0.0}\n",
    "   \n",
    "    pos_neg_ratio['positive'] = lookup(freqs,word,1)\n",
    "\n",
    "    pos_neg_ratio['negative'] = lookup(freqs,word,0)\n",
    "\n",
    "    # calculate the ratio of positive to negative counts for the word\n",
    "    pos_neg_ratio['ratio'] = (pos_neg_ratio['positive'] + 1)/(pos_neg_ratio['negative'] + 1)\n",
    "\n",
    "    return pos_neg_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'negative': 18, 'positive': 161, 'ratio': 8.526315789473685}"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ratio(freqs, 'happi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_words_by_threshold(freqs,label,threshold):\n",
    "    word_list = {}\n",
    "\n",
    "    for key in freqs.keys():\n",
    "        word, _ = key\n",
    "\n",
    "        pos_neg_ratio = get_ratio(freqs, word)\n",
    "\n",
    "        if label == 1 and pos_neg_ratio['ratio'] >= threshold :\n",
    "\n",
    "            word_list[word] = pos_neg_ratio\n",
    "\n",
    "        elif label == 0 and pos_neg_ratio['ratio'] <= threshold:\n",
    "            \n",
    "            word_list[word] = pos_neg_ratio\n",
    "\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{':)': {'negative': 2, 'positive': 2847, 'ratio': 949.3333333333334},\n",
       " ':-)': {'negative': 0, 'positive': 543, 'ratio': 544.0},\n",
       " ':D': {'negative': 0, 'positive': 498, 'ratio': 499.0},\n",
       " ':p': {'negative': 0, 'positive': 103, 'ratio': 104.0},\n",
       " ';)': {'negative': 0, 'positive': 22, 'ratio': 23.0},\n",
       " 'arriv': {'negative': 4, 'positive': 57, 'ratio': 11.6},\n",
       " 'bam': {'negative': 0, 'positive': 44, 'ratio': 45.0},\n",
       " 'blog': {'negative': 0, 'positive': 27, 'ratio': 28.0},\n",
       " 'commun': {'negative': 1, 'positive': 27, 'ratio': 14.0},\n",
       " 'fav': {'negative': 0, 'positive': 11, 'ratio': 12.0},\n",
       " 'fback': {'negative': 0, 'positive': 26, 'ratio': 27.0},\n",
       " 'flipkartfashionfriday': {'negative': 0, 'positive': 16, 'ratio': 17.0},\n",
       " 'followfriday': {'negative': 0, 'positive': 23, 'ratio': 24.0},\n",
       " 'glad': {'negative': 2, 'positive': 41, 'ratio': 14.0},\n",
       " \"here'\": {'negative': 0, 'positive': 20, 'ratio': 21.0},\n",
       " 'influenc': {'negative': 0, 'positive': 16, 'ratio': 17.0},\n",
       " 'pleasur': {'negative': 0, 'positive': 10, 'ratio': 11.0},\n",
       " 'shout': {'negative': 0, 'positive': 11, 'ratio': 12.0},\n",
       " 'stat': {'negative': 0, 'positive': 51, 'ratio': 52.0},\n",
       " 'via': {'negative': 1, 'positive': 60, 'ratio': 30.5},\n",
       " 'warsaw': {'negative': 0, 'positive': 44, 'ratio': 45.0},\n",
       " 'youth': {'negative': 0, 'positive': 14, 'ratio': 15.0}}"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_words_by_threshold(freqs, label=1, threshold=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truth Predicted Tweet\n",
      "1\t0.00\tb''\n",
      "1\t0.00\tb'truli later move know queen bee upward bound movingonup'\n",
      "1\t0.00\tb'new report talk burn calori cold work harder warm feel better weather :p'\n",
      "1\t0.00\tb'harri niall 94 harri born ik stupid wanna chang :D'\n",
      "1\t0.00\tb''\n",
      "1\t0.00\tb''\n",
      "1\t0.00\tb'park get sunlight'\n",
      "1\t0.00\tb'uff itna miss karhi thi ap :p'\n",
      "0\t1.00\tb'hello info possibl interest jonatha close join beti :( great'\n",
      "0\t1.00\tb'u prob fun david'\n",
      "0\t1.00\tb'pat jay'\n",
      "0\t1.00\tb'whatev stil l young >:-('\n"
     ]
    }
   ],
   "source": [
    "# error demo\n",
    "print('Truth Predicted Tweet')\n",
    "for x, y in zip(test_x, test_y):\n",
    "    y_hat = naive_bayes_predict(x, logprior, loglikelihood)\n",
    "    if y != (np.sign(y_hat) > 0):\n",
    "        print('%d\\t%0.2f\\t%s' % (y, np.sign(y_hat) > 0, ' '.join(\n",
    "            process_tweet(x)).encode('ascii', 'ignore')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8389995664033365\n"
     ]
    }
   ],
   "source": [
    "my_tweet = 'Oh shit, this is fantastic'\n",
    "\n",
    "p = naive_bayes_predict(my_tweet, logprior, loglikelihood)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Starting PCA and Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city1</th>\n",
       "      <th>country1</th>\n",
       "      <th>city2</th>\n",
       "      <th>country2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Bangkok</td>\n",
       "      <td>Thailand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Beijing</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Bern</td>\n",
       "      <td>Switzerland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Cairo</td>\n",
       "      <td>Egypt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    city1 country1    city2     country2\n",
       "0  Athens   Greece  Bangkok     Thailand\n",
       "1  Athens   Greece  Beijing        China\n",
       "2  Athens   Greece   Berlin      Germany\n",
       "3  Athens   Greece     Bern  Switzerland\n",
       "4  Athens   Greece    Cairo        Egypt"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('capitals.txt',delimiter=' ')\n",
    "data.columns=['city1','country1','city2','country2']\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# embeddings = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)\n",
    "# f = open('capitals.txt', 'r').read()\n",
    "# set_words = set(nltk.word_tokenize(f))\n",
    "# select_words = words = ['king', 'queen', 'oil', 'gas', 'happy', 'sad', 'city', 'town', 'village', 'country', 'continent', 'petroleum', 'joyful']\n",
    "# for w in select_words:\n",
    "#     set_words.add(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def get_word_embeddings(embeddings):\n",
    "\n",
    "#     word_embeddings = {}\n",
    "#     for word in embeddings.vocab:\n",
    "#         if word in set_words:\n",
    "#             word_embeddings[word] = embeddings[word]\n",
    "#     return word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Testing your function\n",
    "# word_embeddings = get_word_embeddings(embeddings)\n",
    "# print(len(word_embeddings))\n",
    "# pickle.dump( word_embeddings, open( \"word_embeddings_subset.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243\n"
     ]
    }
   ],
   "source": [
    "word_embeddings=pickle.load(open('./word_embeddings_subset.p','rb'))\n",
    "print(len(word_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimension: 300\n"
     ]
    }
   ],
   "source": [
    "print(\"dimension: {}\".format(word_embeddings['Spain'].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Predict Similarity between words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(A,B):\n",
    "    dot=np.dot(A,B)\n",
    "    \n",
    "    norma=np.sqrt((np.dot(A,A)))\n",
    "    normb=np.sqrt((np.dot(B,B)))\n",
    "    cos=dot/(norma*normb)\n",
    "    \n",
    "    return cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6510956"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try\n",
    "\n",
    "king = word_embeddings['king']\n",
    "queen = word_embeddings['queen']\n",
    "\n",
    "cosine_similarity(king, queen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def euclidean(A,B):\n",
    "    return np.linalg.norm(A-B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4796925"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "euclidean(king, queen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_country(city1,country1,city2,embeddings):\n",
    "    group=set((city1,country1,city2))\n",
    "    \n",
    "    # get the embedding of city1\n",
    "    city1_emb=word_embeddings[city1]\n",
    "    \n",
    "    # get the embedding of country1\n",
    "    country1_emb=word_embeddings[country1]\n",
    "    \n",
    "    # get the embedding of city2\n",
    "    city2_emb=word_embeddings[city2]\n",
    "    \n",
    "    # formula:King - Man + Woman = Queen\n",
    "    vec=country1_emb-city1_emb+city2_emb\n",
    "    \n",
    "    similarity=-1\n",
    "    country=''\n",
    "    \n",
    "    for word in embeddings.keys():\n",
    "        if word not in group:\n",
    "            word_emb=word_embeddings[word]\n",
    "            cur_similarity=cosine_similarity(vec,word_emb)\n",
    "            \n",
    "            if cur_similarity>similarity:\n",
    "                similarity=cur_similarity\n",
    "                country=(word,similarity)\n",
    "    return country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Egypt', 0.7626821)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "get_country('Athens', 'Greece', 'Cairo', word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_accuracy(word_embedding,data):\n",
    "    num_correct=0\n",
    "    \n",
    "    for i,row in data.iterrows():\n",
    "        city1=row['city1']\n",
    "        country1=row['country1']\n",
    "        city2=row['city2']\n",
    "        country2=row['country2']\n",
    "        \n",
    "        # get_country function to find the predicted country2\n",
    "        predicted_country2,_=get_country(city1,country1,city2,word_embedding)\n",
    "        \n",
    "        if predicted_country2==country2:\n",
    "            num_correct+=1\n",
    "    \n",
    "    accuracy=num_correct/len(data)\n",
    "    \n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.92\n"
     ]
    }
   ],
   "source": [
    "accuracy = get_accuracy(word_embeddings, data)\n",
    "print(f\"Accuracy is {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Plotting the vector using VCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_pca(X,n_components=2):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        X: of dimension (m,n) where each row corresponds to a word vector\n",
    "        n_components: Number of components you want to keep.\n",
    "    Output:\n",
    "        X_reduced: data transformed in 2 dims/columns + regenerated original data\n",
    "    \"\"\"\n",
    "    X_demeaned=X-np.mean(X,axis=0)\n",
    "    print(\"X_demeaned.shape: \",X_demeaned.shape)\n",
    "    \n",
    "    # calculate the covariance matrix\n",
    "    covariance_matrix = np.cov(X_demeaned, rowvar=False)\n",
    "\n",
    "    # calculate eigenvectors & eigenvalues of the covariance matrix\n",
    "    eigen_vals, eigen_vecs = np.linalg.eigh(covariance_matrix, UPLO='L')\n",
    "    \n",
    "    # sort eigenvalue in increasing order (get the indices from the sort)\n",
    "    idx_sorted = np.argsort(eigen_vals)\n",
    "    \n",
    "    # reverse the order so that it's from highest to lowest.\n",
    "    idx_sorted_decreasing = idx_sorted[::-1]\n",
    "    \n",
    "    # sort the eigen values by idx_sorted_decreasing\n",
    "    eigen_vals_sorted = eigen_vals[idx_sorted_decreasing]\n",
    "\n",
    "    # sort eigenvectors using the idx_sorted_decreasing indices\n",
    "    eigen_vecs_sorted = eigen_vecs[:,idx_sorted_decreasing]\n",
    "\n",
    "    # select the first n eigenvectors (n is desired dimension\n",
    "    # of rescaled data array, or dims_rescaled_data)\n",
    "    eigen_vecs_subset = eigen_vecs_sorted[:,0:n_components]\n",
    "\n",
    "    # transform the data by multiplying the transpose of the eigenvectors \n",
    "    # with the transpose of the de-meaned data\n",
    "    # Then take the transpose of that product.\n",
    "    X_reduced = np.dot(eigen_vecs_subset.transpose(),X_demeaned.transpose()).transpose()\n",
    "\n",
    "    return X_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_demeaned.shape:  (3, 10)\n",
      "Your original matrix was (3, 10) and it became:\n",
      "[[ 0.43437323  0.49820384]\n",
      " [ 0.42077249 -0.50351448]\n",
      " [-0.85514571  0.00531064]]\n"
     ]
    }
   ],
   "source": [
    "# Testi\n",
    "\n",
    "np.random.seed(1)\n",
    "X = np.random.rand(3, 10)\n",
    "\n",
    "X_reduced = compute_pca(X, n_components=2)\n",
    "print(\"Your original matrix was \" + str(X.shape) + \" and it became:\")\n",
    "print(X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_vectors(embeddings, words):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        embeddings: a word \n",
    "        fr_embeddings:\n",
    "        words: a list of words\n",
    "    Output: \n",
    "        X: a matrix where the rows are the embeddings corresponding to the rows on the list\n",
    "        \n",
    "    \"\"\"\n",
    "    m = len(words)\n",
    "    X = np.zeros((1, 300))\n",
    "    for word in words:\n",
    "        english = word\n",
    "        eng_emb = embeddings[english]\n",
    "        X = np.row_stack((X, eng_emb))\n",
    "    X = X[1:,:]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 11 words each of 300 dimensions thus X.shape is: (11, 300)\n"
     ]
    }
   ],
   "source": [
    "words = ['oil', 'gas', 'happy', 'sad', 'city', 'town',\n",
    "         'village', 'country', 'continent', 'petroleum', 'joyful']\n",
    "\n",
    "# given a list of words and the embeddings, it returns a matrix with all the embeddings\n",
    "X = get_vectors(word_embeddings, words)\n",
    "\n",
    "print('You have 11 words each of 300 dimensions thus X.shape is:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_demeaned.shape:  (11, 300)\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Created with matplotlib (https://matplotlib.org/) -->\n",
       "<svg height=\"252.093393pt\" version=\"1.1\" viewBox=\"0 0 412.339272 252.093393\" width=\"412.339272pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2021-06-18T18:12:12.302352</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.3.4, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M -0 252.093393 \n",
       "L 412.339272 252.093393 \n",
       "L 412.339272 0 \n",
       "L -0 0 \n",
       "z\n",
       "\" style=\"fill:none;\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 38.482813 228.215268 \n",
       "L 373.282813 228.215268 \n",
       "L 373.282813 10.775268 \n",
       "L 38.482813 10.775268 \n",
       "z\n",
       "\" style=\"fill:#ffffff;\"/>\n",
       "   </g>\n",
       "   <g id=\"PathCollection_1\">\n",
       "    <defs>\n",
       "     <path d=\"M 0 3 \n",
       "C 0.795609 3 1.55874 2.683901 2.12132 2.12132 \n",
       "C 2.683901 1.55874 3 0.795609 3 0 \n",
       "C 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \n",
       "C 1.55874 -2.683901 0.795609 -3 0 -3 \n",
       "C -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \n",
       "C -2.683901 -1.55874 -3 -0.795609 -3 0 \n",
       "C -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \n",
       "C -1.55874 2.683901 -0.795609 3 0 3 \n",
       "z\n",
       "\" id=\"medbf9646a9\" style=\"stroke:#1f77b4;\"/>\n",
       "    </defs>\n",
       "    <g clip-path=\"url(#p55c1b397eb)\">\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"337.169364\" xlink:href=\"#medbf9646a9\" y=\"110.570131\"/>\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"317.115494\" xlink:href=\"#medbf9646a9\" y=\"121.587571\"/>\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"67.313188\" xlink:href=\"#medbf9646a9\" y=\"49.485539\"/>\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"53.700994\" xlink:href=\"#medbf9646a9\" y=\"20.658904\"/>\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"103.769318\" xlink:href=\"#medbf9646a9\" y=\"187.916936\"/>\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"81.611173\" xlink:href=\"#medbf9646a9\" y=\"199.809049\"/>\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"69.860607\" xlink:href=\"#medbf9646a9\" y=\"218.331631\"/>\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"128.520495\" xlink:href=\"#medbf9646a9\" y=\"148.188417\"/>\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"137.882184\" xlink:href=\"#medbf9646a9\" y=\"157.599829\"/>\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"358.064631\" xlink:href=\"#medbf9646a9\" y=\"106.393369\"/>\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"63.853823\" xlink:href=\"#medbf9646a9\" y=\"41.78559\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" id=\"md47a36b18f\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"69.034195\" xlink:href=\"#md47a36b18f\" y=\"228.215268\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- −1.0 -->\n",
       "      <g transform=\"translate(56.892789 242.813705)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path d=\"M 10.59375 35.5 \n",
       "L 73.1875 35.5 \n",
       "L 73.1875 27.203125 \n",
       "L 10.59375 27.203125 \n",
       "z\n",
       "\" id=\"DejaVuSans-8722\"/>\n",
       "        <path d=\"M 12.40625 8.296875 \n",
       "L 28.515625 8.296875 \n",
       "L 28.515625 63.921875 \n",
       "L 10.984375 60.40625 \n",
       "L 10.984375 69.390625 \n",
       "L 28.421875 72.90625 \n",
       "L 38.28125 72.90625 \n",
       "L 38.28125 8.296875 \n",
       "L 54.390625 8.296875 \n",
       "L 54.390625 0 \n",
       "L 12.40625 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-49\"/>\n",
       "        <path d=\"M 10.6875 12.40625 \n",
       "L 21 12.40625 \n",
       "L 21 0 \n",
       "L 10.6875 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-46\"/>\n",
       "        <path d=\"M 31.78125 66.40625 \n",
       "Q 24.171875 66.40625 20.328125 58.90625 \n",
       "Q 16.5 51.421875 16.5 36.375 \n",
       "Q 16.5 21.390625 20.328125 13.890625 \n",
       "Q 24.171875 6.390625 31.78125 6.390625 \n",
       "Q 39.453125 6.390625 43.28125 13.890625 \n",
       "Q 47.125 21.390625 47.125 36.375 \n",
       "Q 47.125 51.421875 43.28125 58.90625 \n",
       "Q 39.453125 66.40625 31.78125 66.40625 \n",
       "z\n",
       "M 31.78125 74.21875 \n",
       "Q 44.046875 74.21875 50.515625 64.515625 \n",
       "Q 56.984375 54.828125 56.984375 36.375 \n",
       "Q 56.984375 17.96875 50.515625 8.265625 \n",
       "Q 44.046875 -1.421875 31.78125 -1.421875 \n",
       "Q 19.53125 -1.421875 13.0625 8.265625 \n",
       "Q 6.59375 17.96875 6.59375 36.375 \n",
       "Q 6.59375 54.828125 13.0625 64.515625 \n",
       "Q 19.53125 74.21875 31.78125 74.21875 \n",
       "z\n",
       "\" id=\"DejaVuSans-48\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-8722\"/>\n",
       "       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"112.647155\" xlink:href=\"#md47a36b18f\" y=\"228.215268\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- −0.5 -->\n",
       "      <g transform=\"translate(100.505749 242.813705)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path d=\"M 10.796875 72.90625 \n",
       "L 49.515625 72.90625 \n",
       "L 49.515625 64.59375 \n",
       "L 19.828125 64.59375 \n",
       "L 19.828125 46.734375 \n",
       "Q 21.96875 47.46875 24.109375 47.828125 \n",
       "Q 26.265625 48.1875 28.421875 48.1875 \n",
       "Q 40.625 48.1875 47.75 41.5 \n",
       "Q 54.890625 34.8125 54.890625 23.390625 \n",
       "Q 54.890625 11.625 47.5625 5.09375 \n",
       "Q 40.234375 -1.421875 26.90625 -1.421875 \n",
       "Q 22.3125 -1.421875 17.546875 -0.640625 \n",
       "Q 12.796875 0.140625 7.71875 1.703125 \n",
       "L 7.71875 11.625 \n",
       "Q 12.109375 9.234375 16.796875 8.0625 \n",
       "Q 21.484375 6.890625 26.703125 6.890625 \n",
       "Q 35.15625 6.890625 40.078125 11.328125 \n",
       "Q 45.015625 15.765625 45.015625 23.390625 \n",
       "Q 45.015625 31 40.078125 35.4375 \n",
       "Q 35.15625 39.890625 26.703125 39.890625 \n",
       "Q 22.75 39.890625 18.8125 39.015625 \n",
       "Q 14.890625 38.140625 10.796875 36.28125 \n",
       "z\n",
       "\" id=\"DejaVuSans-53\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-8722\"/>\n",
       "       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"156.260116\" xlink:href=\"#md47a36b18f\" y=\"228.215268\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 0.0 -->\n",
       "      <g transform=\"translate(148.308553 242.813705)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"199.873076\" xlink:href=\"#md47a36b18f\" y=\"228.215268\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 0.5 -->\n",
       "      <g transform=\"translate(191.921514 242.813705)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"243.486037\" xlink:href=\"#md47a36b18f\" y=\"228.215268\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 1.0 -->\n",
       "      <g transform=\"translate(235.534474 242.813705)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"287.098997\" xlink:href=\"#md47a36b18f\" y=\"228.215268\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 1.5 -->\n",
       "      <g transform=\"translate(279.147434 242.813705)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_7\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"330.711957\" xlink:href=\"#md47a36b18f\" y=\"228.215268\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 2.0 -->\n",
       "      <g transform=\"translate(322.760395 242.813705)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path d=\"M 19.1875 8.296875 \n",
       "L 53.609375 8.296875 \n",
       "L 53.609375 0 \n",
       "L 7.328125 0 \n",
       "L 7.328125 8.296875 \n",
       "Q 12.9375 14.109375 22.625 23.890625 \n",
       "Q 32.328125 33.6875 34.8125 36.53125 \n",
       "Q 39.546875 41.84375 41.421875 45.53125 \n",
       "Q 43.3125 49.21875 43.3125 52.78125 \n",
       "Q 43.3125 58.59375 39.234375 62.25 \n",
       "Q 35.15625 65.921875 28.609375 65.921875 \n",
       "Q 23.96875 65.921875 18.8125 64.3125 \n",
       "Q 13.671875 62.703125 7.8125 59.421875 \n",
       "L 7.8125 69.390625 \n",
       "Q 13.765625 71.78125 18.9375 73 \n",
       "Q 24.125 74.21875 28.421875 74.21875 \n",
       "Q 39.75 74.21875 46.484375 68.546875 \n",
       "Q 53.21875 62.890625 53.21875 53.421875 \n",
       "Q 53.21875 48.921875 51.53125 44.890625 \n",
       "Q 49.859375 40.875 45.40625 35.40625 \n",
       "Q 44.1875 33.984375 37.640625 27.21875 \n",
       "Q 31.109375 20.453125 19.1875 8.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-50\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" id=\"m04af88c520\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"38.482813\" xlink:href=\"#m04af88c520\" y=\"211.754904\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- −1.5 -->\n",
       "      <g transform=\"translate(7.2 215.554123)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-8722\"/>\n",
       "       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"38.482813\" xlink:href=\"#m04af88c520\" y=\"182.452572\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- −1.0 -->\n",
       "      <g transform=\"translate(7.2 186.25179)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-8722\"/>\n",
       "       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"38.482813\" xlink:href=\"#m04af88c520\" y=\"153.150239\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- −0.5 -->\n",
       "      <g transform=\"translate(7.2 156.949458)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-8722\"/>\n",
       "       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"38.482813\" xlink:href=\"#m04af88c520\" y=\"123.847906\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 0.0 -->\n",
       "      <g transform=\"translate(15.579688 127.647125)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"38.482813\" xlink:href=\"#m04af88c520\" y=\"94.545573\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 0.5 -->\n",
       "      <g transform=\"translate(15.579688 98.344792)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"38.482813\" xlink:href=\"#m04af88c520\" y=\"65.243241\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 1.0 -->\n",
       "      <g transform=\"translate(15.579688 69.042459)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_7\">\n",
       "     <g id=\"line2d_14\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"38.482813\" xlink:href=\"#m04af88c520\" y=\"35.940908\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_14\">\n",
       "      <!-- 1.5 -->\n",
       "      <g transform=\"translate(15.579688 39.740127)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 38.482813 228.215268 \n",
       "L 38.482813 10.775268 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 373.282813 228.215268 \n",
       "L 373.282813 10.775268 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 38.482812 228.215268 \n",
       "L 373.282812 228.215268 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 38.482812 10.775268 \n",
       "L 373.282812 10.775268 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"text_15\">\n",
       "    <!-- oil -->\n",
       "    <g transform=\"translate(332.808068 104.709664)scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path d=\"M 30.609375 48.390625 \n",
       "Q 23.390625 48.390625 19.1875 42.75 \n",
       "Q 14.984375 37.109375 14.984375 27.296875 \n",
       "Q 14.984375 17.484375 19.15625 11.84375 \n",
       "Q 23.34375 6.203125 30.609375 6.203125 \n",
       "Q 37.796875 6.203125 41.984375 11.859375 \n",
       "Q 46.1875 17.53125 46.1875 27.296875 \n",
       "Q 46.1875 37.015625 41.984375 42.703125 \n",
       "Q 37.796875 48.390625 30.609375 48.390625 \n",
       "z\n",
       "M 30.609375 56 \n",
       "Q 42.328125 56 49.015625 48.375 \n",
       "Q 55.71875 40.765625 55.71875 27.296875 \n",
       "Q 55.71875 13.875 49.015625 6.21875 \n",
       "Q 42.328125 -1.421875 30.609375 -1.421875 \n",
       "Q 18.84375 -1.421875 12.171875 6.21875 \n",
       "Q 5.515625 13.875 5.515625 27.296875 \n",
       "Q 5.515625 40.765625 12.171875 48.375 \n",
       "Q 18.84375 56 30.609375 56 \n",
       "z\n",
       "\" id=\"DejaVuSans-111\"/>\n",
       "      <path d=\"M 9.421875 54.6875 \n",
       "L 18.40625 54.6875 \n",
       "L 18.40625 0 \n",
       "L 9.421875 0 \n",
       "z\n",
       "M 9.421875 75.984375 \n",
       "L 18.40625 75.984375 \n",
       "L 18.40625 64.59375 \n",
       "L 9.421875 64.59375 \n",
       "z\n",
       "\" id=\"DejaVuSans-105\"/>\n",
       "      <path d=\"M 9.421875 75.984375 \n",
       "L 18.40625 75.984375 \n",
       "L 18.40625 0 \n",
       "L 9.421875 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-108\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-111\"/>\n",
       "     <use x=\"61.181641\" xlink:href=\"#DejaVuSans-105\"/>\n",
       "     <use x=\"88.964844\" xlink:href=\"#DejaVuSans-108\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_16\">\n",
       "    <!-- gas -->\n",
       "    <g transform=\"translate(312.754198 115.727104)scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path d=\"M 45.40625 27.984375 \n",
       "Q 45.40625 37.75 41.375 43.109375 \n",
       "Q 37.359375 48.484375 30.078125 48.484375 \n",
       "Q 22.859375 48.484375 18.828125 43.109375 \n",
       "Q 14.796875 37.75 14.796875 27.984375 \n",
       "Q 14.796875 18.265625 18.828125 12.890625 \n",
       "Q 22.859375 7.515625 30.078125 7.515625 \n",
       "Q 37.359375 7.515625 41.375 12.890625 \n",
       "Q 45.40625 18.265625 45.40625 27.984375 \n",
       "z\n",
       "M 54.390625 6.78125 \n",
       "Q 54.390625 -7.171875 48.1875 -13.984375 \n",
       "Q 42 -20.796875 29.203125 -20.796875 \n",
       "Q 24.46875 -20.796875 20.265625 -20.09375 \n",
       "Q 16.0625 -19.390625 12.109375 -17.921875 \n",
       "L 12.109375 -9.1875 \n",
       "Q 16.0625 -11.328125 19.921875 -12.34375 \n",
       "Q 23.78125 -13.375 27.78125 -13.375 \n",
       "Q 36.625 -13.375 41.015625 -8.765625 \n",
       "Q 45.40625 -4.15625 45.40625 5.171875 \n",
       "L 45.40625 9.625 \n",
       "Q 42.625 4.78125 38.28125 2.390625 \n",
       "Q 33.9375 0 27.875 0 \n",
       "Q 17.828125 0 11.671875 7.65625 \n",
       "Q 5.515625 15.328125 5.515625 27.984375 \n",
       "Q 5.515625 40.671875 11.671875 48.328125 \n",
       "Q 17.828125 56 27.875 56 \n",
       "Q 33.9375 56 38.28125 53.609375 \n",
       "Q 42.625 51.21875 45.40625 46.390625 \n",
       "L 45.40625 54.6875 \n",
       "L 54.390625 54.6875 \n",
       "z\n",
       "\" id=\"DejaVuSans-103\"/>\n",
       "      <path d=\"M 34.28125 27.484375 \n",
       "Q 23.390625 27.484375 19.1875 25 \n",
       "Q 14.984375 22.515625 14.984375 16.5 \n",
       "Q 14.984375 11.71875 18.140625 8.90625 \n",
       "Q 21.296875 6.109375 26.703125 6.109375 \n",
       "Q 34.1875 6.109375 38.703125 11.40625 \n",
       "Q 43.21875 16.703125 43.21875 25.484375 \n",
       "L 43.21875 27.484375 \n",
       "z\n",
       "M 52.203125 31.203125 \n",
       "L 52.203125 0 \n",
       "L 43.21875 0 \n",
       "L 43.21875 8.296875 \n",
       "Q 40.140625 3.328125 35.546875 0.953125 \n",
       "Q 30.953125 -1.421875 24.3125 -1.421875 \n",
       "Q 15.921875 -1.421875 10.953125 3.296875 \n",
       "Q 6 8.015625 6 15.921875 \n",
       "Q 6 25.140625 12.171875 29.828125 \n",
       "Q 18.359375 34.515625 30.609375 34.515625 \n",
       "L 43.21875 34.515625 \n",
       "L 43.21875 35.40625 \n",
       "Q 43.21875 41.609375 39.140625 45 \n",
       "Q 35.0625 48.390625 27.6875 48.390625 \n",
       "Q 23 48.390625 18.546875 47.265625 \n",
       "Q 14.109375 46.140625 10.015625 43.890625 \n",
       "L 10.015625 52.203125 \n",
       "Q 14.9375 54.109375 19.578125 55.046875 \n",
       "Q 24.21875 56 28.609375 56 \n",
       "Q 40.484375 56 46.34375 49.84375 \n",
       "Q 52.203125 43.703125 52.203125 31.203125 \n",
       "z\n",
       "\" id=\"DejaVuSans-97\"/>\n",
       "      <path d=\"M 44.28125 53.078125 \n",
       "L 44.28125 44.578125 \n",
       "Q 40.484375 46.53125 36.375 47.5 \n",
       "Q 32.28125 48.484375 27.875 48.484375 \n",
       "Q 21.1875 48.484375 17.84375 46.4375 \n",
       "Q 14.5 44.390625 14.5 40.28125 \n",
       "Q 14.5 37.15625 16.890625 35.375 \n",
       "Q 19.28125 33.59375 26.515625 31.984375 \n",
       "L 29.59375 31.296875 \n",
       "Q 39.15625 29.25 43.1875 25.515625 \n",
       "Q 47.21875 21.78125 47.21875 15.09375 \n",
       "Q 47.21875 7.46875 41.1875 3.015625 \n",
       "Q 35.15625 -1.421875 24.609375 -1.421875 \n",
       "Q 20.21875 -1.421875 15.453125 -0.5625 \n",
       "Q 10.6875 0.296875 5.421875 2 \n",
       "L 5.421875 11.28125 \n",
       "Q 10.40625 8.6875 15.234375 7.390625 \n",
       "Q 20.0625 6.109375 24.8125 6.109375 \n",
       "Q 31.15625 6.109375 34.5625 8.28125 \n",
       "Q 37.984375 10.453125 37.984375 14.40625 \n",
       "Q 37.984375 18.0625 35.515625 20.015625 \n",
       "Q 33.0625 21.96875 24.703125 23.78125 \n",
       "L 21.578125 24.515625 \n",
       "Q 13.234375 26.265625 9.515625 29.90625 \n",
       "Q 5.8125 33.546875 5.8125 39.890625 \n",
       "Q 5.8125 47.609375 11.28125 51.796875 \n",
       "Q 16.75 56 26.8125 56 \n",
       "Q 31.78125 56 36.171875 55.265625 \n",
       "Q 40.578125 54.546875 44.28125 53.078125 \n",
       "z\n",
       "\" id=\"DejaVuSans-115\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-103\"/>\n",
       "     <use x=\"63.476562\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "     <use x=\"124.755859\" xlink:href=\"#DejaVuSans-115\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_17\">\n",
       "    <!-- happy -->\n",
       "    <g transform=\"translate(62.951892 43.625072)scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path d=\"M 54.890625 33.015625 \n",
       "L 54.890625 0 \n",
       "L 45.90625 0 \n",
       "L 45.90625 32.71875 \n",
       "Q 45.90625 40.484375 42.875 44.328125 \n",
       "Q 39.84375 48.1875 33.796875 48.1875 \n",
       "Q 26.515625 48.1875 22.3125 43.546875 \n",
       "Q 18.109375 38.921875 18.109375 30.90625 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "L 9.078125 75.984375 \n",
       "L 18.109375 75.984375 \n",
       "L 18.109375 46.1875 \n",
       "Q 21.34375 51.125 25.703125 53.5625 \n",
       "Q 30.078125 56 35.796875 56 \n",
       "Q 45.21875 56 50.046875 50.171875 \n",
       "Q 54.890625 44.34375 54.890625 33.015625 \n",
       "z\n",
       "\" id=\"DejaVuSans-104\"/>\n",
       "      <path d=\"M 18.109375 8.203125 \n",
       "L 18.109375 -20.796875 \n",
       "L 9.078125 -20.796875 \n",
       "L 9.078125 54.6875 \n",
       "L 18.109375 54.6875 \n",
       "L 18.109375 46.390625 \n",
       "Q 20.953125 51.265625 25.265625 53.625 \n",
       "Q 29.59375 56 35.59375 56 \n",
       "Q 45.5625 56 51.78125 48.09375 \n",
       "Q 58.015625 40.1875 58.015625 27.296875 \n",
       "Q 58.015625 14.40625 51.78125 6.484375 \n",
       "Q 45.5625 -1.421875 35.59375 -1.421875 \n",
       "Q 29.59375 -1.421875 25.265625 0.953125 \n",
       "Q 20.953125 3.328125 18.109375 8.203125 \n",
       "z\n",
       "M 48.6875 27.296875 \n",
       "Q 48.6875 37.203125 44.609375 42.84375 \n",
       "Q 40.53125 48.484375 33.40625 48.484375 \n",
       "Q 26.265625 48.484375 22.1875 42.84375 \n",
       "Q 18.109375 37.203125 18.109375 27.296875 \n",
       "Q 18.109375 17.390625 22.1875 11.75 \n",
       "Q 26.265625 6.109375 33.40625 6.109375 \n",
       "Q 40.53125 6.109375 44.609375 11.75 \n",
       "Q 48.6875 17.390625 48.6875 27.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-112\"/>\n",
       "      <path d=\"M 32.171875 -5.078125 \n",
       "Q 28.375 -14.84375 24.75 -17.8125 \n",
       "Q 21.140625 -20.796875 15.09375 -20.796875 \n",
       "L 7.90625 -20.796875 \n",
       "L 7.90625 -13.28125 \n",
       "L 13.1875 -13.28125 \n",
       "Q 16.890625 -13.28125 18.9375 -11.515625 \n",
       "Q 21 -9.765625 23.484375 -3.21875 \n",
       "L 25.09375 0.875 \n",
       "L 2.984375 54.6875 \n",
       "L 12.5 54.6875 \n",
       "L 29.59375 11.921875 \n",
       "L 46.6875 54.6875 \n",
       "L 56.203125 54.6875 \n",
       "z\n",
       "\" id=\"DejaVuSans-121\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-104\"/>\n",
       "     <use x=\"63.378906\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "     <use x=\"124.658203\" xlink:href=\"#DejaVuSans-112\"/>\n",
       "     <use x=\"188.134766\" xlink:href=\"#DejaVuSans-112\"/>\n",
       "     <use x=\"251.611328\" xlink:href=\"#DejaVuSans-121\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_18\">\n",
       "    <!-- sad -->\n",
       "    <g transform=\"translate(49.339698 14.798438)scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path d=\"M 45.40625 46.390625 \n",
       "L 45.40625 75.984375 \n",
       "L 54.390625 75.984375 \n",
       "L 54.390625 0 \n",
       "L 45.40625 0 \n",
       "L 45.40625 8.203125 \n",
       "Q 42.578125 3.328125 38.25 0.953125 \n",
       "Q 33.9375 -1.421875 27.875 -1.421875 \n",
       "Q 17.96875 -1.421875 11.734375 6.484375 \n",
       "Q 5.515625 14.40625 5.515625 27.296875 \n",
       "Q 5.515625 40.1875 11.734375 48.09375 \n",
       "Q 17.96875 56 27.875 56 \n",
       "Q 33.9375 56 38.25 53.625 \n",
       "Q 42.578125 51.265625 45.40625 46.390625 \n",
       "z\n",
       "M 14.796875 27.296875 \n",
       "Q 14.796875 17.390625 18.875 11.75 \n",
       "Q 22.953125 6.109375 30.078125 6.109375 \n",
       "Q 37.203125 6.109375 41.296875 11.75 \n",
       "Q 45.40625 17.390625 45.40625 27.296875 \n",
       "Q 45.40625 37.203125 41.296875 42.84375 \n",
       "Q 37.203125 48.484375 30.078125 48.484375 \n",
       "Q 22.953125 48.484375 18.875 42.84375 \n",
       "Q 14.796875 37.203125 14.796875 27.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-100\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-115\"/>\n",
       "     <use x=\"52.099609\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "     <use x=\"113.378906\" xlink:href=\"#DejaVuSans-100\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_19\">\n",
       "    <!-- city -->\n",
       "    <g transform=\"translate(99.408022 182.05647)scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path d=\"M 48.78125 52.59375 \n",
       "L 48.78125 44.1875 \n",
       "Q 44.96875 46.296875 41.140625 47.34375 \n",
       "Q 37.3125 48.390625 33.40625 48.390625 \n",
       "Q 24.65625 48.390625 19.8125 42.84375 \n",
       "Q 14.984375 37.3125 14.984375 27.296875 \n",
       "Q 14.984375 17.28125 19.8125 11.734375 \n",
       "Q 24.65625 6.203125 33.40625 6.203125 \n",
       "Q 37.3125 6.203125 41.140625 7.25 \n",
       "Q 44.96875 8.296875 48.78125 10.40625 \n",
       "L 48.78125 2.09375 \n",
       "Q 45.015625 0.34375 40.984375 -0.53125 \n",
       "Q 36.96875 -1.421875 32.421875 -1.421875 \n",
       "Q 20.0625 -1.421875 12.78125 6.34375 \n",
       "Q 5.515625 14.109375 5.515625 27.296875 \n",
       "Q 5.515625 40.671875 12.859375 48.328125 \n",
       "Q 20.21875 56 33.015625 56 \n",
       "Q 37.15625 56 41.109375 55.140625 \n",
       "Q 45.0625 54.296875 48.78125 52.59375 \n",
       "z\n",
       "\" id=\"DejaVuSans-99\"/>\n",
       "      <path d=\"M 18.3125 70.21875 \n",
       "L 18.3125 54.6875 \n",
       "L 36.8125 54.6875 \n",
       "L 36.8125 47.703125 \n",
       "L 18.3125 47.703125 \n",
       "L 18.3125 18.015625 \n",
       "Q 18.3125 11.328125 20.140625 9.421875 \n",
       "Q 21.96875 7.515625 27.59375 7.515625 \n",
       "L 36.8125 7.515625 \n",
       "L 36.8125 0 \n",
       "L 27.59375 0 \n",
       "Q 17.1875 0 13.234375 3.875 \n",
       "Q 9.28125 7.765625 9.28125 18.015625 \n",
       "L 9.28125 47.703125 \n",
       "L 2.6875 47.703125 \n",
       "L 2.6875 54.6875 \n",
       "L 9.28125 54.6875 \n",
       "L 9.28125 70.21875 \n",
       "z\n",
       "\" id=\"DejaVuSans-116\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-99\"/>\n",
       "     <use x=\"54.980469\" xlink:href=\"#DejaVuSans-105\"/>\n",
       "     <use x=\"82.763672\" xlink:href=\"#DejaVuSans-116\"/>\n",
       "     <use x=\"121.972656\" xlink:href=\"#DejaVuSans-121\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_20\">\n",
       "    <!-- town -->\n",
       "    <g transform=\"translate(77.249877 193.948582)scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path d=\"M 4.203125 54.6875 \n",
       "L 13.1875 54.6875 \n",
       "L 24.421875 12.015625 \n",
       "L 35.59375 54.6875 \n",
       "L 46.1875 54.6875 \n",
       "L 57.421875 12.015625 \n",
       "L 68.609375 54.6875 \n",
       "L 77.59375 54.6875 \n",
       "L 63.28125 0 \n",
       "L 52.6875 0 \n",
       "L 40.921875 44.828125 \n",
       "L 29.109375 0 \n",
       "L 18.5 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-119\"/>\n",
       "      <path d=\"M 54.890625 33.015625 \n",
       "L 54.890625 0 \n",
       "L 45.90625 0 \n",
       "L 45.90625 32.71875 \n",
       "Q 45.90625 40.484375 42.875 44.328125 \n",
       "Q 39.84375 48.1875 33.796875 48.1875 \n",
       "Q 26.515625 48.1875 22.3125 43.546875 \n",
       "Q 18.109375 38.921875 18.109375 30.90625 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "L 9.078125 54.6875 \n",
       "L 18.109375 54.6875 \n",
       "L 18.109375 46.1875 \n",
       "Q 21.34375 51.125 25.703125 53.5625 \n",
       "Q 30.078125 56 35.796875 56 \n",
       "Q 45.21875 56 50.046875 50.171875 \n",
       "Q 54.890625 44.34375 54.890625 33.015625 \n",
       "z\n",
       "\" id=\"DejaVuSans-110\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-116\"/>\n",
       "     <use x=\"39.208984\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "     <use x=\"100.390625\" xlink:href=\"#DejaVuSans-119\"/>\n",
       "     <use x=\"182.177734\" xlink:href=\"#DejaVuSans-110\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_21\">\n",
       "    <!-- village -->\n",
       "    <g transform=\"translate(65.499311 212.471165)scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path d=\"M 2.984375 54.6875 \n",
       "L 12.5 54.6875 \n",
       "L 29.59375 8.796875 \n",
       "L 46.6875 54.6875 \n",
       "L 56.203125 54.6875 \n",
       "L 35.6875 0 \n",
       "L 23.484375 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-118\"/>\n",
       "      <path d=\"M 56.203125 29.59375 \n",
       "L 56.203125 25.203125 \n",
       "L 14.890625 25.203125 \n",
       "Q 15.484375 15.921875 20.484375 11.0625 \n",
       "Q 25.484375 6.203125 34.421875 6.203125 \n",
       "Q 39.59375 6.203125 44.453125 7.46875 \n",
       "Q 49.3125 8.734375 54.109375 11.28125 \n",
       "L 54.109375 2.78125 \n",
       "Q 49.265625 0.734375 44.1875 -0.34375 \n",
       "Q 39.109375 -1.421875 33.890625 -1.421875 \n",
       "Q 20.796875 -1.421875 13.15625 6.1875 \n",
       "Q 5.515625 13.8125 5.515625 26.8125 \n",
       "Q 5.515625 40.234375 12.765625 48.109375 \n",
       "Q 20.015625 56 32.328125 56 \n",
       "Q 43.359375 56 49.78125 48.890625 \n",
       "Q 56.203125 41.796875 56.203125 29.59375 \n",
       "z\n",
       "M 47.21875 32.234375 \n",
       "Q 47.125 39.59375 43.09375 43.984375 \n",
       "Q 39.0625 48.390625 32.421875 48.390625 \n",
       "Q 24.90625 48.390625 20.390625 44.140625 \n",
       "Q 15.875 39.890625 15.1875 32.171875 \n",
       "z\n",
       "\" id=\"DejaVuSans-101\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-118\"/>\n",
       "     <use x=\"59.179688\" xlink:href=\"#DejaVuSans-105\"/>\n",
       "     <use x=\"86.962891\" xlink:href=\"#DejaVuSans-108\"/>\n",
       "     <use x=\"114.746094\" xlink:href=\"#DejaVuSans-108\"/>\n",
       "     <use x=\"142.529297\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "     <use x=\"203.808594\" xlink:href=\"#DejaVuSans-103\"/>\n",
       "     <use x=\"267.285156\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_22\">\n",
       "    <!-- country -->\n",
       "    <g transform=\"translate(124.159199 142.327951)scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path d=\"M 8.5 21.578125 \n",
       "L 8.5 54.6875 \n",
       "L 17.484375 54.6875 \n",
       "L 17.484375 21.921875 \n",
       "Q 17.484375 14.15625 20.5 10.265625 \n",
       "Q 23.53125 6.390625 29.59375 6.390625 \n",
       "Q 36.859375 6.390625 41.078125 11.03125 \n",
       "Q 45.3125 15.671875 45.3125 23.6875 \n",
       "L 45.3125 54.6875 \n",
       "L 54.296875 54.6875 \n",
       "L 54.296875 0 \n",
       "L 45.3125 0 \n",
       "L 45.3125 8.40625 \n",
       "Q 42.046875 3.421875 37.71875 1 \n",
       "Q 33.40625 -1.421875 27.6875 -1.421875 \n",
       "Q 18.265625 -1.421875 13.375 4.4375 \n",
       "Q 8.5 10.296875 8.5 21.578125 \n",
       "z\n",
       "M 31.109375 56 \n",
       "z\n",
       "\" id=\"DejaVuSans-117\"/>\n",
       "      <path d=\"M 41.109375 46.296875 \n",
       "Q 39.59375 47.171875 37.8125 47.578125 \n",
       "Q 36.03125 48 33.890625 48 \n",
       "Q 26.265625 48 22.1875 43.046875 \n",
       "Q 18.109375 38.09375 18.109375 28.8125 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "L 9.078125 54.6875 \n",
       "L 18.109375 54.6875 \n",
       "L 18.109375 46.1875 \n",
       "Q 20.953125 51.171875 25.484375 53.578125 \n",
       "Q 30.03125 56 36.53125 56 \n",
       "Q 37.453125 56 38.578125 55.875 \n",
       "Q 39.703125 55.765625 41.0625 55.515625 \n",
       "z\n",
       "\" id=\"DejaVuSans-114\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-99\"/>\n",
       "     <use x=\"54.980469\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "     <use x=\"116.162109\" xlink:href=\"#DejaVuSans-117\"/>\n",
       "     <use x=\"179.541016\" xlink:href=\"#DejaVuSans-110\"/>\n",
       "     <use x=\"242.919922\" xlink:href=\"#DejaVuSans-116\"/>\n",
       "     <use x=\"282.128906\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "     <use x=\"323.242188\" xlink:href=\"#DejaVuSans-121\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_23\">\n",
       "    <!-- continent -->\n",
       "    <g transform=\"translate(133.520888 151.739363)scale(0.1 -0.1)\">\n",
       "     <use xlink:href=\"#DejaVuSans-99\"/>\n",
       "     <use x=\"54.980469\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "     <use x=\"116.162109\" xlink:href=\"#DejaVuSans-110\"/>\n",
       "     <use x=\"179.541016\" xlink:href=\"#DejaVuSans-116\"/>\n",
       "     <use x=\"218.75\" xlink:href=\"#DejaVuSans-105\"/>\n",
       "     <use x=\"246.533203\" xlink:href=\"#DejaVuSans-110\"/>\n",
       "     <use x=\"309.912109\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "     <use x=\"371.435547\" xlink:href=\"#DejaVuSans-110\"/>\n",
       "     <use x=\"434.814453\" xlink:href=\"#DejaVuSans-116\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_24\">\n",
       "    <!-- petroleum -->\n",
       "    <g transform=\"translate(353.703335 100.532902)scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path d=\"M 52 44.1875 \n",
       "Q 55.375 50.25 60.0625 53.125 \n",
       "Q 64.75 56 71.09375 56 \n",
       "Q 79.640625 56 84.28125 50.015625 \n",
       "Q 88.921875 44.046875 88.921875 33.015625 \n",
       "L 88.921875 0 \n",
       "L 79.890625 0 \n",
       "L 79.890625 32.71875 \n",
       "Q 79.890625 40.578125 77.09375 44.375 \n",
       "Q 74.3125 48.1875 68.609375 48.1875 \n",
       "Q 61.625 48.1875 57.5625 43.546875 \n",
       "Q 53.515625 38.921875 53.515625 30.90625 \n",
       "L 53.515625 0 \n",
       "L 44.484375 0 \n",
       "L 44.484375 32.71875 \n",
       "Q 44.484375 40.625 41.703125 44.40625 \n",
       "Q 38.921875 48.1875 33.109375 48.1875 \n",
       "Q 26.21875 48.1875 22.15625 43.53125 \n",
       "Q 18.109375 38.875 18.109375 30.90625 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "L 9.078125 54.6875 \n",
       "L 18.109375 54.6875 \n",
       "L 18.109375 46.1875 \n",
       "Q 21.1875 51.21875 25.484375 53.609375 \n",
       "Q 29.78125 56 35.6875 56 \n",
       "Q 41.65625 56 45.828125 52.96875 \n",
       "Q 50 49.953125 52 44.1875 \n",
       "z\n",
       "\" id=\"DejaVuSans-109\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-112\"/>\n",
       "     <use x=\"63.476562\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "     <use x=\"125\" xlink:href=\"#DejaVuSans-116\"/>\n",
       "     <use x=\"164.208984\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "     <use x=\"203.072266\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "     <use x=\"264.253906\" xlink:href=\"#DejaVuSans-108\"/>\n",
       "     <use x=\"292.037109\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "     <use x=\"353.560547\" xlink:href=\"#DejaVuSans-117\"/>\n",
       "     <use x=\"416.939453\" xlink:href=\"#DejaVuSans-109\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_25\">\n",
       "    <!-- joyful -->\n",
       "    <g transform=\"translate(59.492527 35.925124)scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path d=\"M 9.421875 54.6875 \n",
       "L 18.40625 54.6875 \n",
       "L 18.40625 -0.984375 \n",
       "Q 18.40625 -11.421875 14.421875 -16.109375 \n",
       "Q 10.453125 -20.796875 1.609375 -20.796875 \n",
       "L -1.8125 -20.796875 \n",
       "L -1.8125 -13.1875 \n",
       "L 0.59375 -13.1875 \n",
       "Q 5.71875 -13.1875 7.5625 -10.8125 \n",
       "Q 9.421875 -8.453125 9.421875 -0.984375 \n",
       "z\n",
       "M 9.421875 75.984375 \n",
       "L 18.40625 75.984375 \n",
       "L 18.40625 64.59375 \n",
       "L 9.421875 64.59375 \n",
       "z\n",
       "\" id=\"DejaVuSans-106\"/>\n",
       "      <path d=\"M 37.109375 75.984375 \n",
       "L 37.109375 68.5 \n",
       "L 28.515625 68.5 \n",
       "Q 23.6875 68.5 21.796875 66.546875 \n",
       "Q 19.921875 64.59375 19.921875 59.515625 \n",
       "L 19.921875 54.6875 \n",
       "L 34.71875 54.6875 \n",
       "L 34.71875 47.703125 \n",
       "L 19.921875 47.703125 \n",
       "L 19.921875 0 \n",
       "L 10.890625 0 \n",
       "L 10.890625 47.703125 \n",
       "L 2.296875 47.703125 \n",
       "L 2.296875 54.6875 \n",
       "L 10.890625 54.6875 \n",
       "L 10.890625 58.5 \n",
       "Q 10.890625 67.625 15.140625 71.796875 \n",
       "Q 19.390625 75.984375 28.609375 75.984375 \n",
       "z\n",
       "\" id=\"DejaVuSans-102\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-106\"/>\n",
       "     <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "     <use x=\"88.964844\" xlink:href=\"#DejaVuSans-121\"/>\n",
       "     <use x=\"148.144531\" xlink:href=\"#DejaVuSans-102\"/>\n",
       "     <use x=\"183.349609\" xlink:href=\"#DejaVuSans-117\"/>\n",
       "     <use x=\"246.728516\" xlink:href=\"#DejaVuSans-108\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p55c1b397eb\">\n",
       "   <rect height=\"217.44\" width=\"334.8\" x=\"38.482813\" y=\"10.775268\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We have done the plotting for you. Just run this cell.\n",
    "result = compute_pca(X, 2)\n",
    "plt.scatter(result[:, 0], result[:, 1])\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, xy=(result[i, 0] - 0.05, result[i, 1] + 0.1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_embeddings_subset=pickle.load(open(\"en_embeddings.p\",\"rb\"))\n",
    "fr_embeddings_subset=pickle.load(open(\"fr_embeddings.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dict(file_name):\n",
    "    my_file=pd.read_csv(file_name,delimiter=' ')\n",
    "    \n",
    "    etof={}\n",
    "    \n",
    "    for i in range(len(my_file)):\n",
    "        en=my_file.iloc[i][0]\n",
    "        fr=my_file.iloc[i][1]\n",
    "        etof[en] = fr\n",
    "        \n",
    "    return etof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the English to French training dictionary is 5000\n",
      "The length of the English to French test dictionary is 5000\n"
     ]
    }
   ],
   "source": [
    "en_fr_train = get_dict('en-fr.train.txt')\n",
    "print('The length of the English to French training dictionary is', len(en_fr_train))\n",
    "en_fr_test = get_dict('en-fr.test.txt')\n",
    "print('The length of the English to French test dictionary is', len(en_fr_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_matrices(en_fr,french_vecs,english_vecs):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        en_fr: English to French dictionary\n",
    "        french_vecs: French words to their corresponding word embeddings.\n",
    "        english_vecs: English words to their corresponding word embeddings.\n",
    "    Output: \n",
    "        X: a matrix where the columns are the English embeddings.\n",
    "        Y: a matrix where the columns correspong to the French embeddings.\n",
    "        R: the projection matrix that minimizes the F norm ||X R -Y||^2.\n",
    "    \"\"\"\n",
    "    \n",
    "    # X_l and Y_l are lists of the english and french word embeddings\n",
    "    X_l=list()\n",
    "    Y_l=list()\n",
    "    \n",
    "    # get the english words (the keys in the dictionary) and store in a set()\n",
    "    english_set=english_vecs.keys()\n",
    "    \n",
    "    # get the french words (the keys in the dictionary) and store in a set()\n",
    "    french_set=french_vecs.keys()\n",
    "    \n",
    "    # store the french words that are part of the english-french dictionary\n",
    "    french_words=set(en_fr.values())\n",
    "    \n",
    "    for en_word,fr_word in en_fr.items():\n",
    "        if fr_word in french_set and en_word in english_set:\n",
    "            en_vec=english_vecs[en_word]\n",
    "            \n",
    "            fr_vec=french_vecs[fr_word]\n",
    "            \n",
    "            X_l.append(en_vec)\n",
    "            \n",
    "            Y_l.append(fr_vec)\n",
    "    \n",
    "    X=np.vstack(X_l)\n",
    "    \n",
    "    Y=np.vstack(Y_l)\n",
    "    \n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, Y_train = get_matrices(\n",
    "    en_fr_train, fr_embeddings_subset, en_embeddings_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_loss(X,Y,R):\n",
    "    '''\n",
    "    Inputs: \n",
    "        X: a matrix of dimension (m,n) where the columns are the English embeddings.\n",
    "        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.\n",
    "        R: a matrix of dimension (n,n) - transformation matrix from English to French vector space embeddings.\n",
    "    Outputs:\n",
    "        L: a matrix of dimension (m,n) - the value of the loss function for given X, Y and R.\n",
    "    '''\n",
    "    \n",
    "    m=X.shape[0]\n",
    "    \n",
    "    diff=np.dot(X,R)-Y\n",
    "    \n",
    "    diff_squared=diff**2\n",
    "    \n",
    "    sum_diff_squared=np.sum(diff_squared)\n",
    "    \n",
    "    loss=sum_diff_squared/m\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_gradient(X,Y,R):\n",
    "    '''\n",
    "    Inputs: \n",
    "        X: a matrix of dimension (m,n) where the columns are the English embeddings.\n",
    "        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.\n",
    "        R: a matrix of dimension (n,n) - transformation matrix from English to French vector space embeddings.\n",
    "    Outputs:\n",
    "        g: a matrix of dimension (n,n) - gradient of the loss function L for given X, Y and R.\n",
    "    '''\n",
    "    m=X.shape[0]\n",
    "    \n",
    "    gradient=np.dot(X.T,np.dot(X,R)-Y)*(2/m)\n",
    "    \n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def align_embeddings(X,Y,train_steps=100,learning_rate=0.0003):\n",
    "    np.random.seed(129)\n",
    "    \n",
    "    R=np.random.rand(X.shape[1],X.shape[1])\n",
    "    \n",
    "    for i in range(train_steps):\n",
    "        if i%25 ==0:\n",
    "            print(\"loss at iteration {} is {:.4f}\".format(i,compute_loss(X,Y,R)))\n",
    "        \n",
    "        gradient=compute_gradient(X,Y,R)\n",
    "        \n",
    "        R-=learning_rate*gradient\n",
    "        \n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iteration 0 is 3.7242\n",
      "loss at iteration 25 is 3.6283\n",
      "loss at iteration 50 is 3.5350\n",
      "loss at iteration 75 is 3.4442\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "np.random.seed(129)\n",
    "m = 10\n",
    "n = 5\n",
    "X = np.random.rand(m, n)\n",
    "Y = np.random.rand(m, n) * .1\n",
    "R = align_embeddings(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iteration 0 is 963.0146\n",
      "loss at iteration 25 is 97.8292\n",
      "loss at iteration 50 is 26.8329\n",
      "loss at iteration 75 is 9.7893\n",
      "loss at iteration 100 is 4.3776\n",
      "loss at iteration 125 is 2.3281\n",
      "loss at iteration 150 is 1.4480\n",
      "loss at iteration 175 is 1.0338\n",
      "loss at iteration 200 is 0.8251\n",
      "loss at iteration 225 is 0.7145\n",
      "loss at iteration 250 is 0.6534\n",
      "loss at iteration 275 is 0.6185\n",
      "loss at iteration 300 is 0.5981\n",
      "loss at iteration 325 is 0.5858\n",
      "loss at iteration 350 is 0.5782\n",
      "loss at iteration 375 is 0.5735\n"
     ]
    }
   ],
   "source": [
    "R_train = align_embeddings(X_train, Y_train, train_steps=400, learning_rate=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(A,B):\n",
    "    cos=0\n",
    "    \n",
    "    dot=np.dot(A,B)\n",
    "    norma=np.linalg.norm(A)\n",
    "    normb=np.linalg.norm(B)\n",
    "    \n",
    "    return dot/(norma*normb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def knn(v,candidates,k=1):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "      - v, the vector you are going find the nearest neighbor for\n",
    "      - candidates: a set of vectors where we will find the neighbors\n",
    "      - k: top k nearest neighbors to find\n",
    "    Output:\n",
    "      - k_idx: the indices of the top k closest vectors in sorted form\n",
    "    \"\"\"\n",
    "    \n",
    "    similarity_l=[]\n",
    "    \n",
    "    for row in candidates:\n",
    "        cos_similarity=cosine_similarity(v,row)\n",
    "        similarity_l.append(cos_similarity)\n",
    "        \n",
    "    sorted_ids=np.argsort(similarity_l)\n",
    "    \n",
    "    k_idx=sorted_ids[-k:]\n",
    "    \n",
    "    return k_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9 9 9]\n",
      " [1 0 5]\n",
      " [2 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "v = np.array([1, 0, 1])\n",
    "candidates = np.array([[1, 0, 5], [-2, 5, 3], [2, 0, 1], [6, -9, 5], [9, 9, 9]])\n",
    "print(candidates[knn(v, candidates, 3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_vocabulary(X,Y,R):\n",
    "    '''\n",
    "    Input:\n",
    "        X: a matrix where the columns are the English embeddings.\n",
    "        Y: a matrix where the columns correspong to the French embeddings.\n",
    "        R: the transform matrix which translates word embeddings from\n",
    "        English to French word vector space.\n",
    "    Output:\n",
    "        accuracy: for the English to French capitals\n",
    "    '''\n",
    "    pred=np.dot(X,R)\n",
    "    num_correct=0\n",
    "    \n",
    "    for i in range(len(pred)):\n",
    "        pred_idx=knn(pred[i],Y)\n",
    "        \n",
    "        if pred_idx==i:\n",
    "            num_correct+=1\n",
    "    \n",
    "    accuracy=num_correct/len(pred)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_val, Y_val = get_matrices(en_fr_test, fr_embeddings_subset, en_embeddings_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test set is 0.557\n"
     ]
    }
   ],
   "source": [
    "acc = test_vocabulary(X_val, Y_val, R_train)  # this might take a minute or two\n",
    "print(f\"accuracy on test set is {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LSH and document seach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "all_tweets = all_positive_tweets + all_negative_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Getting the document embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_document_embedding(tweet,en_embeddings):    \n",
    "    '''\n",
    "    Input:\n",
    "        - tweet: a string\n",
    "        - en_embeddings: a dictionary of word embeddings\n",
    "    Output:\n",
    "        - tweet_embedding: a\n",
    "    '''\n",
    "    \n",
    "    doc_embedding=np.zeros(300)\n",
    "    \n",
    "    processed_doc=process_tweet(tweet)\n",
    "    \n",
    "    for word in processed_doc:\n",
    "        doc_embedding+=en_embeddings.get(word,0)\n",
    "        \n",
    "    return doc_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00268555, -0.15378189, -0.55761719, -0.07216644, -0.32263184])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "custom_tweet = \"RT @Twitter @chapagain Hello There! Have a great day. :) #good #morning http://chapagain.com.np\"\n",
    "\n",
    "tweet_embedding = get_document_embedding(custom_tweet, en_embeddings_subset)\n",
    "tweet_embedding[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_document_vecs(all_docs,en_embeddings):\n",
    "    '''\n",
    "    Input:\n",
    "        - all_docs: list of strings - all tweets in our dataset.\n",
    "        - en_embeddings: dictionary with words as the keys and their embeddings as the values.\n",
    "    Output:\n",
    "        - document_vec_matrix: matrix of tweet embeddings.\n",
    "        - ind2Doc_dict: dictionary with indices of tweets in vecs as keys and their embeddings as the values.\n",
    "    '''\n",
    "    \n",
    "    ind2Doc_dict={}\n",
    "    \n",
    "    document_vec_l=[]\n",
    "    \n",
    "    for i,doc in enumerate(all_docs):\n",
    "        \n",
    "        # get the document embedding of the tweet\n",
    "        doc_embedding=get_document_embedding(doc,en_embeddings)\n",
    "        \n",
    "        ind2Doc_dict[i]=doc_embedding\n",
    "        \n",
    "        document_vec_l.append(doc_embedding)\n",
    "        \n",
    "    document_vec_matrix=np.vstack(document_vec_l)\n",
    "    \n",
    "    return document_vec_matrix,ind2Doc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_vecs, ind2Tweet = get_document_vecs(all_tweets, en_embeddings_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dictionary 10000\n",
      "shape of document_vecs (10000, 300)\n"
     ]
    }
   ],
   "source": [
    "print(f\"length of dictionary {len(ind2Tweet)}\")\n",
    "print(f\"shape of document_vecs {document_vecs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Looking up the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_tweet = 'i am sad'\n",
    "process_tweet(my_tweet)\n",
    "tweet_embedding = get_document_embedding(my_tweet, en_embeddings_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@zoeeylim sad sad sad kid :( it's ok I help you watch the match HAHAHAHAHA\n"
     ]
    }
   ],
   "source": [
    "idx = np.argmax(cosine_similarity(document_vecs, tweet_embedding))\n",
    "print(all_tweets[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vectors is 10000 and each has 300 dimensions.\n"
     ]
    }
   ],
   "source": [
    "N_VECS = len(all_tweets)       # This many vectors.\n",
    "N_DIMS = len(ind2Tweet[1])     # Vector dimensionality.\n",
    "print(f\"Number of vectors is {N_VECS} and each has {N_DIMS} dimensions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The number of planes. We use log2(256) to have ~16 vectors/bucket.\n",
    "N_PLANES = 10\n",
    "# Number of times to repeat the hashing to improve the search.\n",
    "N_UNIVERSES = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Getting the hash number for a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "planes_l = [np.random.normal(size=(N_DIMS, N_PLANES))\n",
    "            for _ in range(N_UNIVERSES)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# UNQ_C17 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def hash_value_of_vector(v, planes):\n",
    "    \"\"\"Create a hash for a vector; hash_id says which random hash to use.\n",
    "    Input:\n",
    "        - v:  vector of tweet. It's dimension is (1, N_DIMS)\n",
    "        - planes: matrix of dimension (N_DIMS, N_PLANES) - the set of planes that divide up the region\n",
    "    Output:\n",
    "        - res: a number which is used as a hash for your vector\n",
    "\n",
    "    \"\"\"\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    # for the set of planes,\n",
    "    # calculate the dot product between the vector and the matrix containing the planes\n",
    "    # remember that planes has shape (300, 10)\n",
    "    # The dot product will have the shape (1,10)\n",
    "    dot_product = np.dot(v,planes)\n",
    "    \n",
    "    # get the sign of the dot product (1,10) shaped vector\n",
    "    sign_of_dot_product = np.sign(dot_product)\n",
    "    \n",
    "    # set h to be false (eqivalent to 0 when used in operations) if the sign is negative,\n",
    "    # and true (equivalent to 1) if the sign is positive (1,10) shaped vector\n",
    "    h = sign_of_dot_product>=0\n",
    "\n",
    "    # remove extra un-used dimensions (convert this from a 2D to a 1D array)\n",
    "    h = np.squeeze(h)\n",
    "\n",
    "    # initialize the hash value to 0\n",
    "    hash_value = 0\n",
    "\n",
    "    n_planes = planes.shape[1]\n",
    "    for i in range(n_planes):\n",
    "        # increment the hash value by 2^i * h_i\n",
    "        hash_value += np.power(2,i)*h[i]\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # cast hash_value as an integer\n",
    "    hash_value = int(hash_value)\n",
    "\n",
    "    return hash_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The hash value for this vector, and the set of planes at index 0, is 768\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "idx = 0\n",
    "planes = planes_l[idx]  # get one 'universe' of planes to test the function\n",
    "vec = np.random.rand(1, 300)\n",
    "print(f\" The hash value for this vector,\",\n",
    "      f\"and the set of planes at index {idx},\",\n",
    "      f\"is {hash_value_of_vector(vec, planes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating a hash table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# UNQ_C19 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# This is the code used to create a hash table: feel free to read over it\n",
    "def make_hash_table(vecs, planes):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        - vecs: list of vectors to be hashed.\n",
    "        - planes: the matrix of planes in a single \"universe\", with shape (embedding dimensions, number of planes).\n",
    "    Output:\n",
    "        - hash_table: dictionary - keys are hashes, values are lists of vectors (hash buckets)\n",
    "        - id_table: dictionary - keys are hashes, values are list of vectors id's\n",
    "                            (it's used to know which tweet corresponds to the hashed vector)\n",
    "    \"\"\"\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "\n",
    "    # number of planes is the number of columns in the planes matrix\n",
    "    num_of_planes = planes.shape[1]\n",
    "\n",
    "    # number of buckets is 2^(number of planes)\n",
    "    num_buckets = 2**num_of_planes\n",
    "\n",
    "    # create the hash table as a dictionary.\n",
    "    # Keys are integers (0,1,2.. number of buckets)\n",
    "    # Values are empty lists\n",
    "    hash_table = {i:[] for i in range(num_buckets)}\n",
    "\n",
    "    # create the id table as a dictionary.\n",
    "    # Keys are integers (0,1,2... number of buckets)\n",
    "    # Values are empty lists\n",
    "    id_table = {i:[] for i in range(num_buckets)}\n",
    "\n",
    "    # for each vector in 'vecs'\n",
    "    for i, v in enumerate(vecs):\n",
    "\n",
    "        # calculate the hash value for the vector\n",
    "        h = hash_value_of_vector(v,planes)\n",
    "        #print(h)\n",
    "        #print('******')\n",
    "        # store the vector into hash_table at key h,\n",
    "        # by appending the vector v to the list at key h\n",
    "        hash_table[h].append(v)\n",
    "\n",
    "        # store the vector's index 'i' (each document is given a unique integer 0,1,2...)\n",
    "        # the key is the h, and the 'i' is appended to the list at key h\n",
    "        id_table[h].append(i)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return hash_table, id_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 10) \n",
      "The hash table at key 0 has 3 document vectors\n",
      "The id table at key 0 has 3\n",
      "The first 5 document indices stored at key 0 of are [3276, 3281, 3282]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "planes = planes_l[0]  # get one 'universe' of planes to test the function\n",
    "vec = np.random.rand(1, 300)\n",
    "print(planes.shape,'')\n",
    "\n",
    "tmp_hash_table, tmp_id_table = make_hash_table(document_vecs, planes)\n",
    "#print(tmp_hash_table[0])\n",
    "#print(tmp_id_table[0])\n",
    "print(f\"The hash table at key 0 has {len(tmp_hash_table[0])} document vectors\")\n",
    "print(f\"The id table at key 0 has {len(tmp_id_table[0])}\")\n",
    "print(f\"The first 5 document indices stored at key 0 of are {tmp_id_table[0][0:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on hash universe #: 0\n",
      "working on hash universe #: 1\n",
      "working on hash universe #: 2\n",
      "working on hash universe #: 3\n",
      "working on hash universe #: 4\n",
      "working on hash universe #: 5\n",
      "working on hash universe #: 6\n",
      "working on hash universe #: 7\n",
      "working on hash universe #: 8\n",
      "working on hash universe #: 9\n",
      "working on hash universe #: 10\n",
      "working on hash universe #: 11\n",
      "working on hash universe #: 12\n",
      "working on hash universe #: 13\n",
      "working on hash universe #: 14\n",
      "working on hash universe #: 15\n",
      "working on hash universe #: 16\n",
      "working on hash universe #: 17\n",
      "working on hash universe #: 18\n",
      "working on hash universe #: 19\n",
      "working on hash universe #: 20\n",
      "working on hash universe #: 21\n",
      "working on hash universe #: 22\n",
      "working on hash universe #: 23\n",
      "working on hash universe #: 24\n"
     ]
    }
   ],
   "source": [
    "# Creating the hashtables\n",
    "hash_tables = []\n",
    "id_tables = []\n",
    "for universe_id in range(N_UNIVERSES):  # there are 25 hashes\n",
    "    print('working on hash universe #:', universe_id)\n",
    "    planes = planes_l[universe_id]\n",
    "    hash_table, id_table = make_hash_table(document_vecs, planes)\n",
    "    hash_tables.append(hash_table)\n",
    "    id_tables.append(id_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the code used to do the fast nearest neighbor search. Feel free to go over it\n",
    "def approximate_knn(doc_id, v, planes_l, k=1, num_universes_to_use=N_UNIVERSES):\n",
    "    \"\"\"Search for k-NN using hashes.\"\"\"\n",
    "    assert num_universes_to_use <= N_UNIVERSES\n",
    "\n",
    "    # Vectors that will be checked as p0ossible nearest neighbor\n",
    "    vecs_to_consider_l = list()\n",
    "\n",
    "    # list of document IDs\n",
    "    ids_to_consider_l = list()\n",
    "\n",
    "    # create a set for ids to consider, for faster checking if a document ID already exists in the set\n",
    "    ids_to_consider_set = set()\n",
    "\n",
    "    # loop through the universes of planes\n",
    "    for universe_id in range(num_universes_to_use):\n",
    "\n",
    "        # get the set of planes from the planes_l list, for this particular universe_id\n",
    "        planes = planes_l[universe_id]\n",
    "\n",
    "        # get the hash value of the vector for this set of planes\n",
    "        hash_value = hash_value_of_vector(v, planes)\n",
    "\n",
    "        # get the hash table for this particular universe_id\n",
    "        hash_table = hash_tables[universe_id]\n",
    "\n",
    "        # get the list of document vectors for this hash table, where the key is the hash_value\n",
    "        document_vectors_l = hash_table[hash_value]\n",
    "\n",
    "        # get the id_table for this particular universe_id\n",
    "        id_table = id_tables[universe_id]\n",
    "\n",
    "        # get the subset of documents to consider as nearest neighbors from this id_table dictionary\n",
    "        new_ids_to_consider = id_table[hash_value]\n",
    "\n",
    "        ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "\n",
    "        # remove the id of the document that we're searching\n",
    "        if doc_id in new_ids_to_consider:\n",
    "            new_ids_to_consider.remove(doc_id)\n",
    "            print(f\"removed doc_id {doc_id} of input vector from new_ids_to_search\")\n",
    "\n",
    "        # loop through the subset of document vectors to consider\n",
    "        for i, new_id in enumerate(new_ids_to_consider):\n",
    "\n",
    "            # if the document ID is not yet in the set ids_to_consider...\n",
    "            if new_id not in ids_to_consider_set:\n",
    "                # access document_vectors_l list at index i to get the embedding\n",
    "                # then append it to the list of vectors to consider as possible nearest neighbors\n",
    "                document_vector_at_i = document_vectors_l[i]\n",
    "                \n",
    "\n",
    "                # append the new_id (the index for the document) to the list of ids to consider\n",
    "                vecs_to_consider_l.append(document_vector_at_i)\n",
    "                ids_to_consider_l.append(new_id)\n",
    "                # also add the new_id to the set of ids to consider\n",
    "                # (use this to check if new_id is not already in the IDs to consider)\n",
    "                ids_to_consider_set.add(new_id)\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    # Now run k-NN on the smaller set of vecs-to-consider.\n",
    "    print(\"Fast considering %d vecs\" % len(vecs_to_consider_l))\n",
    "\n",
    "    # convert the vecs to consider set to a list, then to a numpy array\n",
    "    vecs_to_consider_arr = np.array(vecs_to_consider_l)\n",
    "\n",
    "    # call nearest neighbors on the reduced list of candidate vectors\n",
    "    nearest_neighbor_idx_l = knn(v, vecs_to_consider_arr, k=k)\n",
    "    print(nearest_neighbor_idx_l)\n",
    "    print(ids_to_consider_l)\n",
    "    # Use the nearest neighbor index list as indices into the ids to consider\n",
    "    # create a list of nearest neighbors by the document ids\n",
    "    nearest_neighbor_ids = [ids_to_consider_l[idx]\n",
    "                            for idx in nearest_neighbor_idx_l]\n",
    "\n",
    "    return nearest_neighbor_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#document_vecs, ind2Tweet\n",
    "doc_id = 0\n",
    "doc_to_search = all_tweets[doc_id]\n",
    "vec_to_search = document_vecs[doc_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed doc_id 0 of input vector from new_ids_to_search\n",
      "removed doc_id 0 of input vector from new_ids_to_search\n",
      "removed doc_id 0 of input vector from new_ids_to_search\n",
      "removed doc_id 0 of input vector from new_ids_to_search\n",
      "removed doc_id 0 of input vector from new_ids_to_search\n",
      "Fast considering 77 vecs\n",
      "[26  8  0]\n",
      "[51, 105, 154, 160, 195, 253, 1876, 2478, 701, 1205, 1300, 1581, 1681, 1685, 2714, 4149, 4157, 4232, 4753, 5684, 6821, 9239, 213, 339, 520, 1729, 2140, 2786, 3028, 3162, 3259, 3654, 4002, 4047, 5263, 5492, 5538, 5649, 5656, 5729, 7076, 9063, 9207, 9789, 9927, 207, 254, 1302, 1480, 1815, 2298, 2620, 2741, 3525, 3837, 4704, 4871, 5327, 5386, 5923, 6033, 6371, 6762, 7288, 7472, 7774, 7790, 7947, 8061, 8224, 8276, 8892, 9096, 9153, 9175, 9323, 9740]\n"
     ]
    }
   ],
   "source": [
    "# Sample\n",
    "nearest_neighbor_ids = approximate_knn(doc_id, vec_to_search, planes_l, k=3, num_universes_to_use=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors for document 0\n",
      "Document contents: #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "\n",
      "Nearest neighbor at document id 2140\n",
      "document contents: @PopsRamjet come one, every now and then is not so bad :)\n",
      "Nearest neighbor at document id 701\n",
      "document contents: With the top cutie of Bohol :) https://t.co/Jh7F6U46UB\n",
      "Nearest neighbor at document id 51\n",
      "document contents: #FollowFriday @France_Espana @reglisse_menthe @CCI_inter for being top engaged members in my community this week :)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Nearest neighbors for document {doc_id}\")\n",
    "print(f\"Document contents: {doc_to_search}\")\n",
    "print(\"\")\n",
    "\n",
    "for neighbor_id in nearest_neighbor_ids:\n",
    "    print(f\"Nearest neighbor at document id {neighbor_id}\")\n",
    "    print(f\"document contents: {all_tweets[neighbor_id]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='svg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_data(file_name):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        A file_name which is found in your current directory. You just have to read it in. \n",
    "    Output: \n",
    "        words: a list containing all the words in the corpus (text file you read) in lower case. \n",
    "    \"\"\"\n",
    "    words=[]\n",
    "    \n",
    "    with open(file_name) as f:\n",
    "        file_name_data=f.read()\n",
    "    file_name_data=file_name_data.lower()\n",
    "    words=re.findall('\\w+',file_name_data)\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first ten words in the text are: \n",
      "['o', 'for', 'a', 'muse', 'of', 'fire', 'that', 'would', 'ascend', 'the']\n",
      "There are 6116 unique words in the vocabulary.\n"
     ]
    }
   ],
   "source": [
    "word_l = process_data('shakespeare.txt')\n",
    "vocab = set(word_l)  # this will be your new vocabulary\n",
    "print(f\"The first ten words in the text are: \\n{word_l[0:10]}\")\n",
    "print(f\"There are {len(vocab)} unique words in the vocabulary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_count(word_l):\n",
    "    '''\n",
    "    Input:\n",
    "        word_l: a set of words representing the corpus. \n",
    "    Output:\n",
    "        word_count_dict: The wordcount dictionary where key is the word and value is its frequency.\n",
    "    '''\n",
    "    word_count_dict=dict()\n",
    "    \n",
    "    word_count_dict=Counter(word_l)\n",
    "    \n",
    "    return word_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6116 key values pairs\n",
      "The count for the word 'thee' is 240\n"
     ]
    }
   ],
   "source": [
    "word_count_dict = get_count(word_l)\n",
    "print(f\"There are {len(word_count_dict)} key values pairs\")\n",
    "print(f\"The count for the word 'thee' is {word_count_dict.get('thee',0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_probs(word_count_dict):\n",
    "    '''\n",
    "    Input:\n",
    "        word_count_dict: The wordcount dictionary where key is the word and value is its frequency.\n",
    "    Output:\n",
    "        probs: A dictionary where keys are the words and the values are the probability that a word will occur. \n",
    "    '''\n",
    "    probs={}\n",
    "    \n",
    "    m=sum(word_count_dict.values())\n",
    "    for key,value in word_count_dict.items():\n",
    "        probs[key]=value/m\n",
    "        \n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of probs is 6116\n",
      "P('thee') is 0.0045\n"
     ]
    }
   ],
   "source": [
    "probs = get_probs(word_count_dict)\n",
    "print(f\"Length of probs is {len(probs)}\")\n",
    "print(f\"P('thee') is {probs['thee']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# String Manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def delete_letter(word,verbose=False):\n",
    "    '''\n",
    "    Input:\n",
    "        word: the string/word for which you will generate all possible words \n",
    "                in the vocabulary which have 1 missing character\n",
    "    Output:\n",
    "        delete_l: a list of all possible strings obtained by deleting 1 character from word\n",
    "    '''\n",
    "    \n",
    "    delete_l=[]\n",
    "    split_l=[]\n",
    "    \n",
    "    for c in range(len(word)):\n",
    "        split_l.append((word[:c],word[c:]))\n",
    "    \n",
    "    for a,b in split_l:\n",
    "        delete_l.append(a+b[1:])\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"input word: {word}, \\nsplit_l = {split_l}, \\ndelete_l = {delete_l}\")\n",
    "    \n",
    "    return delete_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input word: cans, \n",
      "split_l = [('', 'cans'), ('c', 'ans'), ('ca', 'ns'), ('can', 's')], \n",
      "delete_l = ['ans', 'cns', 'cas', 'can']\n"
     ]
    }
   ],
   "source": [
    "delete_word_l = delete_letter(word=\"cans\",\n",
    "                        verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outputs of delete_letter('at') is 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of outputs of delete_letter('at') is {len(delete_letter('at'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def switch_letter(word,verbose=True):\n",
    "    '''\n",
    "    Input:\n",
    "        word: input string\n",
    "    Output:\n",
    "        switches: a list of all possible strings with one adjacent charater switched\n",
    "    '''\n",
    "    \n",
    "    def swap(c,i,j):\n",
    "        c=list(c)\n",
    "        c[i],c[j]=c[j],c[i]\n",
    "        \n",
    "        return ''.join(c)\n",
    "    \n",
    "    switch_l = []\n",
    "    split_l = []\n",
    "    \n",
    "    len_word=len(word)\n",
    "    \n",
    "    for c in range(len_word):\n",
    "        split_l.append((word[:c],word[c:]))\n",
    "    \n",
    "    switch_l=[a+b[1]+b[0]+b[2:] for a,b in split_l if len(b) >=2 ]\n",
    "    \n",
    "    if verbose: print(f\"Input word = {word} \\nsplit_l = {split_l} \\nswitch_l = {switch_l}\") \n",
    "\n",
    "    return switch_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input word = eta \n",
      "split_l = [('', 'eta'), ('e', 'ta'), ('et', 'a')] \n",
      "switch_l = ['tea', 'eat']\n"
     ]
    }
   ],
   "source": [
    "switch_word_l = switch_letter(word=\"eta\",\n",
    "                         verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input word = at \n",
      "split_l = [('', 'at'), ('a', 't')] \n",
      "switch_l = ['ta']\n",
      "Number of outputs of switch_letter('at') is 1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of outputs of switch_letter('at') is {len(switch_letter('at'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_letter(word,verbose=False):\n",
    "    '''\n",
    "    Input:\n",
    "        word: the input string/word \n",
    "    Output:\n",
    "        replaces: a list of all possible strings where we replaced one letter from the original word. \n",
    "    ''' \n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    replace_l = []\n",
    "    split_l = []\n",
    "    \n",
    "    for c in range(len(word)):\n",
    "        split_l.append((word[:c],word[c:]))\n",
    "        \n",
    "    replace_l = [a + l + (b[1:] if len(b)> 1 else '') for a,b in split_l if b for l in letters]\n",
    "    replace_set=set(replace_l)    \n",
    "    replace_set.remove(word)\n",
    "    \n",
    "    replace_l = sorted(list(replace_set))\n",
    "    \n",
    "    if verbose: print(f\"Input word = {word} \\nsplit_l = {split_l} \\nreplace_l {replace_l}\")   \n",
    "\n",
    "    return replace_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input word = can \n",
      "split_l = [('', 'can'), ('c', 'an'), ('ca', 'n')] \n",
      "replace_l ['aan', 'ban', 'caa', 'cab', 'cac', 'cad', 'cae', 'caf', 'cag', 'cah', 'cai', 'caj', 'cak', 'cal', 'cam', 'cao', 'cap', 'caq', 'car', 'cas', 'cat', 'cau', 'cav', 'caw', 'cax', 'cay', 'caz', 'cbn', 'ccn', 'cdn', 'cen', 'cfn', 'cgn', 'chn', 'cin', 'cjn', 'ckn', 'cln', 'cmn', 'cnn', 'con', 'cpn', 'cqn', 'crn', 'csn', 'ctn', 'cun', 'cvn', 'cwn', 'cxn', 'cyn', 'czn', 'dan', 'ean', 'fan', 'gan', 'han', 'ian', 'jan', 'kan', 'lan', 'man', 'nan', 'oan', 'pan', 'qan', 'ran', 'san', 'tan', 'uan', 'van', 'wan', 'xan', 'yan', 'zan']\n"
     ]
    }
   ],
   "source": [
    "replace_l = replace_letter(word='can',\n",
    "                              verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def insert_letter(word,verbose=False):\n",
    "    '''\n",
    "    Input:\n",
    "        word: the input string/word \n",
    "    Output:\n",
    "        inserts: a set of all possible strings with one new letter inserted at every offset\n",
    "    ''' \n",
    "    \n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    insert_l = []\n",
    "    split_l = []\n",
    "    \n",
    "    for c in range(len(word)+1):\n",
    "        split_l.append((word[:c],word[c:]))\n",
    "        \n",
    "    insert_l=[a+l+b for a,b in split_l for l in letters]\n",
    "    \n",
    "    if verbose: print(f\"Input word {word} \\nsplit_l = {split_l} \\ninsert_l = {insert_l}\")\n",
    "    \n",
    "    return insert_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input word at \n",
      "split_l = [('', 'at'), ('a', 't'), ('at', '')] \n",
      "insert_l = ['aat', 'bat', 'cat', 'dat', 'eat', 'fat', 'gat', 'hat', 'iat', 'jat', 'kat', 'lat', 'mat', 'nat', 'oat', 'pat', 'qat', 'rat', 'sat', 'tat', 'uat', 'vat', 'wat', 'xat', 'yat', 'zat', 'aat', 'abt', 'act', 'adt', 'aet', 'aft', 'agt', 'aht', 'ait', 'ajt', 'akt', 'alt', 'amt', 'ant', 'aot', 'apt', 'aqt', 'art', 'ast', 'att', 'aut', 'avt', 'awt', 'axt', 'ayt', 'azt', 'ata', 'atb', 'atc', 'atd', 'ate', 'atf', 'atg', 'ath', 'ati', 'atj', 'atk', 'atl', 'atm', 'atn', 'ato', 'atp', 'atq', 'atr', 'ats', 'att', 'atu', 'atv', 'atw', 'atx', 'aty', 'atz']\n",
      "Number of strings output by insert_letter('at') is 78\n"
     ]
    }
   ],
   "source": [
    "insert_l = insert_letter('at', True)\n",
    "print(f\"Number of strings output by insert_letter('at') is {len(insert_l)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outputs of insert_letter('at') is 78\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of outputs of insert_letter('at') is {len(insert_letter('at'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combining the edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def edit_one_letter(word,allow_switches=True):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        word: the string/word for which we will generate all possible wordsthat are one edit away.\n",
    "    Output:\n",
    "        edit_one_set: a set of words with one possible edit. Please return a set. and not a list.\n",
    "    \"\"\"\n",
    "    \n",
    "    edit_one_set=set()\n",
    "    \n",
    "    edit_one_set.update(delete_letter(word))\n",
    "    \n",
    "    if allow_switches:\n",
    "        edit_one_set.update(switch_letter(word))\n",
    "        \n",
    "    edit_one_set.update(replace_letter(word))\n",
    "    edit_one_set.update(insert_letter(word))\n",
    "    \n",
    "    return edit_one_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input word = at \n",
      "split_l = [('', 'at'), ('a', 't')] \n",
      "switch_l = ['ta']\n",
      "input word at \n",
      "edit_one_l \n",
      "['a', 'aa', 'aat', 'ab', 'abt', 'ac', 'act', 'ad', 'adt', 'ae', 'aet', 'af', 'aft', 'ag', 'agt', 'ah', 'aht', 'ai', 'ait', 'aj', 'ajt', 'ak', 'akt', 'al', 'alt', 'am', 'amt', 'an', 'ant', 'ao', 'aot', 'ap', 'apt', 'aq', 'aqt', 'ar', 'art', 'as', 'ast', 'ata', 'atb', 'atc', 'atd', 'ate', 'atf', 'atg', 'ath', 'ati', 'atj', 'atk', 'atl', 'atm', 'atn', 'ato', 'atp', 'atq', 'atr', 'ats', 'att', 'atu', 'atv', 'atw', 'atx', 'aty', 'atz', 'au', 'aut', 'av', 'avt', 'aw', 'awt', 'ax', 'axt', 'ay', 'ayt', 'az', 'azt', 'bat', 'bt', 'cat', 'ct', 'dat', 'dt', 'eat', 'et', 'fat', 'ft', 'gat', 'gt', 'hat', 'ht', 'iat', 'it', 'jat', 'jt', 'kat', 'kt', 'lat', 'lt', 'mat', 'mt', 'nat', 'nt', 'oat', 'ot', 'pat', 'pt', 'qat', 'qt', 'rat', 'rt', 'sat', 'st', 't', 'ta', 'tat', 'tt', 'uat', 'ut', 'vat', 'vt', 'wat', 'wt', 'xat', 'xt', 'yat', 'yt', 'zat', 'zt']\n",
      "\n",
      "The type of the returned object should be a set <class 'set'>\n",
      "Input word = at \n",
      "split_l = [('', 'at'), ('a', 't')] \n",
      "switch_l = ['ta']\n",
      "Number of outputs from edit_one_letter('at') is 129\n"
     ]
    }
   ],
   "source": [
    "tmp_word = \"at\"\n",
    "tmp_edit_one_set = edit_one_letter(tmp_word)\n",
    "# turn this into a list to sort it, in order to view it\n",
    "tmp_edit_one_l = sorted(list(tmp_edit_one_set))\n",
    "\n",
    "print(f\"input word {tmp_word} \\nedit_one_l \\n{tmp_edit_one_l}\\n\")\n",
    "print(f\"The type of the returned object should be a set {type(tmp_edit_one_set)}\")\n",
    "print(f\"Number of outputs from edit_one_letter('at') is {len(edit_one_letter('at'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def edit_two_letters(word,allow_switches=True):\n",
    "    '''\n",
    "    Input:\n",
    "        word: the input string/word \n",
    "    Output:\n",
    "        edit_two_set: a set of strings with all possible two edits\n",
    "    '''\n",
    "    \n",
    "    edit_two_set = set()\n",
    "    edit_one = edit_one_letter(word,allow_switches=allow_switches)\n",
    "\n",
    "    for w in edit_one:\n",
    "        if w:\n",
    "            edit_two = edit_one_letter(w,allow_switches=allow_switches)\n",
    "            edit_two_set.update(edit_two)\n",
    "    \n",
    "    return edit_two_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input word = a \n",
      "split_l = [('', 'a')] \n",
      "switch_l = []\n",
      "Input word = v \n",
      "split_l = [('', 'v')] \n",
      "switch_l = []\n",
      "Input word = w \n",
      "split_l = [('', 'w')] \n",
      "switch_l = []\n",
      "Input word = ua \n",
      "split_l = [('', 'ua'), ('u', 'a')] \n",
      "switch_l = ['au']\n",
      "Input word = ba \n",
      "split_l = [('', 'ba'), ('b', 'a')] \n",
      "switch_l = ['ab']\n",
      "Input word = ay \n",
      "split_l = [('', 'ay'), ('a', 'y')] \n",
      "switch_l = ['ya']\n",
      "Input word = f \n",
      "split_l = [('', 'f')] \n",
      "switch_l = []\n",
      "Input word = la \n",
      "split_l = [('', 'la'), ('l', 'a')] \n",
      "switch_l = ['al']\n",
      "Input word = at \n",
      "split_l = [('', 'at'), ('a', 't')] \n",
      "switch_l = ['ta']\n",
      "Input word = az \n",
      "split_l = [('', 'az'), ('a', 'z')] \n",
      "switch_l = ['za']\n",
      "Input word = ak \n",
      "split_l = [('', 'ak'), ('a', 'k')] \n",
      "switch_l = ['ka']\n",
      "Input word = g \n",
      "split_l = [('', 'g')] \n",
      "switch_l = []\n",
      "Input word = oa \n",
      "split_l = [('', 'oa'), ('o', 'a')] \n",
      "switch_l = ['ao']\n",
      "Input word = ao \n",
      "split_l = [('', 'ao'), ('a', 'o')] \n",
      "switch_l = ['oa']\n",
      "Input word = q \n",
      "split_l = [('', 'q')] \n",
      "switch_l = []\n",
      "Input word = aw \n",
      "split_l = [('', 'aw'), ('a', 'w')] \n",
      "switch_l = ['wa']\n",
      "Input word = x \n",
      "split_l = [('', 'x')] \n",
      "switch_l = []\n",
      "Input word = na \n",
      "split_l = [('', 'na'), ('n', 'a')] \n",
      "switch_l = ['an']\n",
      "Input word = ka \n",
      "split_l = [('', 'ka'), ('k', 'a')] \n",
      "switch_l = ['ak']\n",
      "Input word = ah \n",
      "split_l = [('', 'ah'), ('a', 'h')] \n",
      "switch_l = ['ha']\n",
      "Input word = y \n",
      "split_l = [('', 'y')] \n",
      "switch_l = []\n",
      "Input word = pa \n",
      "split_l = [('', 'pa'), ('p', 'a')] \n",
      "switch_l = ['ap']\n",
      "Input word = j \n",
      "split_l = [('', 'j')] \n",
      "switch_l = []\n",
      "Input word = ea \n",
      "split_l = [('', 'ea'), ('e', 'a')] \n",
      "switch_l = ['ae']\n",
      "Input word = va \n",
      "split_l = [('', 'va'), ('v', 'a')] \n",
      "switch_l = ['av']\n",
      "Input word = ab \n",
      "split_l = [('', 'ab'), ('a', 'b')] \n",
      "switch_l = ['ba']\n",
      "Input word = l \n",
      "split_l = [('', 'l')] \n",
      "switch_l = []\n",
      "Input word = m \n",
      "split_l = [('', 'm')] \n",
      "switch_l = []\n",
      "Input word = ag \n",
      "split_l = [('', 'ag'), ('a', 'g')] \n",
      "switch_l = ['ga']\n",
      "Input word = al \n",
      "split_l = [('', 'al'), ('a', 'l')] \n",
      "switch_l = ['la']\n",
      "Input word = ya \n",
      "split_l = [('', 'ya'), ('y', 'a')] \n",
      "switch_l = ['ay']\n",
      "Input word = ma \n",
      "split_l = [('', 'ma'), ('m', 'a')] \n",
      "switch_l = ['am']\n",
      "Input word = ar \n",
      "split_l = [('', 'ar'), ('a', 'r')] \n",
      "switch_l = ['ra']\n",
      "Input word = r \n",
      "split_l = [('', 'r')] \n",
      "switch_l = []\n",
      "Input word = ja \n",
      "split_l = [('', 'ja'), ('j', 'a')] \n",
      "switch_l = ['aj']\n",
      "Input word = wa \n",
      "split_l = [('', 'wa'), ('w', 'a')] \n",
      "switch_l = ['aw']\n",
      "Input word = ai \n",
      "split_l = [('', 'ai'), ('a', 'i')] \n",
      "switch_l = ['ia']\n",
      "Input word = o \n",
      "split_l = [('', 'o')] \n",
      "switch_l = []\n",
      "Input word = z \n",
      "split_l = [('', 'z')] \n",
      "switch_l = []\n",
      "Input word = s \n",
      "split_l = [('', 's')] \n",
      "switch_l = []\n",
      "Input word = za \n",
      "split_l = [('', 'za'), ('z', 'a')] \n",
      "switch_l = ['az']\n",
      "Input word = c \n",
      "split_l = [('', 'c')] \n",
      "switch_l = []\n",
      "Input word = ia \n",
      "split_l = [('', 'ia'), ('i', 'a')] \n",
      "switch_l = ['ai']\n",
      "Input word = n \n",
      "split_l = [('', 'n')] \n",
      "switch_l = []\n",
      "Input word = aj \n",
      "split_l = [('', 'aj'), ('a', 'j')] \n",
      "switch_l = ['ja']\n",
      "Input word = an \n",
      "split_l = [('', 'an'), ('a', 'n')] \n",
      "switch_l = ['na']\n",
      "Input word = ad \n",
      "split_l = [('', 'ad'), ('a', 'd')] \n",
      "switch_l = ['da']\n",
      "Input word = p \n",
      "split_l = [('', 'p')] \n",
      "switch_l = []\n",
      "Input word = ra \n",
      "split_l = [('', 'ra'), ('r', 'a')] \n",
      "switch_l = ['ar']\n",
      "Input word = au \n",
      "split_l = [('', 'au'), ('a', 'u')] \n",
      "switch_l = ['ua']\n",
      "Input word = da \n",
      "split_l = [('', 'da'), ('d', 'a')] \n",
      "switch_l = ['ad']\n",
      "Input word = av \n",
      "split_l = [('', 'av'), ('a', 'v')] \n",
      "switch_l = ['va']\n",
      "Input word = e \n",
      "split_l = [('', 'e')] \n",
      "switch_l = []\n",
      "Input word = ae \n",
      "split_l = [('', 'ae'), ('a', 'e')] \n",
      "switch_l = ['ea']\n",
      "Input word = ap \n",
      "split_l = [('', 'ap'), ('a', 'p')] \n",
      "switch_l = ['pa']\n",
      "Input word = sa \n",
      "split_l = [('', 'sa'), ('s', 'a')] \n",
      "switch_l = ['as']\n",
      "Input word = b \n",
      "split_l = [('', 'b')] \n",
      "switch_l = []\n",
      "Input word = qa \n",
      "split_l = [('', 'qa'), ('q', 'a')] \n",
      "switch_l = ['aq']\n",
      "Input word = u \n",
      "split_l = [('', 'u')] \n",
      "switch_l = []\n",
      "Input word = af \n",
      "split_l = [('', 'af'), ('a', 'f')] \n",
      "switch_l = ['fa']\n",
      "Input word = aa \n",
      "split_l = [('', 'aa'), ('a', 'a')] \n",
      "switch_l = ['aa']\n",
      "Input word = as \n",
      "split_l = [('', 'as'), ('a', 's')] \n",
      "switch_l = ['sa']\n",
      "Input word = d \n",
      "split_l = [('', 'd')] \n",
      "switch_l = []\n",
      "Input word = ac \n",
      "split_l = [('', 'ac'), ('a', 'c')] \n",
      "switch_l = ['ca']\n",
      "Input word = xa \n",
      "split_l = [('', 'xa'), ('x', 'a')] \n",
      "switch_l = ['ax']\n",
      "Input word = ax \n",
      "split_l = [('', 'ax'), ('a', 'x')] \n",
      "switch_l = ['xa']\n",
      "Input word = ca \n",
      "split_l = [('', 'ca'), ('c', 'a')] \n",
      "switch_l = ['ac']\n",
      "Input word = k \n",
      "split_l = [('', 'k')] \n",
      "switch_l = []\n",
      "Input word = t \n",
      "split_l = [('', 't')] \n",
      "switch_l = []\n",
      "Input word = i \n",
      "split_l = [('', 'i')] \n",
      "switch_l = []\n",
      "Input word = fa \n",
      "split_l = [('', 'fa'), ('f', 'a')] \n",
      "switch_l = ['af']\n",
      "Input word = ta \n",
      "split_l = [('', 'ta'), ('t', 'a')] \n",
      "switch_l = ['at']\n",
      "Input word = aq \n",
      "split_l = [('', 'aq'), ('a', 'q')] \n",
      "switch_l = ['qa']\n",
      "Input word = ga \n",
      "split_l = [('', 'ga'), ('g', 'a')] \n",
      "switch_l = ['ag']\n",
      "Input word = h \n",
      "split_l = [('', 'h')] \n",
      "switch_l = []\n",
      "Input word = am \n",
      "split_l = [('', 'am'), ('a', 'm')] \n",
      "switch_l = ['ma']\n",
      "Input word = ha \n",
      "split_l = [('', 'ha'), ('h', 'a')] \n",
      "switch_l = ['ah']\n",
      "Number of strings with edit distance of two: 2654\n",
      "First 10 strings ['', 'a', 'aa', 'aaa', 'aab', 'aac', 'aad', 'aae', 'aaf', 'aag']\n",
      "Last 10 strings ['zv', 'zva', 'zw', 'zwa', 'zx', 'zxa', 'zy', 'zya', 'zz', 'zza']\n",
      "The data type of the returned object should be a set <class 'set'>\n",
      "Input word = at \n",
      "split_l = [('', 'at'), ('a', 't')] \n",
      "switch_l = ['ta']\n",
      "Input word = att \n",
      "split_l = [('', 'att'), ('a', 'tt'), ('at', 't')] \n",
      "switch_l = ['tat', 'att']\n",
      "Input word = atg \n",
      "split_l = [('', 'atg'), ('a', 'tg'), ('at', 'g')] \n",
      "switch_l = ['tag', 'agt']\n",
      "Input word = atu \n",
      "split_l = [('', 'atu'), ('a', 'tu'), ('at', 'u')] \n",
      "switch_l = ['tau', 'aut']\n",
      "Input word = atp \n",
      "split_l = [('', 'atp'), ('a', 'tp'), ('at', 'p')] \n",
      "switch_l = ['tap', 'apt']\n",
      "Input word = tt \n",
      "split_l = [('', 'tt'), ('t', 't')] \n",
      "switch_l = ['tt']\n",
      "Input word = ax \n",
      "split_l = [('', 'ax'), ('a', 'x')] \n",
      "switch_l = ['xa']\n",
      "Input word = ab \n",
      "split_l = [('', 'ab'), ('a', 'b')] \n",
      "switch_l = ['ba']\n",
      "Input word = ot \n",
      "split_l = [('', 'ot'), ('o', 't')] \n",
      "switch_l = ['to']\n",
      "Input word = ar \n",
      "split_l = [('', 'ar'), ('a', 'r')] \n",
      "switch_l = ['ra']\n",
      "Input word = zat \n",
      "split_l = [('', 'zat'), ('z', 'at'), ('za', 't')] \n",
      "switch_l = ['azt', 'zta']\n",
      "Input word = dt \n",
      "split_l = [('', 'dt'), ('d', 't')] \n",
      "switch_l = ['td']\n",
      "Input word = atv \n",
      "split_l = [('', 'atv'), ('a', 'tv'), ('at', 'v')] \n",
      "switch_l = ['tav', 'avt']\n",
      "Input word = aqt \n",
      "split_l = [('', 'aqt'), ('a', 'qt'), ('aq', 't')] \n",
      "switch_l = ['qat', 'atq']\n",
      "Input word = ad \n",
      "split_l = [('', 'ad'), ('a', 'd')] \n",
      "switch_l = ['da']\n",
      "Input word = yt \n",
      "split_l = [('', 'yt'), ('y', 't')] \n",
      "switch_l = ['ty']\n",
      "Input word = af \n",
      "split_l = [('', 'af'), ('a', 'f')] \n",
      "switch_l = ['fa']\n",
      "Input word = ait \n",
      "split_l = [('', 'ait'), ('a', 'it'), ('ai', 't')] \n",
      "switch_l = ['iat', 'ati']\n",
      "Input word = atr \n",
      "split_l = [('', 'atr'), ('a', 'tr'), ('at', 'r')] \n",
      "switch_l = ['tar', 'art']\n",
      "Input word = nat \n",
      "split_l = [('', 'nat'), ('n', 'at'), ('na', 't')] \n",
      "switch_l = ['ant', 'nta']\n",
      "Input word = atc \n",
      "split_l = [('', 'atc'), ('a', 'tc'), ('at', 'c')] \n",
      "switch_l = ['tac', 'act']\n",
      "Input word = aq \n",
      "split_l = [('', 'aq'), ('a', 'q')] \n",
      "switch_l = ['qa']\n",
      "Input word = atl \n",
      "split_l = [('', 'atl'), ('a', 'tl'), ('at', 'l')] \n",
      "switch_l = ['tal', 'alt']\n",
      "Input word = bat \n",
      "split_l = [('', 'bat'), ('b', 'at'), ('ba', 't')] \n",
      "switch_l = ['abt', 'bta']\n",
      "Input word = art \n",
      "split_l = [('', 'art'), ('a', 'rt'), ('ar', 't')] \n",
      "switch_l = ['rat', 'atr']\n",
      "Input word = az \n",
      "split_l = [('', 'az'), ('a', 'z')] \n",
      "switch_l = ['za']\n",
      "Input word = ak \n",
      "split_l = [('', 'ak'), ('a', 'k')] \n",
      "switch_l = ['ka']\n",
      "Input word = ao \n",
      "split_l = [('', 'ao'), ('a', 'o')] \n",
      "switch_l = ['oa']\n",
      "Input word = lt \n",
      "split_l = [('', 'lt'), ('l', 't')] \n",
      "switch_l = ['tl']\n",
      "Input word = aw \n",
      "split_l = [('', 'aw'), ('a', 'w')] \n",
      "switch_l = ['wa']\n",
      "Input word = aat \n",
      "split_l = [('', 'aat'), ('a', 'at'), ('aa', 't')] \n",
      "switch_l = ['aat', 'ata']\n",
      "Input word = ag \n",
      "split_l = [('', 'ag'), ('a', 'g')] \n",
      "switch_l = ['ga']\n",
      "Input word = atk \n",
      "split_l = [('', 'atk'), ('a', 'tk'), ('at', 'k')] \n",
      "switch_l = ['tak', 'akt']\n",
      "Input word = al \n",
      "split_l = [('', 'al'), ('a', 'l')] \n",
      "switch_l = ['la']\n",
      "Input word = qat \n",
      "split_l = [('', 'qat'), ('q', 'at'), ('qa', 't')] \n",
      "switch_l = ['aqt', 'qta']\n",
      "Input word = qt \n",
      "split_l = [('', 'qt'), ('q', 't')] \n",
      "switch_l = ['tq']\n",
      "Input word = iat \n",
      "split_l = [('', 'iat'), ('i', 'at'), ('ia', 't')] \n",
      "switch_l = ['ait', 'ita']\n",
      "Input word = atx \n",
      "split_l = [('', 'atx'), ('a', 'tx'), ('at', 'x')] \n",
      "switch_l = ['tax', 'axt']\n",
      "Input word = xat \n",
      "split_l = [('', 'xat'), ('x', 'at'), ('xa', 't')] \n",
      "switch_l = ['axt', 'xta']\n",
      "Input word = aot \n",
      "split_l = [('', 'aot'), ('a', 'ot'), ('ao', 't')] \n",
      "switch_l = ['oat', 'ato']\n",
      "Input word = et \n",
      "split_l = [('', 'et'), ('e', 't')] \n",
      "switch_l = ['te']\n",
      "Input word = aj \n",
      "split_l = [('', 'aj'), ('a', 'j')] \n",
      "switch_l = ['ja']\n",
      "Input word = ate \n",
      "split_l = [('', 'ate'), ('a', 'te'), ('at', 'e')] \n",
      "switch_l = ['tae', 'aet']\n",
      "Input word = mat \n",
      "split_l = [('', 'mat'), ('m', 'at'), ('ma', 't')] \n",
      "switch_l = ['amt', 'mta']\n",
      "Input word = ap \n",
      "split_l = [('', 'ap'), ('a', 'p')] \n",
      "switch_l = ['pa']\n",
      "Input word = nt \n",
      "split_l = [('', 'nt'), ('n', 't')] \n",
      "switch_l = ['tn']\n",
      "Input word = aet \n",
      "split_l = [('', 'aet'), ('a', 'et'), ('ae', 't')] \n",
      "switch_l = ['eat', 'ate']\n",
      "Input word = ats \n",
      "split_l = [('', 'ats'), ('a', 'ts'), ('at', 's')] \n",
      "switch_l = ['tas', 'ast']\n",
      "Input word = gat \n",
      "split_l = [('', 'gat'), ('g', 'at'), ('ga', 't')] \n",
      "switch_l = ['agt', 'gta']\n",
      "Input word = oat \n",
      "split_l = [('', 'oat'), ('o', 'at'), ('oa', 't')] \n",
      "switch_l = ['aot', 'ota']\n",
      "Input word = aa \n",
      "split_l = [('', 'aa'), ('a', 'a')] \n",
      "switch_l = ['aa']\n",
      "Input word = a \n",
      "split_l = [('', 'a')] \n",
      "switch_l = []\n",
      "Input word = wt \n",
      "split_l = [('', 'wt'), ('w', 't')] \n",
      "switch_l = ['tw']\n",
      "Input word = sat \n",
      "split_l = [('', 'sat'), ('s', 'at'), ('sa', 't')] \n",
      "switch_l = ['ast', 'sta']\n",
      "Input word = ta \n",
      "split_l = [('', 'ta'), ('t', 'a')] \n",
      "switch_l = ['at']\n",
      "Input word = st \n",
      "split_l = [('', 'st'), ('s', 't')] \n",
      "switch_l = ['ts']\n",
      "Input word = am \n",
      "split_l = [('', 'am'), ('a', 'm')] \n",
      "switch_l = ['ma']\n",
      "Input word = ut \n",
      "split_l = [('', 'ut'), ('u', 't')] \n",
      "switch_l = ['tu']\n",
      "Input word = ay \n",
      "split_l = [('', 'ay'), ('a', 'y')] \n",
      "switch_l = ['ya']\n",
      "Input word = mt \n",
      "split_l = [('', 'mt'), ('m', 't')] \n",
      "switch_l = ['tm']\n",
      "Input word = xt \n",
      "split_l = [('', 'xt'), ('x', 't')] \n",
      "switch_l = ['tx']\n",
      "Input word = aut \n",
      "split_l = [('', 'aut'), ('a', 'ut'), ('au', 't')] \n",
      "switch_l = ['uat', 'atu']\n",
      "Input word = zt \n",
      "split_l = [('', 'zt'), ('z', 't')] \n",
      "switch_l = ['tz']\n",
      "Input word = ant \n",
      "split_l = [('', 'ant'), ('a', 'nt'), ('an', 't')] \n",
      "switch_l = ['nat', 'atn']\n",
      "Input word = hat \n",
      "split_l = [('', 'hat'), ('h', 'at'), ('ha', 't')] \n",
      "switch_l = ['aht', 'hta']\n",
      "Input word = vat \n",
      "split_l = [('', 'vat'), ('v', 'at'), ('va', 't')] \n",
      "switch_l = ['avt', 'vta']\n",
      "Input word = atm \n",
      "split_l = [('', 'atm'), ('a', 'tm'), ('at', 'm')] \n",
      "switch_l = ['tam', 'amt']\n",
      "Input word = ato \n",
      "split_l = [('', 'ato'), ('a', 'to'), ('at', 'o')] \n",
      "switch_l = ['tao', 'aot']\n",
      "Input word = atw \n",
      "split_l = [('', 'atw'), ('a', 'tw'), ('at', 'w')] \n",
      "switch_l = ['taw', 'awt']\n",
      "Input word = uat \n",
      "split_l = [('', 'uat'), ('u', 'at'), ('ua', 't')] \n",
      "switch_l = ['aut', 'uta']\n",
      "Input word = jt \n",
      "split_l = [('', 'jt'), ('j', 't')] \n",
      "switch_l = ['tj']\n",
      "Input word = rat \n",
      "split_l = [('', 'rat'), ('r', 'at'), ('ra', 't')] \n",
      "switch_l = ['art', 'rta']\n",
      "Input word = aty \n",
      "split_l = [('', 'aty'), ('a', 'ty'), ('at', 'y')] \n",
      "switch_l = ['tay', 'ayt']\n",
      "Input word = kt \n",
      "split_l = [('', 'kt'), ('k', 't')] \n",
      "switch_l = ['tk']\n",
      "Input word = cat \n",
      "split_l = [('', 'cat'), ('c', 'at'), ('ca', 't')] \n",
      "switch_l = ['act', 'cta']\n",
      "Input word = akt \n",
      "split_l = [('', 'akt'), ('a', 'kt'), ('ak', 't')] \n",
      "switch_l = ['kat', 'atk']\n",
      "Input word = aft \n",
      "split_l = [('', 'aft'), ('a', 'ft'), ('af', 't')] \n",
      "switch_l = ['fat', 'atf']\n",
      "Input word = atq \n",
      "split_l = [('', 'atq'), ('a', 'tq'), ('at', 'q')] \n",
      "switch_l = ['taq', 'aqt']\n",
      "Input word = av \n",
      "split_l = [('', 'av'), ('a', 'v')] \n",
      "switch_l = ['va']\n",
      "Input word = rt \n",
      "split_l = [('', 'rt'), ('r', 't')] \n",
      "switch_l = ['tr']\n",
      "Input word = ati \n",
      "split_l = [('', 'ati'), ('a', 'ti'), ('at', 'i')] \n",
      "switch_l = ['tai', 'ait']\n",
      "Input word = gt \n",
      "split_l = [('', 'gt'), ('g', 't')] \n",
      "switch_l = ['tg']\n",
      "Input word = ast \n",
      "split_l = [('', 'ast'), ('a', 'st'), ('as', 't')] \n",
      "switch_l = ['sat', 'ats']\n",
      "Input word = pt \n",
      "split_l = [('', 'pt'), ('p', 't')] \n",
      "switch_l = ['tp']\n",
      "Input word = fat \n",
      "split_l = [('', 'fat'), ('f', 'at'), ('fa', 't')] \n",
      "switch_l = ['aft', 'fta']\n",
      "Input word = t \n",
      "split_l = [('', 't')] \n",
      "switch_l = []\n",
      "Input word = yat \n",
      "split_l = [('', 'yat'), ('y', 'at'), ('ya', 't')] \n",
      "switch_l = ['ayt', 'yta']\n",
      "Input word = kat \n",
      "split_l = [('', 'kat'), ('k', 'at'), ('ka', 't')] \n",
      "switch_l = ['akt', 'kta']\n",
      "Input word = atf \n",
      "split_l = [('', 'atf'), ('a', 'tf'), ('at', 'f')] \n",
      "switch_l = ['taf', 'aft']\n",
      "Input word = atj \n",
      "split_l = [('', 'atj'), ('a', 'tj'), ('at', 'j')] \n",
      "switch_l = ['taj', 'ajt']\n",
      "Input word = ct \n",
      "split_l = [('', 'ct'), ('c', 't')] \n",
      "switch_l = ['tc']\n",
      "Input word = jat \n",
      "split_l = [('', 'jat'), ('j', 'at'), ('ja', 't')] \n",
      "switch_l = ['ajt', 'jta']\n",
      "Input word = aht \n",
      "split_l = [('', 'aht'), ('a', 'ht'), ('ah', 't')] \n",
      "switch_l = ['hat', 'ath']\n",
      "Input word = ft \n",
      "split_l = [('', 'ft'), ('f', 't')] \n",
      "switch_l = ['tf']\n",
      "Input word = ah \n",
      "split_l = [('', 'ah'), ('a', 'h')] \n",
      "switch_l = ['ha']\n",
      "Input word = eat \n",
      "split_l = [('', 'eat'), ('e', 'at'), ('ea', 't')] \n",
      "switch_l = ['aet', 'eta']\n",
      "Input word = vt \n",
      "split_l = [('', 'vt'), ('v', 't')] \n",
      "switch_l = ['tv']\n",
      "Input word = pat \n",
      "split_l = [('', 'pat'), ('p', 'at'), ('pa', 't')] \n",
      "switch_l = ['apt', 'pta']\n",
      "Input word = axt \n",
      "split_l = [('', 'axt'), ('a', 'xt'), ('ax', 't')] \n",
      "switch_l = ['xat', 'atx']\n",
      "Input word = azt \n",
      "split_l = [('', 'azt'), ('a', 'zt'), ('az', 't')] \n",
      "switch_l = ['zat', 'atz']\n",
      "Input word = ata \n",
      "split_l = [('', 'ata'), ('a', 'ta'), ('at', 'a')] \n",
      "switch_l = ['taa', 'aat']\n",
      "Input word = ath \n",
      "split_l = [('', 'ath'), ('a', 'th'), ('at', 'h')] \n",
      "switch_l = ['tah', 'aht']\n",
      "Input word = ajt \n",
      "split_l = [('', 'ajt'), ('a', 'jt'), ('aj', 't')] \n",
      "switch_l = ['jat', 'atj']\n",
      "Input word = ai \n",
      "split_l = [('', 'ai'), ('a', 'i')] \n",
      "switch_l = ['ia']\n",
      "Input word = abt \n",
      "split_l = [('', 'abt'), ('a', 'bt'), ('ab', 't')] \n",
      "switch_l = ['bat', 'atb']\n",
      "Input word = atn \n",
      "split_l = [('', 'atn'), ('a', 'tn'), ('at', 'n')] \n",
      "switch_l = ['tan', 'ant']\n",
      "Input word = it \n",
      "split_l = [('', 'it'), ('i', 't')] \n",
      "switch_l = ['ti']\n",
      "Input word = avt \n",
      "split_l = [('', 'avt'), ('a', 'vt'), ('av', 't')] \n",
      "switch_l = ['vat', 'atv']\n",
      "Input word = bt \n",
      "split_l = [('', 'bt'), ('b', 't')] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "switch_l = ['tb']\n",
      "Input word = atz \n",
      "split_l = [('', 'atz'), ('a', 'tz'), ('at', 'z')] \n",
      "switch_l = ['taz', 'azt']\n",
      "Input word = ayt \n",
      "split_l = [('', 'ayt'), ('a', 'yt'), ('ay', 't')] \n",
      "switch_l = ['yat', 'aty']\n",
      "Input word = an \n",
      "split_l = [('', 'an'), ('a', 'n')] \n",
      "switch_l = ['na']\n",
      "Input word = wat \n",
      "split_l = [('', 'wat'), ('w', 'at'), ('wa', 't')] \n",
      "switch_l = ['awt', 'wta']\n",
      "Input word = au \n",
      "split_l = [('', 'au'), ('a', 'u')] \n",
      "switch_l = ['ua']\n",
      "Input word = alt \n",
      "split_l = [('', 'alt'), ('a', 'lt'), ('al', 't')] \n",
      "switch_l = ['lat', 'atl']\n",
      "Input word = ae \n",
      "split_l = [('', 'ae'), ('a', 'e')] \n",
      "switch_l = ['ea']\n",
      "Input word = apt \n",
      "split_l = [('', 'apt'), ('a', 'pt'), ('ap', 't')] \n",
      "switch_l = ['pat', 'atp']\n",
      "Input word = awt \n",
      "split_l = [('', 'awt'), ('a', 'wt'), ('aw', 't')] \n",
      "switch_l = ['wat', 'atw']\n",
      "Input word = as \n",
      "split_l = [('', 'as'), ('a', 's')] \n",
      "switch_l = ['sa']\n",
      "Input word = atb \n",
      "split_l = [('', 'atb'), ('a', 'tb'), ('at', 'b')] \n",
      "switch_l = ['tab', 'abt']\n",
      "Input word = atd \n",
      "split_l = [('', 'atd'), ('a', 'td'), ('at', 'd')] \n",
      "switch_l = ['tad', 'adt']\n",
      "Input word = ac \n",
      "split_l = [('', 'ac'), ('a', 'c')] \n",
      "switch_l = ['ca']\n",
      "Input word = adt \n",
      "split_l = [('', 'adt'), ('a', 'dt'), ('ad', 't')] \n",
      "switch_l = ['dat', 'atd']\n",
      "Input word = amt \n",
      "split_l = [('', 'amt'), ('a', 'mt'), ('am', 't')] \n",
      "switch_l = ['mat', 'atm']\n",
      "Input word = ht \n",
      "split_l = [('', 'ht'), ('h', 't')] \n",
      "switch_l = ['th']\n",
      "Input word = tat \n",
      "split_l = [('', 'tat'), ('t', 'at'), ('ta', 't')] \n",
      "switch_l = ['att', 'tta']\n",
      "Input word = lat \n",
      "split_l = [('', 'lat'), ('l', 'at'), ('la', 't')] \n",
      "switch_l = ['alt', 'lta']\n",
      "Input word = dat \n",
      "split_l = [('', 'dat'), ('d', 'at'), ('da', 't')] \n",
      "switch_l = ['adt', 'dta']\n",
      "Input word = agt \n",
      "split_l = [('', 'agt'), ('a', 'gt'), ('ag', 't')] \n",
      "switch_l = ['gat', 'atg']\n",
      "Input word = act \n",
      "split_l = [('', 'act'), ('a', 'ct'), ('ac', 't')] \n",
      "switch_l = ['cat', 'atc']\n",
      "Number of strings that are 2 edit distances from 'at' is 7154\n"
     ]
    }
   ],
   "source": [
    "tmp_edit_two_set = edit_two_letters(\"a\")\n",
    "tmp_edit_two_l = sorted(list(tmp_edit_two_set))\n",
    "print(f\"Number of strings with edit distance of two: {len(tmp_edit_two_l)}\")\n",
    "print(f\"First 10 strings {tmp_edit_two_l[:10]}\")\n",
    "print(f\"Last 10 strings {tmp_edit_two_l[-10:]}\")\n",
    "print(f\"The data type of the returned object should be a set {type(tmp_edit_two_set)}\")\n",
    "print(f\"Number of strings that are 2 edit distances from 'at' is {len(edit_two_letters('at'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['a', 'b']\n",
      "['Most', 'Likely']\n",
      "['least', 'of', 'all']\n"
     ]
    }
   ],
   "source": [
    "# example of logical operation on lists or sets\n",
    "print( [] and [\"a\",\"b\"] )\n",
    "print( [] or [\"a\",\"b\"] )\n",
    "#example of Short circuit behavior\n",
    "val1 =  [\"Most\",\"Likely\"] or [\"Less\",\"so\"] or [\"least\",\"of\",\"all\"]  # selects first, does not evalute remainder\n",
    "print(val1)\n",
    "val2 =  [] or [] or [\"least\",\"of\",\"all\"] # continues evaluation until there is a non-empty list\n",
    "print(val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_corrections(word,probs,vocab,n=2,verbose=False):\n",
    "    '''\n",
    "    Input: \n",
    "        word: a user entered string to check for suggestions\n",
    "        probs: a dictionary that maps each word to its probability in the corpus\n",
    "        vocab: a set containing all the vocabulary\n",
    "        n: number of possible word corrections you want returned in the dictionary\n",
    "    Output: \n",
    "        n_best: a list of tuples with the most probable n corrected words and their probabilities.\n",
    "    '''\n",
    "    \n",
    "    suggestions = []\n",
    "    n_best = []\n",
    "    \n",
    "    suggestions=list((word in vocab and word) or edit_one_letter(word).intersection(vocab)\\\n",
    "                     or edit_two_letters(word).intersection(vocab))\n",
    "    \n",
    "    n_best=[[s,probs[s]] for s in list(reversed(suggestions))]\n",
    "    \n",
    "    if verbose: print(\"suggestions = \", suggestions)\n",
    "\n",
    "    return n_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input word = dys \n",
      "split_l = [('', 'dys'), ('d', 'ys'), ('dy', 's')] \n",
      "switch_l = ['yds', 'dsy']\n",
      "word 0: dye, probability 0.000019\n",
      "word 1: days, probability 0.000410\n",
      "data type of corrections <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "my_word = 'dys' \n",
    "tmp_corrections = get_corrections(my_word, probs, vocab, 2, verbose=False)\n",
    "for i, word_prob in enumerate(tmp_corrections):\n",
    "    print(f\"word {i}: {word_prob[0]}, probability {word_prob[1]:.6f}\")\n",
    "# CODE REVIEW COMMENT: using \"tmp_corrections\" insteads of \"cors\". \"cors\" is not defined\n",
    "print(f\"data type of corrections {type(tmp_corrections)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Minimum edit distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def min_edit_distance(source, target, ins_cost = 1, del_cost = 1, rep_cost = 2):\n",
    "    '''\n",
    "    Input: \n",
    "        source: a string corresponding to the string you are starting with\n",
    "        target: a string corresponding to the string you want to end with\n",
    "        ins_cost: an integer setting the insert cost\n",
    "        del_cost: an integer setting the delete cost\n",
    "        rep_cost: an integer setting the replace cost\n",
    "    Output:\n",
    "        D: a matrix of len(source)+1 by len(target)+1 containing minimum edit distances\n",
    "        med: the minimum edit distance (med) required to convert the source string to the target\n",
    "    '''\n",
    "    \n",
    "    # use deletion and insertion cost as 1\n",
    "    m=len(source)\n",
    "    n=len(target)\n",
    "    \n",
    "    # initialize cost matrix with zeros and dimensions (m+1,n+1)\n",
    "    D=np.zeros((m+1,n+1),dtype=int)\n",
    "    \n",
    "    for row in range(1,m+1):\n",
    "        D[row,0]=D[row-1,0]+del_cost\n",
    "    \n",
    "    for col in range(1,n+1):\n",
    "        D[0,col]=D[0,col-1]+ins_cost\n",
    "        \n",
    "    for row in range(1,m+1):\n",
    "        for col in range(1,n+1):\n",
    "            r_cost=rep_cost\n",
    "            \n",
    "            if source[row-1]==target[col-1]:\n",
    "                r_cost=0\n",
    "            \n",
    "            D[row,col]=min([D[row-1,col]+del_cost, D[row,col-1]+ins_cost, D[row-1,col-1]+r_cost])\n",
    "            \n",
    "    med=D[m,n]\n",
    "    \n",
    "    return D,med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum edits:  4 \n",
      "\n",
      "   #  s  t  a  y\n",
      "#  0  1  2  3  4\n",
      "p  1  2  3  4  5\n",
      "l  2  3  4  5  6\n",
      "a  3  4  5  4  5\n",
      "y  4  5  6  5  4\n"
     ]
    }
   ],
   "source": [
    "# Testing \n",
    "source =  'play'\n",
    "target = 'stay'\n",
    "matrix, min_edits = min_edit_distance(source, target)\n",
    "print(\"minimum edits: \",min_edits, \"\\n\")\n",
    "idx = list('#' + source)\n",
    "cols = list('#' + target)\n",
    "df = pd.DataFrame(matrix, index=idx, columns= cols)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum edits:  3 \n",
      "\n",
      "   #  n  e  a  r\n",
      "#  0  1  2  3  4\n",
      "e  1  2  1  2  3\n",
      "e  2  3  2  3  4\n",
      "r  3  4  3  4  3\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "source =  'eer'\n",
    "target = 'near'\n",
    "matrix, min_edits = min_edit_distance(source, target)\n",
    "print(\"minimum edits: \",min_edits, \"\\n\")\n",
    "idx = list(source)\n",
    "idx.insert(0, '#')\n",
    "cols = list(target)\n",
    "cols.insert(0, '#')\n",
    "df = pd.DataFrame(matrix, index=idx, columns= cols)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source = \"eer\"\n",
    "targets = edit_one_letter(source,allow_switches = False)  #disable switches since min_edit_distance does not include them\n",
    "for t in targets:\n",
    "    _, min_edits = min_edit_distance(source, t,1,1,1)  # set ins, del, sub costs all to one\n",
    "    if min_edits != 1: print(source, t, min_edits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer eer 0\n"
     ]
    }
   ],
   "source": [
    "source = \"eer\"\n",
    "targets = edit_two_letters(source,allow_switches = False) #disable switches since min_edit_distance does not include them\n",
    "for t in targets:\n",
    "    _, min_edits = min_edit_distance(source, t,1,1,1)  # set ins, del, sub costs all to one\n",
    "    if min_edits != 2 and min_edits != 1: print(source, t, min_edits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parts of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A few items of the training corpus list\n",
      "['In\\tIN\\n', 'an\\tDT\\n', 'Oct.\\tNNP\\n', '19\\tCD\\n', 'review\\tNN\\n']\n"
     ]
    }
   ],
   "source": [
    "# load in the training corpus\n",
    "with open(\"WSJ_02-21.pos\",\"r\") as f:\n",
    "    training_corpus=f.readlines()\n",
    "    \n",
    "print(f\"A few items of the training corpus list\")\n",
    "print(training_corpus[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A few items of the vocabulary list\n",
      "['!', '#', '$', '%', '&', \"'\", \"''\", \"'40s\", \"'60s\", \"'70s\", \"'80s\", \"'86\", \"'90s\", \"'N\", \"'S\", \"'d\", \"'em\", \"'ll\", \"'m\", \"'n'\", \"'re\", \"'s\", \"'til\", \"'ve\", '(', ')', ',', '-', '--', '--n--', '--unk--', '--unk_adj--', '--unk_adv--', '--unk_digit--', '--unk_noun--', '--unk_punct--', '--unk_upper--', '--unk_verb--', '.', '...', '0.01', '0.0108', '0.02', '0.03', '0.05', '0.1', '0.10', '0.12', '0.13', '0.15']\n",
      "\n",
      "A few items at the end of the vocabulary list\n",
      "['yards', 'yardstick', 'year', 'year-ago', 'year-before', 'year-earlier', 'year-end', 'year-on-year', 'year-round', 'year-to-date', 'year-to-year', 'yearlong', 'yearly', 'years', 'yeast', 'yelled', 'yelling', 'yellow', 'yen', 'yes', 'yesterday', 'yet', 'yield', 'yielded', 'yielding', 'yields', 'you', 'young', 'younger', 'youngest', 'youngsters', 'your', 'yourself', 'youth', 'youthful', 'yuppie', 'yuppies', 'zero', 'zero-coupon', 'zeroing', 'zeros', 'zinc', 'zip', 'zombie', 'zone', 'zones', 'zoning', '{', '}', '']\n"
     ]
    }
   ],
   "source": [
    "# read the vocabulary data, split by each line of text, and save the list\n",
    "with open(\"hmm_vocab.txt\",\"r\") as f:\n",
    "    voc_l=f.read().split('\\n')\n",
    "    \n",
    "print(\"A few items of the vocabulary list\")\n",
    "print(voc_l[0:50])\n",
    "print()\n",
    "print(\"A few items at the end of the vocabulary list\")\n",
    "print(voc_l[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary dictionary, key is the word, value is a unique integer\n",
      ":0\n",
      "!:1\n",
      "#:2\n",
      "$:3\n",
      "%:4\n",
      "&:5\n",
      "':6\n",
      "'':7\n",
      "'40s:8\n",
      "'60s:9\n",
      "'70s:10\n",
      "'80s:11\n",
      "'86:12\n",
      "'90s:13\n",
      "'N:14\n",
      "'S:15\n",
      "'d:16\n",
      "'em:17\n",
      "'ll:18\n",
      "'m:19\n",
      "'n':20\n"
     ]
    }
   ],
   "source": [
    "# vocab: dictionary that has the index of the corresponding words\n",
    "vocab={}\n",
    "\n",
    "# Get the index of the corresponding words:\n",
    "for i,word in enumerate(sorted(voc_l)):\n",
    "    vocab[word]=i\n",
    "\n",
    "    \n",
    "print(\"Vocabulary dictionary, key is the word, value is a unique integer\")\n",
    "cnt = 0\n",
    "for k,v in vocab.items():\n",
    "    print(f\"{k}:{v}\")\n",
    "    cnt += 1\n",
    "    if cnt > 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A sample of the test corpus\n",
      "['The\\tDT\\n', 'economy\\tNN\\n', \"'s\\tPOS\\n\", 'temperature\\tNN\\n', 'will\\tMD\\n', 'be\\tVB\\n', 'taken\\tVBN\\n', 'from\\tIN\\n', 'several\\tJJ\\n', 'vantage\\tNN\\n']\n"
     ]
    }
   ],
   "source": [
    "# load in the test corpus\n",
    "with open(\"WSJ_24.pos\", 'r') as f:\n",
    "    y = f.readlines()\n",
    "\n",
    "print(\"A sample of the test corpus\")\n",
    "print(y[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Punctuation characters\n",
    "punct = set(string.punctuation)\n",
    "\n",
    "# Morphology rules used to assign unknown word tokens\n",
    "noun_suffix = [\"action\", \"age\", \"ance\", \"cy\", \"dom\", \"ee\", \"ence\", \"er\", \"hood\", \"ion\", \"ism\", \"ist\", \"ity\", \"ling\", \"ment\", \"ness\", \"or\", \"ry\", \"scape\", \"ship\", \"ty\"]\n",
    "verb_suffix = [\"ate\", \"ify\", \"ise\", \"ize\"]\n",
    "adj_suffix = [\"able\", \"ese\", \"ful\", \"i\", \"ian\", \"ible\", \"ic\", \"ish\", \"ive\", \"less\", \"ly\", \"ous\"]\n",
    "adv_suffix = [\"ward\", \"wards\", \"wise\"]\n",
    "\n",
    "\n",
    "def assign_unk(tok):\n",
    "    \"\"\"\n",
    "    Assign unknown word tokens\n",
    "    \"\"\"\n",
    "    # Digits\n",
    "    if any(char.isdigit() for char in tok):\n",
    "        return \"--unk_digit--\"\n",
    "\n",
    "    # Punctuation\n",
    "    elif any(char in punct for char in tok):\n",
    "        return \"--unk_punct--\"\n",
    "\n",
    "    # Upper-case\n",
    "    elif any(char.isupper() for char in tok):\n",
    "        return \"--unk_upper--\"\n",
    "\n",
    "    # Nouns\n",
    "    elif any(tok.endswith(suffix) for suffix in noun_suffix):\n",
    "        return \"--unk_noun--\"\n",
    "\n",
    "    # Verbs\n",
    "    elif any(tok.endswith(suffix) for suffix in verb_suffix):\n",
    "        return \"--unk_verb--\"\n",
    "\n",
    "    # Adjectives\n",
    "    elif any(tok.endswith(suffix) for suffix in adj_suffix):\n",
    "        return \"--unk_adj--\"\n",
    "\n",
    "    # Adverbs\n",
    "    elif any(tok.endswith(suffix) for suffix in adv_suffix):\n",
    "        return \"--unk_adv--\"\n",
    "\n",
    "    return \"--unk--\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(vocab,data_fp):\n",
    "    \"\"\"\n",
    "    Preprocess data\n",
    "    \"\"\"\n",
    "    orig=[]\n",
    "    prep=[]\n",
    "    \n",
    "    with open(data_fp,\"r\") as data_file:\n",
    "        \n",
    "        for cnt,word in enumerate(data_file):\n",
    "            \n",
    "            # End of sentence\n",
    "            if not word.split():\n",
    "                orig.append(word.strip())\n",
    "                word='--n--'\n",
    "                prep.append(word)\n",
    "                continue\n",
    "            \n",
    "            # Handle unk\n",
    "            elif word.strip() not in vocab:\n",
    "                orig.append(word.strip())\n",
    "                word=assign_unk(word)\n",
    "                prep.append(word)\n",
    "                continue\n",
    "            \n",
    "            else:\n",
    "                orig.append(word.strip())\n",
    "                prep.append(word.strip())\n",
    "                \n",
    "    assert(len(orig) == len(open(data_fp, \"r\").readlines()))\n",
    "    assert(len(prep) == len(open(data_fp, \"r\").readlines()))\n",
    "\n",
    "    return orig, prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the preprocessed test corpus:  34199\n",
      "This is a sample of the test_corpus: \n",
      "['The', 'economy', \"'s\", 'temperature', 'will', 'be', 'taken', 'from', 'several', '--unk--']\n"
     ]
    }
   ],
   "source": [
    "#corpus without tags, preprocessed\n",
    "_, prep = preprocess(vocab, \"test.words\")     \n",
    "\n",
    "print('The length of the preprocessed test corpus: ', len(prep))\n",
    "print('This is a sample of the test_corpus: ')\n",
    "print(prep[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parts of speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word_tag(line, vocab): \n",
    "    if not line.split():\n",
    "        word = \"--n--\"\n",
    "        tag = \"--s--\"\n",
    "        return word, tag\n",
    "    else:\n",
    "        word, tag = line.split()\n",
    "        if word not in vocab: \n",
    "            # Handle unknown words\n",
    "            word = assign_unk(word)\n",
    "        return word, tag\n",
    "    return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dictionaries(training_corpus, vocab):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        training_corpus: a corpus where each line has a word followed by its tag.\n",
    "        vocab: a dictionary where keys are words in vocabulary and value is an index\n",
    "    Output: \n",
    "        emission_counts: a dictionary where the keys are (tag, word) and the values are the counts\n",
    "        transition_counts: a dictionary where the keys are (prev_tag, tag) and the values are the counts\n",
    "        tag_counts: a dictionary where the keys are the tags and the values are the counts\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the dictionaries using defaultdict\n",
    "    emission_counts=defaultdict(int)\n",
    "    transition_counts=defaultdict(int)\n",
    "    tag_counts=defaultdict(int)\n",
    "    \n",
    "    # Initialize 'prev_tag' (previous tag) with the start state\n",
    "    # denoted by '--s--'\n",
    "    \n",
    "    prev_tag='--s--'\n",
    "    \n",
    "    i=0\n",
    "    \n",
    "    # Each item in the training corpus contains a word and its POS tag\n",
    "    # Go through each word and its tafg in the training corpus\n",
    "    for word_tag in training_corpus:\n",
    "        \n",
    "        i+=1\n",
    "        \n",
    "        if i % 50000 == 0:\n",
    "            print(f'word count = {i}')\n",
    "        \n",
    "        word,tag=get_word_tag(word_tag,vocab)\n",
    "        \n",
    "        transition_counts[(prev_tag,tag)]+=1\n",
    "        \n",
    "        emission_counts[(tag,word)]+=1\n",
    "        \n",
    "        tag_counts[tag]+=1\n",
    "        \n",
    "        # Set the previous tag to this tag (for the next iteration of the loop)\n",
    "        prev_tag = tag\n",
    "        \n",
    "    return emission_counts, transition_counts, tag_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word count = 50000\n",
      "word count = 100000\n",
      "word count = 150000\n",
      "word count = 200000\n",
      "word count = 250000\n",
      "word count = 300000\n",
      "word count = 350000\n",
      "word count = 400000\n",
      "word count = 450000\n",
      "word count = 500000\n",
      "word count = 550000\n",
      "word count = 600000\n",
      "word count = 650000\n",
      "word count = 700000\n",
      "word count = 750000\n",
      "word count = 800000\n",
      "word count = 850000\n",
      "word count = 900000\n",
      "word count = 950000\n"
     ]
    }
   ],
   "source": [
    "emission_counts, transition_counts, tag_counts = create_dictionaries(training_corpus, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of POS tags (number of 'states'): 46\n",
      "View these POS tags (states)\n",
      "['#', '$', \"''\", '(', ')', ',', '--s--', '.', ':', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']\n"
     ]
    }
   ],
   "source": [
    "# Get all the POS states\n",
    "\n",
    "states=sorted(tag_counts.keys())\n",
    "print(f\"Number of POS tags (number of 'states'): {len(states)}\")\n",
    "print(\"View these POS tags (states)\")\n",
    "print(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transition examples: \n",
      "(('--s--', 'IN'), 5050)\n",
      "(('IN', 'DT'), 32364)\n",
      "(('DT', 'NNP'), 9044)\n",
      "\n",
      "emission examples: \n",
      "(('DT', 'any'), 721)\n",
      "(('NN', 'decrease'), 7)\n",
      "(('NN', 'insider-trading'), 5)\n",
      "\n",
      "ambiguous word example: \n",
      "('RB', 'back') 304\n",
      "('VB', 'back') 20\n",
      "('RP', 'back') 84\n",
      "('JJ', 'back') 25\n",
      "('NN', 'back') 29\n",
      "('VBP', 'back') 4\n"
     ]
    }
   ],
   "source": [
    "print(\"transition examples: \")\n",
    "for ex in list(transition_counts.items())[:3]:\n",
    "    print(ex)\n",
    "print()\n",
    "\n",
    "print(\"emission examples: \")\n",
    "for ex in list(emission_counts.items())[200:203]:\n",
    "    print (ex)\n",
    "print()\n",
    "\n",
    "print(\"ambiguous word example: \")\n",
    "for tup,cnt in emission_counts.items():\n",
    "    if tup[1] == 'back': \n",
    "        print (tup, cnt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_pos(prep,y,emission_counts,vocab,states):\n",
    "    '''\n",
    "    Input: \n",
    "        prep: a preprocessed version of 'y'. A list with the 'word' component of the tuples.\n",
    "        y: a corpus composed of a list of tuples where each tuple consists of (word, POS)\n",
    "        emission_counts: a dictionary where the keys are (tag,word) tuples and the value is the count\n",
    "        vocab: a dictionary where keys are words in vocabulary and value is an index\n",
    "        states: a sorted list of all possible tags for this assignment\n",
    "    Output: \n",
    "        accuracy: Number of times you classified a word correctly\n",
    "    '''\n",
    "    \n",
    "    num_corret=0\n",
    "    \n",
    "    # Get the (tag,word) tuples stored as set\n",
    "    all_words=set(emission_counts.keys())\n",
    "    \n",
    "    # Get the number of (word,POS) tuples in th corpus y\n",
    "    total=len(y)\n",
    "    \n",
    "    for word,y_tup in zip(prep,y):\n",
    "        \n",
    "        # Split the (word,POS) string into a list of towo items\n",
    "        y_tup_l=y_tup.split()\n",
    "        \n",
    "        # Verify the y_tup contain both word and POS\n",
    "        if len(y_tup_l)==2:\n",
    "            \n",
    "            # Set the true POS label for this word\n",
    "            true_label=y_tup_l[1]\n",
    "            \n",
    "        else:\n",
    "            # If the y_tup didn't contain word and POS\n",
    "            continue\n",
    "            \n",
    "        count_final=0\n",
    "        pos_final=''\n",
    "        \n",
    "        # If the word is in the vocabulary\n",
    "        if word in vocab:\n",
    "            \n",
    "            for pos in states:\n",
    "                \n",
    "                # Define the key as the tuple containing the POS and word\n",
    "                key=(pos,word)\n",
    "                \n",
    "                # Check if the (pos,word) key exists in the emission_conuts dictionary\n",
    "                if key in emission_counts:\n",
    "                    \n",
    "                    # Get the emission count of the (pos,word) tuple\n",
    "                    count=emission_counts[key]\n",
    "                    \n",
    "                    # Keep track of the POS with the largest count\n",
    "                    if count>count_final:\n",
    "                        \n",
    "                        # update the final count\n",
    "                        count_final=count\n",
    "                        \n",
    "                        # update the final pos\n",
    "                        pos_final=pos\n",
    "                        \n",
    "            # If the final POS (with the largest count) matches the true POS:            \n",
    "            if pos_final == true_label:\n",
    "                \n",
    "                # update the number of correct predictions\n",
    "                num_corret+=1\n",
    "                \n",
    "    accuracy=num_corret/total\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of prediction using predict_pos is 0.8889\n"
     ]
    }
   ],
   "source": [
    "accuracy_predict_pos = predict_pos(prep, y, emission_counts, vocab, states)\n",
    "print(f\"Accuracy of prediction using predict_pos is {accuracy_predict_pos:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hidden Markov Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_transition_matrix(alpha, tag_counts, transition_counts):\n",
    "    ''' \n",
    "    Input: \n",
    "        alpha: number used for smoothing\n",
    "        tag_counts: a dictionary mapping each tag to its respective count\n",
    "        transition_counts: transition count for the previous word and tag\n",
    "    Output:\n",
    "        A: matrix of dimension (num_tags,num_tags)\n",
    "    '''\n",
    "    \n",
    "    # Get a sorted list of unique POS tags\n",
    "    all_tags = sorted(tag_counts.keys())\n",
    "    \n",
    "    # Get the number of unique POS tags\n",
    "    num_tags = len(all_tags)\n",
    "    \n",
    "    # Initialize the transition matrix 'A'\n",
    "    A = np.zeros((num_tags,num_tags))\n",
    "    \n",
    "    # Get the unique transition tuples (previous POS,current POS)\n",
    "    trains_keys=set(transition_counts.keys())\n",
    "    \n",
    "    for i in range(num_tags):\n",
    "        \n",
    "        for j in range(num_tags):\n",
    "        \n",
    "            count=0\n",
    "\n",
    "            # Get the tag as position i and tag at position j (from the all_tags list)\n",
    "            key = (all_tags[i],all_tags[j])\n",
    "\n",
    "            # exists in the transition counts dictionary\n",
    "            if key in transition_counts:\n",
    "\n",
    "                # for the (prev POS , current POS) tuple\n",
    "                count = transition_counts[key]\n",
    "\n",
    "            # Get the count of the previous tag (index postion i) from tag_counts\n",
    "            count_prev_tag = tag_counts[all_tags[i]]\n",
    "\n",
    "            # Apply smoothing using count of the tuple , alpha\n",
    "            # count of previous tag, alpha, and the number of total tags\n",
    "            A[i,j] = (count + alpha) / (count_prev_tag +alpha*num_tags)\n",
    "        \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A at row 0, col 0: 0.000007040\n",
      "A at row 3, col 1: 0.1691\n",
      "              RBS            RP           SYM        TO            UH\n",
      "RBS  2.217069e-06  2.217069e-06  2.217069e-06  0.008870  2.217069e-06\n",
      "RP   3.756509e-07  7.516775e-04  3.756509e-07  0.051089  3.756509e-07\n",
      "SYM  1.722772e-05  1.722772e-05  1.722772e-05  0.000017  1.722772e-05\n",
      "TO   4.477336e-05  4.472863e-08  4.472863e-08  0.000090  4.477336e-05\n",
      "UH   1.030439e-05  1.030439e-05  1.030439e-05  0.061837  3.092348e-02\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.001\n",
    "for i in range(46):\n",
    "    tag_counts.pop(i,None)\n",
    "\n",
    "A = create_transition_matrix(alpha, tag_counts, transition_counts)\n",
    "# Testing your function\n",
    "print(f\"A at row 0, col 0: {A[0,0]:.9f}\")\n",
    "print(f\"A at row 3, col 1: {A[3,1]:.4f}\")\n",
    "\n",
    "#print(\"View a subset of transition matrix A\")\n",
    "A_sub = pd.DataFrame(A[30:35,30:35], index=states[30:35], columns = states[30:35] )\n",
    "print(A_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_emission_matrix(alpha, tag_counts, emission_counts, vocab):\n",
    "    '''\n",
    "    Input: \n",
    "        alpha: tuning parameter used in smoothing \n",
    "        tag_counts: a dictionary mapping each tag to its respective count\n",
    "        emission_counts: a dictionary where the keys are (tag, word) and the values are the counts\n",
    "        vocab: a dictionary where keys are words in vocabulary and value is an index\n",
    "    Output:\n",
    "        B: a matrix of dimension (num_tags, len(vocab))\n",
    "    '''\n",
    "    \n",
    "    # Get the number of POS tag\n",
    "    num_tags=len(tag_counts)\n",
    "    \n",
    "    # Get a list of all POS tags\n",
    "    all_tags=sorted(tag_counts.keys())\n",
    "    \n",
    "    # Get the total number of unique words in the vocabulary\n",
    "    num_words=len(vocab)\n",
    "    \n",
    "    # Initialize the emission matrix B with places for \n",
    "    # tags in the rows and words in the columns\n",
    "    B = np.zeros((num_tags,num_words))\n",
    "    \n",
    "    # Get a set of all (POS,word) tuples\n",
    "    # from the keys of the emisson_counts dictionary\n",
    "    emis_keys=set(list(emission_counts.keys()))\n",
    "    \n",
    "    for i in range(num_tags):\n",
    "        \n",
    "        for j in range(num_words):\n",
    "            \n",
    "            count=0\n",
    "            \n",
    "            # Define the (POS tag,word) tuple for this row and column\n",
    "            key = (all_tags[i],vocab[j])\n",
    "            \n",
    "            # Check if the (POS tag,word) tuple exists as a key in emission counts\n",
    "            if key in emission_counts.keys():\n",
    "                \n",
    "                # Get the count of (POS tag,word) from the emission_counts\n",
    "                count = emission_counts[key]\n",
    "                \n",
    "            # Get the count of the POS tag\n",
    "            count_tag = tag_counts[all_tags[i]]\n",
    "            \n",
    "            # Apply smoothing and store the smoothed value \n",
    "            # into the emission matrix B for this row and column\n",
    "            B[i,j]=(count + alpha)/(count_tag+alpha*num_words)\n",
    "            \n",
    "    return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View Matrix position at row 0, column 0: 0.000006032\n",
      "View Matrix position at row 3, column 1: 0.000000720\n",
      "              725      adroitly     engineers      promoted       synergy\n",
      "CD   8.201296e-05  2.732854e-08  2.732854e-08  2.732854e-08  2.732854e-08\n",
      "NN   7.521128e-09  7.521128e-09  7.521128e-09  7.521128e-09  2.257091e-05\n",
      "NNS  1.670013e-08  1.670013e-08  4.676203e-04  1.670013e-08  1.670013e-08\n",
      "VB   3.779036e-08  3.779036e-08  3.779036e-08  3.779036e-08  3.779036e-08\n",
      "RB   3.226454e-08  6.456135e-05  3.226454e-08  3.226454e-08  3.226454e-08\n",
      "RP   3.723317e-07  3.723317e-07  3.723317e-07  3.723317e-07  3.723317e-07\n"
     ]
    }
   ],
   "source": [
    "# creating your emission probability matrix. this takes a few minutes to run. \n",
    "for i in range(46):\n",
    "    tag_counts.pop(i,None)\n",
    "\n",
    "B = create_emission_matrix(alpha, tag_counts, emission_counts, list(vocab))\n",
    "\n",
    "print(f\"View Matrix position at row 0, column 0: {B[0,0]:.9f}\")\n",
    "print(f\"View Matrix position at row 3, column 1: {B[3,1]:.9f}\")\n",
    "\n",
    "# Try viewing emissions for a few words in a sample dataframe\n",
    "cidx  = ['725','adroitly','engineers', 'promoted', 'synergy']\n",
    "\n",
    "# Get the integer ID for each word\n",
    "cols = [vocab[a] for a in cidx]\n",
    "\n",
    "# Choose POS tags to show in a sample dataframe\n",
    "rvals =['CD','NN','NNS', 'VB','RB','RP']\n",
    "\n",
    "# For each POS tag, get the row number from the 'states' list\n",
    "rows = [states.index(a) for a in rvals]\n",
    "\n",
    "# Get the emissions for the sample of words, and the sample of POS tags\n",
    "B_sub = pd.DataFrame(B[np.ix_(rows,cols)], index=rvals, columns = cidx )\n",
    "print(B_sub)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Viterbi Algorithm and Dynamic Programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize(states, tag_counts, A, B, corpus, vocab):\n",
    "    '''\n",
    "    Input: \n",
    "        states: a list of all possible parts-of-speech\n",
    "        tag_counts: a dictionary mapping each tag to its respective count\n",
    "        A: Transition Matrix of dimension (num_tags, num_tags)\n",
    "        B: Emission Matrix of dimension (num_tags, len(vocab))\n",
    "        corpus: a sequence of words whose POS is to be identified in a list \n",
    "        vocab: a dictionary where keys are words in vocabulary and value is an index\n",
    "    Output:\n",
    "        best_probs: matrix of dimension (num_tags, len(corpus)) of floats\n",
    "        best_paths: matrix of dimension (num_tags, len(corpus)) of integers\n",
    "    '''\n",
    "    # Get the total number of unique POS tags\n",
    "    num_tags=len(tag_counts)\n",
    "    \n",
    "    # Initialize best_probs matrix\n",
    "    # POS tag in the rows, number of words in the corpus as the columns\n",
    "    best_probs=np.zeros((num_tags,len(corpus)))\n",
    "    \n",
    "    # Initialize best_path matrix\n",
    "    # POS tag in the rows, number of words in the corpus as columns\n",
    "    best_paths=np.zeros((num_tags,len(corpus)),dtype=int)\n",
    "    \n",
    "    # Define the start token\n",
    "    s_idx=states.index(\"--s--\")\n",
    "    \n",
    "    for i in range(num_tags):\n",
    "        \n",
    "        # Handle the special case when the transition from start token to POS tag i is zero\n",
    "        if A[s_idx,i] == 0:\n",
    "            \n",
    "            # Initialize best_probs as POS tag 'i',column 0,to negative infinity\n",
    "            best_probs[i,0]=float('-inf')\n",
    "        \n",
    "        # For all other cases when transition from start token to POS tag i is non-zero:\n",
    "        else:\n",
    "            # Initialize best_probs at POS tag 'i', column 0\n",
    "            # Check the formula in the instructions above\n",
    "            best_probs[i,0] = math.log(A[s_idx,i]) + math.log(B[i,vocab[corpus[0]]] )\n",
    "    \n",
    "    return best_probs, best_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_probs, best_paths = initialize(states, tag_counts, A, B, prep, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_probs[0,0]: -22.6098\n",
      "best_paths[2,3]: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Test the function\n",
    "print(f\"best_probs[0,0]: {best_probs[0,0]:.4f}\") \n",
    "print(f\"best_paths[2,3]: {best_paths[2,3]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def viterbi_forward(A, B, test_corpus, best_probs, best_paths, vocab):\n",
    "    '''\n",
    "    Input: \n",
    "        A, B: The transiton and emission matrices respectively\n",
    "        test_corpus: a list containing a preprocessed corpus\n",
    "        best_probs: an initilized matrix of dimension (num_tags, len(corpus))\n",
    "        best_paths: an initilized matrix of dimension (num_tags, len(corpus))\n",
    "        vocab: a dictionary where keys are words in vocabulary and value is an index \n",
    "    Output: \n",
    "        best_probs: a completed matrix of dimension (num_tags, len(corpus))\n",
    "        best_paths: a completed matrix of dimension (num_tags, len(corpus))\n",
    "    '''\n",
    "    # Get the number of unique POS tags (which is the num of rows in best_probs)\n",
    "    num_tags = best_probs.shape[0]\n",
    "    \n",
    "    # Go through every word in the corpus starting from word 1\n",
    "    # Recall that word 0 was initialized in `initialize()`\n",
    "    for i in range(1, len(test_corpus)): \n",
    "        \n",
    "        # Print number of words processed, every 5000 words\n",
    "        if i % 5000 == 0:\n",
    "            print(\"Words processed: {:>8}\".format(i))\n",
    "            \n",
    "        ### START CODE HERE (Replace instances of 'None' with your code EXCEPT the first 'best_path_i = None') ###\n",
    "        # For each unique POS tag that the current word can be\n",
    "        for j in range(num_tags): # complete this line\n",
    "            \n",
    "            # Initialize best_prob for word i to negative infinity\n",
    "            best_prob_i =  float(\"-inf\")\n",
    "            \n",
    "            # Initialize best_path for current word i to None\n",
    "            best_path_i = None\n",
    "\n",
    "            # For each POS tag that the previous word can be:\n",
    "            for k in range(num_tags): # complete this line\n",
    "            \n",
    "                # Calculate the probability = \n",
    "                # best probs of POS tag k, previous word i-1 + \n",
    "                # log(prob of transition from POS k to POS j) + \n",
    "                # log(prob that emission of POS j is word i)\n",
    "                prob = best_probs[k,i-1]+math.log(A[k,j]) +math.log(B[j,vocab[test_corpus[i]]])\n",
    "                # check if this path's probability is greater than\n",
    "                # the best probability up to and before this point\n",
    "                if prob > best_prob_i: # complete this line\n",
    "                    \n",
    "                    # Keep track of the best probability\n",
    "                    best_prob_i = prob\n",
    "                    \n",
    "                    # keep track of the POS tag of the previous word\n",
    "                    # that is part of the best path.  \n",
    "                    # Save the index (integer) associated with \n",
    "                    # that previous word's POS tag\n",
    "                    best_path_i = k\n",
    "\n",
    "            # Save the best probability for the \n",
    "            # given current word's POS tag\n",
    "            # and the position of the current word inside the corpus\n",
    "            best_probs[j,i] = best_prob_i\n",
    "            \n",
    "            # Save the unique integer ID of the previous POS tag\n",
    "            # into best_paths matrix, for the POS tag of the current word\n",
    "            # and the position of the current word inside the corpus.\n",
    "            best_paths[j,i] = best_path_i\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "    return best_probs, best_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words processed:     5000\n",
      "Words processed:    10000\n",
      "Words processed:    15000\n",
      "Words processed:    20000\n",
      "Words processed:    25000\n",
      "Words processed:    30000\n"
     ]
    }
   ],
   "source": [
    "# this will take a few minutes to run => processes ~ 30,000 words\n",
    "best_probs, best_paths = viterbi_forward(A, B, prep, best_probs, best_paths, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test this function \n",
    "print(f\"best_probs[0,1]: {best_probs[0,1]:.4f}\") \n",
    "print(f\"best_probs[0,4]: {best_probs[0,4]:.4f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Viterbi Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def viterbi_backward(best_probs, best_paths, corpus, states):\n",
    "    '''\n",
    "    This function returns the best path.\n",
    "    \n",
    "    '''\n",
    "    # Get the number of words in the corpus\n",
    "    # which is also the number of columns in best_probs, best_paths\n",
    "    m = best_paths.shape[1] \n",
    "    \n",
    "    # Initialize array z, same length as the corpus\n",
    "    z = [None] * m\n",
    "    \n",
    "    # Get the number of unique POS tags\n",
    "    num_tags = best_probs.shape[0]\n",
    "    \n",
    "    # Initialize the best probability for the last word\n",
    "    best_prob_for_last_word = float('-inf')\n",
    "    \n",
    "    # Initialize pred array, same length as corpus\n",
    "    pred = [None] * m\n",
    "    \n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "    ## Step 1 ##\n",
    "    \n",
    "    # Go through each POS tag for the last word (last column of best_probs)\n",
    "    # in order to find the row (POS tag integer ID) \n",
    "    # with highest probability for the last word\n",
    "    for k in range(num_tags): # complete this line\n",
    "\n",
    "        # If the probability of POS tag at row k \n",
    "        # is better than the previosly best probability for the last word:\n",
    "        if best_probs[k,-1]>best_prob_for_last_word: # complete this line\n",
    "            \n",
    "            # Store the new best probability for the last word\n",
    "            best_prob_for_last_word = best_probs[k,-1]\n",
    "    \n",
    "            # Store the unique integer ID of the POS tag\n",
    "            # which is also the row number in best_probs\n",
    "            z[m - 1]=k\n",
    "            \n",
    "    # Convert the last word's predicted POS tag\n",
    "    # from its unique integer ID into the string representation\n",
    "    # using the 'states' dictionary\n",
    "    # store this in the 'pred' array for the last word\n",
    "    pred[m - 1] = states[k]\n",
    "    \n",
    "    ## Step 2 ##\n",
    "    # Find the best POS tags by walking backward through the best_paths\n",
    "    # From the last word in the corpus to the 0th word in the corpus\n",
    "    for i in range(len(corpus)-1, -1, -1): # complete this line\n",
    "        \n",
    "        # Retrieve the unique integer ID of\n",
    "        # the POS tag for the word at position 'i' in the corpus\n",
    "        pos_tag_for_word_i = best_paths[np.argmax(best_probs[:,i]),i]\n",
    "        \n",
    "        # In best_paths, go to the row representing the POS tag of word i\n",
    "        # and the column representing the word's position in the corpus\n",
    "        # to retrieve the predicted POS for the word at position i-1 in the corpus\n",
    "        z[i - 1] = best_paths[pos_tag_for_word_i,i]\n",
    "        \n",
    "        # Get the previous word's POS tag in string form\n",
    "        # Use the 'states' dictionary, \n",
    "        # where the key is the unique integer ID of the POS tag,\n",
    "        # and the value is the string representation of that POS tag\n",
    "        pred[i - 1] = states[pos_tag_for_word_i]\n",
    "        \n",
    "     ### END CODE HERE ###\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction for pred[-7:m-1] is: \n",
      " ['see', 'them', 'here', 'with', 'us', '.'] \n",
      " ['VB', 'PRP', 'RB', 'IN', 'PRP', '.'] \n",
      "\n",
      "The prediction for pred[0:8] is: \n",
      " ['DT', 'NN', 'POS', 'NN', 'MD', 'VB', 'VBN'] \n",
      " ['The', 'economy', \"'s\", 'temperature', 'will', 'be', 'taken']\n"
     ]
    }
   ],
   "source": [
    "# Run and test your function\n",
    "pred = viterbi_backward(best_probs, best_paths, prep, states)\n",
    "m=len(pred)\n",
    "print('The prediction for pred[-7:m-1] is: \\n', prep[-7:m-1], \"\\n\", pred[-7:m-1], \"\\n\")\n",
    "print('The prediction for pred[0:8] is: \\n', pred[0:7], \"\\n\", prep[0:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predicting a data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The third word is: temperature\n",
      "Your prediction is: NN\n",
      "Your corresponding label y is:  temperature\tNN\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('The third word is:', prep[3])\n",
    "print('Your prediction is:', pred[3])\n",
    "print('Your corresponding label y is: ', y[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(pred, y):\n",
    "    '''\n",
    "    Input: \n",
    "        pred: a list of the predicted parts-of-speech \n",
    "        y: a list of lines where each word is separated by a '\\t' (i.e. word \\t tag)\n",
    "    Output: \n",
    "        \n",
    "    '''\n",
    "    num_correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Zip together the prediction and the labels\n",
    "    for prediction, y in zip(pred, y):\n",
    "        ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "        # Split the label into the word and the POS tag\n",
    "        word_tag_tuple = y.split()\n",
    "        \n",
    "        # Check that there is actually a word and a tag\n",
    "        # no more and no less than 2 items\n",
    "        if len(word_tag_tuple)!=2: # complete this line\n",
    "            continue \n",
    "        # store the word and tag separately\n",
    "        word, tag = word_tag_tuple\n",
    "        # Check if the POS tag label matches the prediction\n",
    "        if prediction == tag: # complete this line\n",
    "            # count the number of times that the prediction\n",
    "            # and label match\n",
    "            num_correct += 1\n",
    "            \n",
    "        # keep track of the total number of examples (that have valid labels)\n",
    "        total += 1\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "    return (num_correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Viterbi algorithm is 0.9528\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy of the Viterbi algorithm is {compute_accuracy(pred, y):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Auto Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type: <class 'str'>\n",
      "Number of letters: 3335477\n",
      "First 300 letters of the data\n",
      "-------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"How are you? Btw thanks for the RT. You gonna be in DC anytime soon? Love to see you. Been way, way too long.\\nWhen you meet someone special... you'll know. Your heart will beat more rapidly and you'll smile for no reason.\\nthey've decided its more fun if I don't.\\nSo Tired D; Played Lazer Tag & Ran A \""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "Last 300 letters of the data\n",
      "-------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"ust had one a few weeks back....hopefully we will be back soon! wish you the best yo\\nColombia is with an 'o'...“: We now ship to 4 countries in South America (fist pump). Please welcome Columbia to the Stunner Family”\\n#GutsiestMovesYouCanMake Giving a cat a bath.\\nCoffee after 5 was a TERRIBLE idea.\\n\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n"
     ]
    }
   ],
   "source": [
    "with open(\"en_US.twitter.txt\",\"r\") as f:\n",
    "    data = f.read()\n",
    "    \n",
    "print(\"Data type:\", type(data))\n",
    "print(\"Number of letters:\", len(data))\n",
    "print(\"First 300 letters of the data\")\n",
    "print(\"-------\")\n",
    "display(data[0:300])\n",
    "print(\"-------\")\n",
    "\n",
    "print(\"Last 300 letters of the data\")\n",
    "print(\"-------\")\n",
    "display(data[-300:])\n",
    "print(\"-------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_to_sentences(data):\n",
    "    \"\"\"\n",
    "    Split data by linebreak \"\\n\"\n",
    "    \n",
    "    Args:\n",
    "        data: str\n",
    "    \n",
    "    Returns:\n",
    "        A list of sentences\n",
    "    \"\"\"\n",
    "    \n",
    "    sentences = data.split('\\n')\n",
    "    \n",
    "    # Additional clearning (This part is already implemented)\n",
    "    # - Remove leading and trailing spaces from each sentence\n",
    "    # - Drop sentences if they are empty strings.\n",
    "    \n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    sentences = [s for s in sentences if len(s) > 0]\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I have a pen.\n",
      "I have an apple. \n",
      "Ah\n",
      "Apple pen.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['I have a pen.', 'I have an apple.', 'Ah', 'Apple pen.']"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test your code\n",
    "x = \"\"\"\n",
    "I have a pen.\\nI have an apple. \\nAh\\nApple pen.\\n\n",
    "\"\"\"\n",
    "print(x)\n",
    "\n",
    "split_to_sentences(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_sentences(sentences):\n",
    "    \"\"\"\n",
    "    Tokenize sentences into tokens (words)\n",
    "    \n",
    "    Args:\n",
    "        sentences: List of strings\n",
    "    \n",
    "    Returns:\n",
    "        List of lists of tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the list of lists of tokenized sentences\n",
    "    tokenized_sentences = []\n",
    "    \n",
    "    # go through each sentence \n",
    "    for sentence in sentences:\n",
    "        \n",
    "        # convert to lowercase letters\n",
    "        sentence = sentence.lower()\n",
    "        \n",
    "        # convert to a list of words\n",
    "        tokenized = nltk.word_tokenize(sentence)\n",
    "        \n",
    "        # append the list of words to the list of lists\n",
    "        tokenized_sentences.append(tokenized)\n",
    "        \n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['sky', 'is', 'blue', '.'],\n",
       " ['leaves', 'are', 'green', '.'],\n",
       " ['roses', 'are', 'red', '.']]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\"Sky is blue.\", \"Leaves are green.\", \"Roses are red.\"]\n",
    "tokenize_sentences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tokenized_data(data):\n",
    "    \"\"\"\n",
    "    Make a list of tokenized sentences\n",
    "    \n",
    "    Args:\n",
    "        data: String\n",
    "    \n",
    "    Returns:\n",
    "        List of lists of tokens\n",
    "    \"\"\"\n",
    "    # get the sentences bt splitting up the data\n",
    "    sentences = split_to_sentences(data)\n",
    "    \n",
    "    # get the list of lists of tokens by tokenizing the sentences\n",
    "    tokenized_sentences = tokenize_sentences(sentences)\n",
    "    \n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['sky', 'is', 'blue', '.'],\n",
       " ['leaves', 'are', 'green'],\n",
       " ['roses', 'are', 'red', '.']]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = \"Sky is blue.\\nLeaves are green\\nRoses are red.\"\n",
    "get_tokenized_data(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split in train and test sets\n",
    "tokenized_data = get_tokenized_data(data)\n",
    "random.seed(87)\n",
    "random.shuffle(tokenized_data)\n",
    "\n",
    "train_size = int(len(tokenized_data) * 0.8)\n",
    "train_data = tokenized_data[0:train_size]\n",
    "test_data = tokenized_data[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47961 data are split into 38368 train and 9593 test set\n",
      "First training sample:\n",
      "['i', 'personally', 'would', 'like', 'as', 'our', 'official', 'glove', 'of', 'the', 'team', 'local', 'company', 'and', 'quality', 'production']\n",
      "First test sample\n",
      "['that', 'picture', 'i', 'just', 'seen', 'whoa', 'dere', '!', '!', '>', '>', '>', '>', '>', '>', '>']\n"
     ]
    }
   ],
   "source": [
    "print(\"{} data are split into {} train and {} test set\".format(\n",
    "    len(tokenized_data), len(train_data), len(test_data)))\n",
    "\n",
    "print(\"First training sample:\")\n",
    "print(train_data[0])\n",
    "      \n",
    "print(\"First test sample\")\n",
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_words(tokenized_sentences):\n",
    "    \"\"\"\n",
    "    Count the number of word appearence in the tokenized sentences\n",
    "    \n",
    "    Args:\n",
    "        tokenized_sentences: List of lists of strings\n",
    "    \n",
    "    Returns:\n",
    "        dict that maps word (str) to the frequency (int)\n",
    "    \"\"\"\n",
    "    word_counts = {}\n",
    "    \n",
    "    # Loop through the sentences\n",
    "    for sentence in tokenized_sentences:\n",
    "        \n",
    "        for token in sentence:\n",
    "            \n",
    "            # If the token is not in the dictionary yet,set the count to 1\n",
    "            if token not in word_counts.keys():\n",
    "                word_counts[token] = 1\n",
    "            \n",
    "            # If the token already in the dictionary,increment the count by 1\n",
    "            else:\n",
    "                word_counts[token] += 1\n",
    "                \n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': 3,\n",
       " 'are': 2,\n",
       " 'blue': 1,\n",
       " 'green': 1,\n",
       " 'is': 1,\n",
       " 'leaves': 1,\n",
       " 'red': 1,\n",
       " 'roses': 1,\n",
       " 'sky': 1}"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentences = [['sky', 'is', 'blue', '.'],\n",
    "                       ['leaves', 'are', 'green', '.'],\n",
    "                       ['roses', 'are', 'red', '.']]\n",
    "count_words(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_words_with_nplus_frequency(tokenized_sentences, count_threshold):\n",
    "    \"\"\"\n",
    "    Find the words that appear N times or more\n",
    "    \n",
    "    Args:\n",
    "        tokenized_sentences: List of lists of sentences\n",
    "        count_threshold: minimum number of occurrences for a word to be in the closed vocabulary.\n",
    "    \n",
    "    Returns:\n",
    "        List of words that appear N times or more\n",
    "    \"\"\"\n",
    "    # Initialize ant empty list to contain in the words that\n",
    "    # appear at least N times\n",
    "    closed_vocab = []\n",
    "    \n",
    "    # Get the word counts of the tokenized sentences\n",
    "    # Use the function that you defined earlier to count the words\n",
    "    word_counts = count_words(tokenized_sentences)\n",
    "    \n",
    "    for word,cnt in word_counts.items():\n",
    "        \n",
    "        # Check that the word's count\n",
    "        # is at least as greater as the minimum count\n",
    "        \n",
    "        if cnt >= count_threshold:\n",
    "            closed_vocab.append(word)\n",
    "            \n",
    "    return closed_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closed vocabulary:\n",
      "['.', 'are']\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentences = [['sky', 'is', 'blue', '.'],\n",
    "                       ['leaves', 'are', 'green', '.'],\n",
    "                       ['roses', 'are', 'red', '.']]\n",
    "tmp_closed_vocab = get_words_with_nplus_frequency(tokenized_sentences, count_threshold=2)\n",
    "print(f\"Closed vocabulary:\")\n",
    "print(tmp_closed_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_oov_words_by_unk(tokenized_sentences, vocabulary, unknown_token=\"<unk>\"):\n",
    "    \"\"\"\n",
    "    Replace words not in the given vocabulary with '<unk>' token.\n",
    "    \n",
    "    Args:\n",
    "        tokenized_sentences: List of lists of strings\n",
    "        vocabulary: List of strings that we will use\n",
    "        unknown_token: A string representing unknown (out-of-vocabulary) words\n",
    "    \n",
    "    Returns:\n",
    "        List of lists of strings, with words not in the vocabulary replaced\n",
    "    \"\"\"\n",
    "    # Place vocabulary into a set for faster search\n",
    "    vocabulary = set(vocabulary)\n",
    "    \n",
    "    # Initialize a list that will hold the sentences \n",
    "    # after less frequent words are replaced by the unk\n",
    "    replaced_tokenized_sentences = []\n",
    "    \n",
    "    for sentence in tokenized_sentences:\n",
    "        \n",
    "        # Initialize the list that will contain\n",
    "        # a single sentence with unk replacements\n",
    "        replaced_sentence = []\n",
    "        \n",
    "        for token in sentence:\n",
    "            \n",
    "            if token in vocabulary:\n",
    "                replaced_sentence.append(token)\n",
    "                \n",
    "            else:\n",
    "                replaced_sentence.append(unknown_token)\n",
    "                \n",
    "        replaced_tokenized_sentences.append(replaced_sentence)\n",
    "        \n",
    "    return replaced_tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence:\n",
      "[['dogs', 'run'], ['cats', 'sleep']]\n",
      "tokenized_sentences with less frequent words converted to '<unk>':\n",
      "[['dogs', '<unk>'], ['<unk>', 'sleep']]\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentences = [[\"dogs\", \"run\"], [\"cats\", \"sleep\"]]\n",
    "vocabulary = [\"dogs\", \"sleep\"]\n",
    "tmp_replaced_tokenized_sentences = replace_oov_words_by_unk(tokenized_sentences, vocabulary)\n",
    "print(f\"Original sentence:\")\n",
    "print(tokenized_sentences)\n",
    "print(f\"tokenized_sentences with less frequent words converted to '<unk>':\")\n",
    "print(tmp_replaced_tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_data(train_data, test_data, count_threshold):\n",
    "    \"\"\"\n",
    "    Preprocess data, i.e.,\n",
    "        - Find tokens that appear at least N times in the training data.\n",
    "        - Replace tokens that appear less than N times by \"<unk>\" both for training and test data.        \n",
    "    Args:\n",
    "        train_data, test_data: List of lists of strings.\n",
    "        count_threshold: Words whose count is less than this are \n",
    "                      treated as unknown.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of\n",
    "        - training data with low frequent words replaced by \"<unk>\"\n",
    "        - test data with low frequent words replaced by \"<unk>\"\n",
    "        - vocabulary of words that appear n times or more in the training data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the closed vocabulary using the train data\n",
    "    vocabulary = get_words_with_nplus_frequency(train_data,count_threshold)\n",
    "    \n",
    "    # For the train data, replace less common words with \"<unk>\"\n",
    "    train_data_replaced = replace_oov_words_by_unk(train_data,vocabulary)\n",
    "    \n",
    "    # For the test data, replace less common words with \"<unk>\"\n",
    "    test_data_replaced = replace_oov_words_by_unk(test_data,vocabulary)\n",
    "\n",
    "    return train_data_replaced, test_data_replaced, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp_train_repl\n",
      "[['sky', 'is', 'blue', '.'], ['leaves', 'are', 'green']]\n",
      "\n",
      "tmp_test_repl\n",
      "[['<unk>', 'are', '<unk>', '.']]\n",
      "\n",
      "tmp_vocab\n",
      "['sky', 'is', 'blue', '.', 'leaves', 'are', 'green']\n"
     ]
    }
   ],
   "source": [
    "tmp_train = [['sky', 'is', 'blue', '.'],\n",
    "     ['leaves', 'are', 'green']]\n",
    "tmp_test = [['roses', 'are', 'red', '.']]\n",
    "\n",
    "tmp_train_repl, tmp_test_repl, tmp_vocab = preprocess_data(tmp_train, \n",
    "                                                           tmp_test, \n",
    "                                                           count_threshold = 1)\n",
    "\n",
    "print(\"tmp_train_repl\")\n",
    "print(tmp_train_repl)\n",
    "print()\n",
    "print(\"tmp_test_repl\")\n",
    "print(tmp_test_repl)\n",
    "print()\n",
    "print(\"tmp_vocab\")\n",
    "print(tmp_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "minimum_freq = 2\n",
    "train_data_processed, test_data_processed, vocabulary = preprocess_data(train_data, \n",
    "                                                                        test_data, \n",
    "                                                                        minimum_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First preprocessed training sample:\n",
      "['i', 'personally', 'would', 'like', 'as', 'our', 'official', 'glove', 'of', 'the', 'team', 'local', 'company', 'and', 'quality', 'production']\n",
      "\n",
      "First preprocessed test sample:\n",
      "['that', 'picture', 'i', 'just', 'seen', 'whoa', 'dere', '!', '!', '>', '>', '>', '>', '>', '>', '>']\n",
      "\n",
      "First 10 vocabulary:\n",
      "['i', 'personally', 'would', 'like', 'as', 'our', 'official', 'glove', 'of', 'the']\n",
      "\n",
      "Size of vocabulary: 14821\n"
     ]
    }
   ],
   "source": [
    "print(\"First preprocessed training sample:\")\n",
    "print(train_data_processed[0])\n",
    "print()\n",
    "print(\"First preprocessed test sample:\")\n",
    "print(test_data_processed[0])\n",
    "print()\n",
    "print(\"First 10 vocabulary:\")\n",
    "print(vocabulary[0:10])\n",
    "print()\n",
    "print(\"Size of vocabulary:\", len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# n-grams\n",
    "def count_n_grams(data, n, start_token='<s>', end_token = '<e>'):\n",
    "    \"\"\"\n",
    "    Count all n-grams in the data\n",
    "    \n",
    "    Args:\n",
    "        data: List of lists of words\n",
    "        n: number of words in a sequence\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary that maps a tuple of n-words to its frequency\n",
    "    \"\"\"\n",
    "    # Initialize dictionary of n-grams and their counts\n",
    "    n_grams = {}\n",
    "    \n",
    "    for sentence in data:\n",
    "        \n",
    "        # prepend start token n times, and append <e> one time\n",
    "        sentence = [start_token] * n + sentence + [end_token]\n",
    "        \n",
    "        # convert list to tuple \n",
    "        # So that the sequence of words can be used as\n",
    "        # a key in the dictionary\n",
    "        sentence = tuple(sentence)\n",
    "        \n",
    "        # Use i to indicate the start of the n-gram\n",
    "        # from index 0\n",
    "        # to the last index where the end of the n-gram\n",
    "        # is within the sentence\n",
    "        \n",
    "        m = len(sentence) if n==1 else len(sentence) -1 \n",
    "        \n",
    "        for i in range(m):\n",
    "            \n",
    "            # get the n-gram is in the dictionary\n",
    "            n_gram = sentence[i: i + n]\n",
    "            \n",
    "            # check if the n-gram if in the dictionary\n",
    "            if n_gram in n_grams.keys():\n",
    "                n_grams[n_gram] += 1\n",
    "            else:\n",
    "                n_grams[n_gram] = 1\n",
    "                \n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uni-gram:\n",
      "{('<s>',): 2, ('i',): 1, ('like',): 2, ('a',): 2, ('cat',): 2, ('<e>',): 2, ('this',): 1, ('dog',): 1, ('is',): 1}\n",
      "Bi-gram:\n",
      "{('<s>', '<s>'): 2, ('<s>', 'i'): 1, ('i', 'like'): 1, ('like', 'a'): 2, ('a', 'cat'): 2, ('cat', '<e>'): 2, ('<s>', 'this'): 1, ('this', 'dog'): 1, ('dog', 'is'): 1, ('is', 'like'): 1}\n"
     ]
    }
   ],
   "source": [
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "print(\"Uni-gram:\")\n",
    "print(count_n_grams(sentences, 1))\n",
    "print(\"Bi-gram:\")\n",
    "print(count_n_grams(sentences, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def estimate_probability(word, previous_n_gram, \n",
    "                         n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
    "    \"\"\"\n",
    "    Estimate the probabilities of a next word using the n-gram counts with k-smoothing\n",
    "    \n",
    "    Args:\n",
    "        word: next word\n",
    "        previous_n_gram: A sequence of words of length n\n",
    "        n_gram_counts: Dictionary of counts of (n+1)-grams\n",
    "        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n",
    "        vocabulary_size: number of words in the vocabulary\n",
    "        k: positive constant, smoothing parameter\n",
    "    \n",
    "    Returns:\n",
    "        A probability\n",
    "    \"\"\"\n",
    "    # convert list to tuple to use it as a dictionary key\n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "    \n",
    "    # Set the denominator\n",
    "    # If the previous n-gram exists in the dictionary of n-gram counts,\n",
    "    # Get its count.  Otherwise set the count to zero\n",
    "    # Use the dictionary that has counts for n-grams\n",
    "    previous_n_gram_count = n_gram_counts[previous_n_gram] if previous_n_gram in n_gram_counts else 0\n",
    "    \n",
    "    # Calculate the denominator using the count of the previous n-gram\n",
    "    # and apply k-smoothing\n",
    "    denominator = previous_n_gram_count + k*vocabulary_size\n",
    "    \n",
    "    # Define n plus 1 gram as the previous n-gram plus the current word as a tuple\n",
    "    n_plus1_gram = previous_n_gram + (word,)\n",
    "    \n",
    "    # Set the count to the count in the dictionary,\n",
    "    # otherwise 0 if not in the dictionary\n",
    "    # use the dictionary that has counts for the n-gram plus current word\n",
    "    n_plus1_gram_count = n_plus1_gram_counts[n_plus1_gram] if n_plus1_gram in n_plus1_gram_counts else 0\n",
    "    \n",
    "    # Define the numerator use the count of the n-gram plus current word,\n",
    "    # and apply smoothing\n",
    "    numerator = n_plus1_gram_count + k\n",
    "    \n",
    "    # Calculate the probability as the numerator divided bt denominator\n",
    "    probability = numerator / denominator\n",
    "    \n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The estimated probability of word 'cat' given the previous n-gram 'a' is: 0.3333\n"
     ]
    }
   ],
   "source": [
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "\n",
    "tmp_prob = estimate_probability(\"cat\", \"a\", unigram_counts, bigram_counts, len(unique_words), k=1)\n",
    "\n",
    "print(f\"The estimated probability of word 'cat' given the previous n-gram 'a' is: {tmp_prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def estimate_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0):\n",
    "    \"\"\"\n",
    "    Estimate the probabilities of next words using the n-gram counts with k-smoothing\n",
    "    \n",
    "    Args:\n",
    "        previous_n_gram: A sequence of words of length n\n",
    "        n_gram_counts: Dictionary of counts of (n+1)-grams\n",
    "        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n",
    "        vocabulary: List of words\n",
    "        k: positive constant, smoothing parameter\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary mapping from next words to the probability.\n",
    "    \"\"\"\n",
    "    # Convert list to tuple to use it as dictionary key\n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "    \n",
    "    # add <e> <unk> to the vocabulary\n",
    "    # <s> is not needed since it should not appear as the next word\n",
    "    vocabulary = vocabulary + ['<e>','<unk>']\n",
    "    vocabulary_size = len(vocabulary)\n",
    "    \n",
    "    probabilities = {}\n",
    "    for word in vocabulary:\n",
    "        probability = estimate_probability(word,previous_n_gram,n_gram_counts,n_plus1_gram_counts,\n",
    "                                           vocabulary_size,k=k)\n",
    "        \n",
    "        probabilities[word] = probability\n",
    "        \n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<e>': 0.09090909090909091,\n",
       " '<unk>': 0.09090909090909091,\n",
       " 'a': 0.09090909090909091,\n",
       " 'cat': 0.2727272727272727,\n",
       " 'dog': 0.09090909090909091,\n",
       " 'i': 0.09090909090909091,\n",
       " 'is': 0.09090909090909091,\n",
       " 'like': 0.09090909090909091,\n",
       " 'this': 0.09090909090909091}"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "estimate_probabilities(\"a\", unigram_counts, bigram_counts, unique_words, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<e>': 0.09090909090909091,\n",
       " '<unk>': 0.09090909090909091,\n",
       " 'a': 0.09090909090909091,\n",
       " 'cat': 0.09090909090909091,\n",
       " 'dog': 0.09090909090909091,\n",
       " 'i': 0.18181818181818182,\n",
       " 'is': 0.09090909090909091,\n",
       " 'like': 0.09090909090909091,\n",
       " 'this': 0.18181818181818182}"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_counts = count_n_grams(sentences, 3)\n",
    "estimate_probabilities([\"<s>\", \"<s>\"], bigram_counts, trigram_counts, unique_words, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_count_matrix(n_plus1_gram_counts, vocabulary):\n",
    "    # add <e> <unk> to the vocabulary\n",
    "    # <s> is omitted since it should not appear as the next word\n",
    "    vocabulary = vocabulary + [\"<e>\", \"<unk>\"]\n",
    "    \n",
    "    # obtain unique n-grams\n",
    "    n_grams = []\n",
    "    for n_plus1_gram in n_plus1_gram_counts.keys():\n",
    "        n_gram = n_plus1_gram[0:-1]\n",
    "        n_grams.append(n_gram)\n",
    "    n_grams = list(set(n_grams))\n",
    "    \n",
    "    # mapping from n-gram to row\n",
    "    row_index = {n_gram:i for i, n_gram in enumerate(n_grams)}\n",
    "    # mapping from next word to column\n",
    "    col_index = {word:j for j, word in enumerate(vocabulary)}\n",
    "    \n",
    "    nrow = len(n_grams)\n",
    "    ncol = len(vocabulary)\n",
    "    count_matrix = np.zeros((nrow, ncol))\n",
    "    for n_plus1_gram, count in n_plus1_gram_counts.items():\n",
    "        n_gram = n_plus1_gram[0:-1]\n",
    "        word = n_plus1_gram[-1]\n",
    "        if word not in vocabulary:\n",
    "            continue\n",
    "        i = row_index[n_gram]\n",
    "        j = col_index[word]\n",
    "        count_matrix[i, j] = count\n",
    "    \n",
    "    count_matrix = pd.DataFrame(count_matrix, index=n_grams, columns=vocabulary)\n",
    "    return count_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram counts\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dog</th>\n",
       "      <th>a</th>\n",
       "      <th>cat</th>\n",
       "      <th>like</th>\n",
       "      <th>this</th>\n",
       "      <th>i</th>\n",
       "      <th>is</th>\n",
       "      <th>&lt;e&gt;</th>\n",
       "      <th>&lt;unk&gt;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(dog,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(a,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(cat,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(is,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(like,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(i,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(this,)</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         dog    a  cat  like  this    i   is  <e>  <unk>\n",
       "(dog,)   0.0  0.0  0.0   0.0   0.0  0.0  1.0  0.0    0.0\n",
       "(a,)     0.0  0.0  2.0   0.0   0.0  0.0  0.0  0.0    0.0\n",
       "(cat,)   0.0  0.0  0.0   0.0   0.0  0.0  0.0  2.0    0.0\n",
       "(is,)    0.0  0.0  0.0   1.0   0.0  0.0  0.0  0.0    0.0\n",
       "(like,)  0.0  2.0  0.0   0.0   0.0  0.0  0.0  0.0    0.0\n",
       "(i,)     0.0  0.0  0.0   1.0   0.0  0.0  0.0  0.0    0.0\n",
       "(this,)  1.0  0.0  0.0   0.0   0.0  0.0  0.0  0.0    0.0\n",
       "(<s>,)   0.0  0.0  0.0   0.0   1.0  1.0  0.0  0.0    0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "                 ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "\n",
    "print('bigram counts')\n",
    "display(make_count_matrix(bigram_counts, unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "trigram counats\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dog</th>\n",
       "      <th>a</th>\n",
       "      <th>cat</th>\n",
       "      <th>like</th>\n",
       "      <th>this</th>\n",
       "      <th>i</th>\n",
       "      <th>is</th>\n",
       "      <th>&lt;e&gt;</th>\n",
       "      <th>&lt;unk&gt;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(like, a)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(is, like)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(dog, is)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;, &lt;s&gt;)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(cat,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(a, cat)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(this, dog)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;, i)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(i, like)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;, this)</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             dog    a  cat  like  this    i   is  <e>  <unk>\n",
       "(like, a)    0.0  0.0  2.0   0.0   0.0  0.0  0.0  0.0    0.0\n",
       "(is, like)   0.0  1.0  0.0   0.0   0.0  0.0  0.0  0.0    0.0\n",
       "(dog, is)    0.0  0.0  0.0   1.0   0.0  0.0  0.0  0.0    0.0\n",
       "(<s>, <s>)   0.0  0.0  0.0   0.0   1.0  1.0  0.0  0.0    0.0\n",
       "(cat,)       0.0  0.0  0.0   0.0   0.0  0.0  0.0  2.0    0.0\n",
       "(a, cat)     0.0  0.0  0.0   0.0   0.0  0.0  0.0  2.0    0.0\n",
       "(this, dog)  0.0  0.0  0.0   0.0   0.0  0.0  1.0  0.0    0.0\n",
       "(<s>, i)     0.0  0.0  0.0   1.0   0.0  0.0  0.0  0.0    0.0\n",
       "(i, like)    0.0  1.0  0.0   0.0   0.0  0.0  0.0  0.0    0.0\n",
       "(<s>, this)  1.0  0.0  0.0   0.0   0.0  0.0  0.0  0.0    0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show trigram counts\n",
    "print('\\ntrigram counats')\n",
    "trigram_counts = count_n_grams(sentences, 3)\n",
    "display(make_count_matrix(trigram_counts, unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_probability_matrix(n_plus1_gram_counts, vocabulary, k):\n",
    "    count_matrix = make_count_matrix(n_plus1_gram_counts, unique_words)\n",
    "    count_matrix += k\n",
    "    prob_matrix = count_matrix.div(count_matrix.sum(axis=1), axis=0)\n",
    "    return prob_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram probabilities\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dog</th>\n",
       "      <th>a</th>\n",
       "      <th>cat</th>\n",
       "      <th>like</th>\n",
       "      <th>this</th>\n",
       "      <th>i</th>\n",
       "      <th>is</th>\n",
       "      <th>&lt;e&gt;</th>\n",
       "      <th>&lt;unk&gt;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(dog,)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(a,)</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(cat,)</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(is,)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(like,)</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(i,)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(this,)</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;,)</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              dog         a       cat      like      this         i        is  \\\n",
       "(dog,)   0.100000  0.100000  0.100000  0.100000  0.100000  0.100000  0.200000   \n",
       "(a,)     0.090909  0.090909  0.272727  0.090909  0.090909  0.090909  0.090909   \n",
       "(cat,)   0.090909  0.090909  0.090909  0.090909  0.090909  0.090909  0.090909   \n",
       "(is,)    0.100000  0.100000  0.100000  0.200000  0.100000  0.100000  0.100000   \n",
       "(like,)  0.090909  0.272727  0.090909  0.090909  0.090909  0.090909  0.090909   \n",
       "(i,)     0.100000  0.100000  0.100000  0.200000  0.100000  0.100000  0.100000   \n",
       "(this,)  0.200000  0.100000  0.100000  0.100000  0.100000  0.100000  0.100000   \n",
       "(<s>,)   0.090909  0.090909  0.090909  0.090909  0.181818  0.181818  0.090909   \n",
       "\n",
       "              <e>     <unk>  \n",
       "(dog,)   0.100000  0.100000  \n",
       "(a,)     0.090909  0.090909  \n",
       "(cat,)   0.272727  0.090909  \n",
       "(is,)    0.100000  0.100000  \n",
       "(like,)  0.090909  0.090909  \n",
       "(i,)     0.100000  0.100000  \n",
       "(this,)  0.100000  0.100000  \n",
       "(<s>,)   0.090909  0.090909  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "                 ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "print(\"bigram probabilities\")\n",
    "display(make_probability_matrix(bigram_counts, unique_words, k=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trigram probabilities\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dog</th>\n",
       "      <th>a</th>\n",
       "      <th>cat</th>\n",
       "      <th>like</th>\n",
       "      <th>this</th>\n",
       "      <th>i</th>\n",
       "      <th>is</th>\n",
       "      <th>&lt;e&gt;</th>\n",
       "      <th>&lt;unk&gt;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(like, a)</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(is, like)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(dog, is)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;, &lt;s&gt;)</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(cat,)</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(a, cat)</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(this, dog)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;, i)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(i, like)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;, this)</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  dog         a       cat      like      this         i  \\\n",
       "(like, a)    0.090909  0.090909  0.272727  0.090909  0.090909  0.090909   \n",
       "(is, like)   0.100000  0.200000  0.100000  0.100000  0.100000  0.100000   \n",
       "(dog, is)    0.100000  0.100000  0.100000  0.200000  0.100000  0.100000   \n",
       "(<s>, <s>)   0.090909  0.090909  0.090909  0.090909  0.181818  0.181818   \n",
       "(cat,)       0.090909  0.090909  0.090909  0.090909  0.090909  0.090909   \n",
       "(a, cat)     0.090909  0.090909  0.090909  0.090909  0.090909  0.090909   \n",
       "(this, dog)  0.100000  0.100000  0.100000  0.100000  0.100000  0.100000   \n",
       "(<s>, i)     0.100000  0.100000  0.100000  0.200000  0.100000  0.100000   \n",
       "(i, like)    0.100000  0.200000  0.100000  0.100000  0.100000  0.100000   \n",
       "(<s>, this)  0.200000  0.100000  0.100000  0.100000  0.100000  0.100000   \n",
       "\n",
       "                   is       <e>     <unk>  \n",
       "(like, a)    0.090909  0.090909  0.090909  \n",
       "(is, like)   0.100000  0.100000  0.100000  \n",
       "(dog, is)    0.100000  0.100000  0.100000  \n",
       "(<s>, <s>)   0.090909  0.090909  0.090909  \n",
       "(cat,)       0.090909  0.272727  0.090909  \n",
       "(a, cat)     0.090909  0.272727  0.090909  \n",
       "(this, dog)  0.200000  0.100000  0.100000  \n",
       "(<s>, i)     0.100000  0.100000  0.100000  \n",
       "(i, like)    0.100000  0.100000  0.100000  \n",
       "(<s>, this)  0.100000  0.100000  0.100000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"trigram probabilities\")\n",
    "trigram_counts = count_n_grams(sentences, 3)\n",
    "display(make_probability_matrix(trigram_counts, unique_words, k=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_perplexity(sentence, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
    "    \"\"\"\n",
    "    Calculate perplexity for a list of sentences\n",
    "    \n",
    "    Args:\n",
    "        sentence: List of strings\n",
    "        n_gram_counts: Dictionary of counts of (n+1)-grams\n",
    "        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n",
    "        vocabulary_size: number of unique words in the vocabulary\n",
    "        k: Positive smoothing constant\n",
    "    \n",
    "    Returns:\n",
    "        Perplexity score\n",
    "    \"\"\"\n",
    "    # length of previous words\n",
    "    n = len(list(n_gram_counts.keys())[0]) \n",
    "    \n",
    "    # prepend <s> and append <e>\n",
    "    sentence = [\"<s>\"] * n + sentence + [\"<e>\"]\n",
    "    \n",
    "    # Cast the sentence from a list to a tuple\n",
    "    sentence = tuple(sentence)\n",
    "    \n",
    "    # length of sentence (after adding <s> and <e> tokens)\n",
    "    N = len(sentence)\n",
    "    \n",
    "    # The variable p will hold the product\n",
    "    # that is calculated inside the n-root\n",
    "    # Update this in the code below\n",
    "    product_pi = 1.0\n",
    "    \n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "    # Index t ranges from n to N - 1\n",
    "    for t in range(n, N): # complete this line\n",
    "\n",
    "        # get the n-gram preceding the word at position t\n",
    "        n_gram = sentence[t-n:t]\n",
    "        \n",
    "        # get the word at position t\n",
    "        word = sentence[t]\n",
    "        \n",
    "        # Estimate the probability of the word given the n-gram\n",
    "        # using the n-gram counts, n-plus1-gram counts,\n",
    "        # vocabulary size, and smoothing constant\n",
    "        probability = estimate_probability(word,n_gram, n_gram_counts, n_plus1_gram_counts, len(unique_words), k=1)\n",
    "        \n",
    "        # Update the product of the probabilities\n",
    "        # This 'product_pi' is a cumulative product \n",
    "        # of the (1/P) factors that are calculated in the loop\n",
    "        product_pi *= 1 / probability\n",
    "\n",
    "    # Take the Nth root of the product\n",
    "    perplexity = product_pi**(1/float(N))\n",
    "    \n",
    "    ### END CODE HERE ### \n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for first train sample: 2.8040\n",
      "Perplexity for test sample: 3.9654\n"
     ]
    }
   ],
   "source": [
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "                 ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "\n",
    "\n",
    "perplexity_train1 = calculate_perplexity(sentences[0],\n",
    "                                         unigram_counts, bigram_counts,\n",
    "                                         len(unique_words), k=1.0)\n",
    "print(f\"Perplexity for first train sample: {perplexity_train1:.4f}\")\n",
    "\n",
    "test_sentence = ['i', 'like', 'a', 'dog']\n",
    "perplexity_test = calculate_perplexity(test_sentence,\n",
    "                                       unigram_counts, bigram_counts,\n",
    "                                       len(unique_words), k=1.0)\n",
    "print(f\"Perplexity for test sample: {perplexity_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build an auto-complete system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def suggest_a_word(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0, start_with=None):\n",
    "    \"\"\"\n",
    "    Get suggestion for the next word\n",
    "    \n",
    "    Args:\n",
    "        previous_tokens: The sentence you input where each token is a word. Must have length > n \n",
    "        n_gram_counts: Dictionary of counts of (n+1)-grams\n",
    "        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n",
    "        vocabulary: List of words\n",
    "        k: positive constant, smoothing parameter\n",
    "        start_with: If not None, specifies the first few letters of the next word\n",
    "        \n",
    "    Returns:\n",
    "        A tuple of \n",
    "          - string of the most likely next word\n",
    "          - corresponding probability\n",
    "    \"\"\"\n",
    "    \n",
    "    # length of previous words\n",
    "    n = len(list(n_gram_counts.keys())[0]) \n",
    "    \n",
    "    # From the words that the user already typed\n",
    "    # get the most recent 'n' words as the previous n-gram\n",
    "    previous_n_gram = previous_tokens[-n:]\n",
    "\n",
    "    # Estimate the probabilities that each word in the vocabulary\n",
    "    # is the next word,\n",
    "    # given the previous n-gram, the dictionary of n-gram counts,\n",
    "    # the dictionary of n plus 1 gram counts, and the smoothing constant\n",
    "    probabilities = estimate_probabilities(previous_n_gram,\n",
    "                                           n_gram_counts, n_plus1_gram_counts,\n",
    "                                           vocabulary, k=k)\n",
    "    \n",
    "    # Initialize suggested word to None\n",
    "    # This will be set to the word with highest probability\n",
    "    suggestion = None\n",
    "    \n",
    "    # Initialize the highest word probability to 0\n",
    "    # this will be set to the highest probability \n",
    "    # of all words to be suggested\n",
    "    max_prob = 0\n",
    "    \n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "    \n",
    "    # For each word and its probability in the probabilities dictionary:\n",
    "    for word, prob in probabilities.items(): # complete this line\n",
    "        \n",
    "        # If the optional start_with string is set\n",
    "        if start_with != None: # complete this line\n",
    "\n",
    "            # Check if the word starts with the letters in 'start_with'\n",
    "            if not word.startswith(start_with): # complete this line\n",
    "\n",
    "                #If so, don't consider this word (move onto the next word)\n",
    "                continue # complete this line\n",
    "        \n",
    "        # Check if this word's probability\n",
    "        # is greater than the current maximum probability\n",
    "        if prob > max_prob: # complete this line\n",
    "            \n",
    "            # If so, save this word as the best suggestion (so far)\n",
    "            suggestion = word\n",
    "            \n",
    "            # Save the new maximum probability\n",
    "            max_prob = prob\n",
    "\n",
    "    ### END CODE HERE\n",
    "    \n",
    "    return suggestion, max_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are 'i like',\n",
      "\tand the suggested word is `a` with a probability of 0.2727\n",
      "\n",
      "The previous words are 'i like', the suggestion must start with `c`\n",
      "\tand the suggested word is `cat` with a probability of 0.0909\n"
     ]
    }
   ],
   "source": [
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "\n",
    "previous_tokens = [\"i\", \"like\"]\n",
    "tmp_suggest1 = suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0)\n",
    "print(f\"The previous words are 'i like',\\n\\tand the suggested word is `{tmp_suggest1[0]}` with a probability of {tmp_suggest1[1]:.4f}\")\n",
    "\n",
    "print()\n",
    "# test your code when setting the starts_with\n",
    "tmp_starts_with = 'c'\n",
    "tmp_suggest2 = suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0, start_with=tmp_starts_with)\n",
    "print(f\"The previous words are 'i like', the suggestion must start with `{tmp_starts_with}`\\n\\tand the suggested word is `{tmp_suggest2[0]}` with a probability of {tmp_suggest2[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=None):\n",
    "    model_counts = len(n_gram_counts_list)\n",
    "    suggestions = []\n",
    "    for i in range(model_counts-1):\n",
    "        n_gram_counts = n_gram_counts_list[i]\n",
    "        n_plus1_gram_counts = n_gram_counts_list[i+1]\n",
    "        \n",
    "        suggestion = suggest_a_word(previous_tokens, n_gram_counts,\n",
    "                                    n_plus1_gram_counts, vocabulary,\n",
    "                                    k=k, start_with=start_with)\n",
    "        suggestions.append(suggestion)\n",
    "    return suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are 'i like', the suggestions are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('a', 0.2727272727272727),\n",
       " ('a', 0.2),\n",
       " ('dog', 0.1111111111111111),\n",
       " ('dog', 0.1111111111111111)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "trigram_counts = count_n_grams(sentences, 3)\n",
    "quadgram_counts = count_n_grams(sentences, 4)\n",
    "qintgram_counts = count_n_grams(sentences, 5)\n",
    "\n",
    "n_gram_counts_list = [unigram_counts, bigram_counts, trigram_counts, quadgram_counts, qintgram_counts]\n",
    "previous_tokens = [\"i\", \"like\"]\n",
    "tmp_suggest3 = get_suggestions(previous_tokens, n_gram_counts_list, unique_words, k=1.0)\n",
    "\n",
    "print(f\"The previous words are 'i like', the suggestions are:\")\n",
    "display(tmp_suggest3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Suggest multiple words using n-grams of varying lenght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing n-gram counts with n = 1 ...\n",
      "Computing n-gram counts with n = 2 ...\n",
      "Computing n-gram counts with n = 3 ...\n",
      "Computing n-gram counts with n = 4 ...\n",
      "Computing n-gram counts with n = 5 ...\n"
     ]
    }
   ],
   "source": [
    "n_gram_counts_list = []\n",
    "for n in range(1, 6):\n",
    "    print(\"Computing n-gram counts with n =\", n, \"...\")\n",
    "    n_model_counts = count_n_grams(train_data_processed, n)\n",
    "    n_gram_counts_list.append(n_model_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are ['i', 'am', 'to'], the suggestions are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('be', 0.027665685098338604),\n",
       " ('have', 0.00013487086115044844),\n",
       " ('have', 0.00013490725126475548),\n",
       " ('i', 6.746272684341901e-05)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "previous_tokens = [\"i\", \"am\", \"to\"]\n",
    "tmp_suggest4 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
    "\n",
    "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
    "display(tmp_suggest4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are ['i', 'want', 'to', 'go'], the suggestions are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('to', 0.014051961029228078),\n",
       " ('to', 0.004697942168993581),\n",
       " ('to', 0.0009424436216762033),\n",
       " ('to', 0.0004044489383215369)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "previous_tokens = [\"i\", \"want\", \"to\", \"go\"]\n",
    "tmp_suggest5 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
    "\n",
    "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
    "display(tmp_suggest5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are ['hey', 'how', 'are'], the suggestions are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('you', 0.023426812585499317),\n",
       " ('you', 0.003559435862995299),\n",
       " ('you', 0.00013491635186184566),\n",
       " ('i', 6.746272684341901e-05)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "previous_tokens = [\"hey\", \"how\", \"are\"]\n",
    "tmp_suggest6 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
    "\n",
    "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
    "display(tmp_suggest6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are ['hey', 'how', 'are', 'you'], the suggestions are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(\"'re\", 0.023973994311255586),\n",
       " ('?', 0.002888465830762161),\n",
       " ('?', 0.0016134453781512605),\n",
       " ('<e>', 0.00013491635186184566)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "previous_tokens = [\"hey\", \"how\", \"are\", \"you\"]\n",
    "tmp_suggest7 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
    "\n",
    "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
    "display(tmp_suggest7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are ['hey', 'how', 'are', 'you'], the suggestions are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('do', 0.009020723283218204),\n",
       " ('doing', 0.0016411737674785006),\n",
       " ('doing', 0.00047058823529411766),\n",
       " ('dvd', 6.745817593092283e-05)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "previous_tokens = [\"hey\", \"how\", \"are\", \"you\"]\n",
    "tmp_suggest8 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=\"d\")\n",
    "\n",
    "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
    "display(tmp_suggest8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from utils2 import sigmoid, get_batches, compute_pca, get_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 60996 \n",
      " ['o', 'for', 'a', 'muse', 'of', 'fire', '.', 'that', 'would', 'ascend', 'the', 'brightest', 'heaven', 'of', 'invention']\n",
      "Size of vocabulary:  5778\n",
      "Most frequent tokens:  [('.', 9630), ('the', 1521), ('and', 1394), ('i', 1257), ('to', 1159), ('of', 1093), ('my', 857), ('that', 781), ('in', 770), ('a', 752), ('you', 748), ('is', 630), ('not', 559), ('for', 467), ('it', 460), ('with', 441), ('his', 434), ('but', 417), ('me', 417), ('your', 397)]\n",
      "Size of vocabulary:  5778\n",
      "Index of the word 'king' :   2745\n",
      "Word which has index 2743:   kindness\n"
     ]
    }
   ],
   "source": [
    "# Download sentence tokenizer\n",
    "nltk.data.path.append('.')\n",
    "\n",
    "# Load, tokenize and process the data\n",
    "import re\n",
    "with open('shakespeare.txt','r') as f:\n",
    "    data = f.read()\n",
    "    \n",
    "data = re.sub(r'[,!?;-]', '.',data)                                 #  Punktuations are replaced by .\n",
    "data = nltk.word_tokenize(data)                                     #  Tokenize string to words\n",
    "data = [ ch.lower() for ch in data if ch.isalpha() or ch == '.']    #  Lower case and drop non-alphabetical tokens\n",
    "print(\"Number of tokens:\", len(data),'\\n', data[:15])               #  print d\n",
    "\n",
    "# Compute the frequency distribution of the words in the dataset (vocabulary)\n",
    "fdist = nltk.FreqDist(word for word in data)\n",
    "print(\"Size of vocabulary: \",len(fdist) )\n",
    "print(\"Most frequent tokens: \",fdist.most_common(20) ) # print the 20 most frequent words and their freq.\n",
    "\n",
    "#### Mapping words to indices and indices to words\n",
    "\n",
    "# get_dict creates two dictionaries, converting words to indices and viceversa.\n",
    "word2Ind, Ind2word = get_dict(data)\n",
    "V = len(word2Ind)\n",
    "print(\"Size of vocabulary: \", V)\n",
    "\n",
    "# example of word to index mapping\n",
    "print(\"Index of the word 'king' :  \",word2Ind['king'] )\n",
    "print(\"Word which has index 2743:  \",Ind2word[2743] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_model(N,V, random_seed=1):\n",
    "    '''\n",
    "    Inputs: \n",
    "        N:  dimension of hidden vector \n",
    "        V:  dimension of vocabulary\n",
    "        random_seed: random seed for consistent results in the unit tests\n",
    "     Outputs: \n",
    "        W1, W2, b1, b2: initialized weights and biases\n",
    "    '''\n",
    "    \n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # W1 has shape (N,V)\n",
    "    W1 = np.random.rand(N,V)\n",
    "    # W2 has shape (V,N)\n",
    "    W2 = np.random.rand(V,N)\n",
    "    # b1 has shape (N,1)\n",
    "    b1 = np.random.rand(N,1)\n",
    "    # b2 has shape (V,1)\n",
    "    b2 = np.random.rand(V,1)\n",
    "\n",
    "    return W1, W2, b1, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp_W1.shape: (4, 10)\n",
      "tmp_W2.shape: (10, 4)\n",
      "tmp_b1.shape: (4, 1)\n",
      "tmp_b2.shape: (10, 1)\n"
     ]
    }
   ],
   "source": [
    "# Test your function example.\n",
    "tmp_N = 4\n",
    "tmp_V = 10\n",
    "tmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(tmp_N,tmp_V)\n",
    "assert tmp_W1.shape == ((tmp_N,tmp_V))\n",
    "assert tmp_W2.shape == ((tmp_V,tmp_N))\n",
    "print(f\"tmp_W1.shape: {tmp_W1.shape}\")\n",
    "print(f\"tmp_W2.shape: {tmp_W2.shape}\")\n",
    "print(f\"tmp_b1.shape: {tmp_b1.shape}\")\n",
    "print(f\"tmp_b2.shape: {tmp_b2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    '''\n",
    "    Inputs: \n",
    "        z: output scores from the hidden layer\n",
    "    Outputs: \n",
    "        yhat: prediction (estimate of y)\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # Calculate yhat (softmax)\n",
    "    e_z = np.exp(z)\n",
    "    yhat = e_z/np.sum(e_z,axis=0)\n",
    "    \n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.73105858, 0.88079708],\n",
       "       [0.5       , 0.26894142, 0.11920292]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test the function\n",
    "tmp = np.array([[1,2,3],\n",
    "                [1,1,1]\n",
    "               ])\n",
    "tmp_sm = softmax(tmp)\n",
    "display(tmp_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Foward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_prop(x, W1, W2, b1, b2):\n",
    "    '''\n",
    "    Inputs: \n",
    "        x:  average one hot vector for the context \n",
    "        W1, W2, b1, b2:  matrices and biases to be learned\n",
    "     Outputs: \n",
    "        z:  output score vector\n",
    "    '''\n",
    "    \n",
    "    ### START CODE HERE (Replace instances of 'None' with your own code) ###\n",
    "    \n",
    "    # Calculate h\n",
    "    h = np.dot(W1,x)+b1\n",
    "    \n",
    "    # Apply the relu on h (store result in h)\n",
    "    h = np.maximum(0,h)\n",
    "    \n",
    "    # Calculate z\n",
    "    z = np.dot(W2,h)+b2\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return z, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x has shape (3, 1)\n",
      "N is 2 and vocabulary size V is 3\n",
      "call forward_prop\n",
      "\n",
      "z has shape (3, 1)\n",
      "z has values:\n",
      "[[0.55379268]\n",
      " [1.58960774]\n",
      " [1.50722933]]\n",
      "\n",
      "h has shape (2, 1)\n",
      "h has values:\n",
      "[[0.92477674]\n",
      " [1.02487333]]\n"
     ]
    }
   ],
   "source": [
    "# Test the function\n",
    "\n",
    "# Create some inputs\n",
    "tmp_N = 2\n",
    "tmp_V = 3\n",
    "tmp_x = np.array([[0,1,0]]).T\n",
    "tmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(N=tmp_N,V=tmp_V, random_seed=1)\n",
    "\n",
    "print(f\"x has shape {tmp_x.shape}\")\n",
    "print(f\"N is {tmp_N} and vocabulary size V is {tmp_V}\")\n",
    "\n",
    "# call function\n",
    "tmp_z, tmp_h = forward_prop(tmp_x, tmp_W1, tmp_W2, tmp_b1, tmp_b2)\n",
    "print(\"call forward_prop\")\n",
    "print()\n",
    "# Look at output\n",
    "print(f\"z has shape {tmp_z.shape}\")\n",
    "print(\"z has values:\")\n",
    "print(tmp_z)\n",
    "\n",
    "print()\n",
    "\n",
    "print(f\"h has shape {tmp_h.shape}\")\n",
    "print(\"h has values:\")\n",
    "print(tmp_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute_cost: cross-entropy cost functioN\n",
    "def compute_cost(y, yhat, batch_size):\n",
    "    # cost function \n",
    "    logprobs = np.multiply(np.log(yhat),y) + np.multiply(np.log(1 - yhat), 1 - y)\n",
    "    cost = - 1/batch_size * np.sum(logprobs)\n",
    "    cost = np.squeeze(cost)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp_x.shape (5778, 4)\n",
      "tmp_y.shape (5778, 4)\n",
      "tmp_W1.shape (50, 5778)\n",
      "tmp_W2.shape (5778, 50)\n",
      "tmp_b1.shape (50, 1)\n",
      "tmp_b2.shape (5778, 1)\n",
      "tmp_z.shape: (5778, 4)\n",
      "tmp_h.shape: (50, 4)\n",
      "tmp_yhat.shape: (5778, 4)\n",
      "call compute_cost\n",
      "tmp_cost 9.9560\n"
     ]
    }
   ],
   "source": [
    "# Test the function\n",
    "tmp_C = 2\n",
    "tmp_N = 50\n",
    "tmp_batch_size = 4\n",
    "tmp_word2Ind, tmp_Ind2word = get_dict(data)\n",
    "tmp_V = len(word2Ind)\n",
    "\n",
    "tmp_x, tmp_y = next(get_batches(data, tmp_word2Ind, tmp_V,tmp_C, tmp_batch_size))\n",
    "        \n",
    "print(f\"tmp_x.shape {tmp_x.shape}\")\n",
    "print(f\"tmp_y.shape {tmp_y.shape}\")\n",
    "\n",
    "tmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(tmp_N,tmp_V)\n",
    "\n",
    "print(f\"tmp_W1.shape {tmp_W1.shape}\")\n",
    "print(f\"tmp_W2.shape {tmp_W2.shape}\")\n",
    "print(f\"tmp_b1.shape {tmp_b1.shape}\")\n",
    "print(f\"tmp_b2.shape {tmp_b2.shape}\")\n",
    "\n",
    "tmp_z, tmp_h = forward_prop(tmp_x, tmp_W1, tmp_W2, tmp_b1, tmp_b2)\n",
    "print(f\"tmp_z.shape: {tmp_z.shape}\")\n",
    "print(f\"tmp_h.shape: {tmp_h.shape}\")\n",
    "\n",
    "tmp_yhat = softmax(tmp_z)\n",
    "print(f\"tmp_yhat.shape: {tmp_yhat.shape}\")\n",
    "\n",
    "tmp_cost = compute_cost(tmp_y, tmp_yhat, tmp_batch_size)\n",
    "print(\"call compute_cost\")\n",
    "print(f\"tmp_cost {tmp_cost:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def back_prop(x, yhat, y, h, W1, W2, b1, b2, batch_size):\n",
    "    '''\n",
    "    Inputs: \n",
    "        x:  average one hot vector for the context \n",
    "        yhat: prediction (estimate of y)\n",
    "        y:  target vector\n",
    "        h:  hidden vector (see eq. 1)\n",
    "        W1, W2, b1, b2:  matrices and biases  \n",
    "        batch_size: batch size \n",
    "     Outputs: \n",
    "        grad_W1, grad_W2, grad_b1, grad_b2:  gradients of matrices and biases   \n",
    "    '''\n",
    "    \n",
    "    # Compute l1 as W2^T (Yhat - Y)\n",
    "    # Re-use it whenever you see W2^T (Yhat - Y) used to compute a gradient\n",
    "    l1 = np.dot(W2.T,(yhat-y))\n",
    "    # Apply relu to l1\n",
    "    l1 = np.maximum(0,l1)\n",
    "    # Compute the gradient of W1\n",
    "    grad_W1 = (1/batch_size)*np.dot(l1,x.T)    #1/m * relu(w2.T(yhat-y)) . xT\n",
    "    # Compute the gradient of W2\n",
    "    grad_W2 = (1/batch_size)*np.dot(yhat-y,h.T)\n",
    "    # Compute the gradient of b1\n",
    "    grad_b1 = np.sum((1/batch_size)*np.dot(l1,x.T),axis=1,keepdims=True)\n",
    "    # Compute the gradient of b2\n",
    "    grad_b2 = np.sum((1/batch_size)*np.dot(yhat-y,h.T),axis=1,keepdims=True)\n",
    "    \n",
    "    return grad_W1, grad_W2, grad_b1, grad_b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get a batch of data\n",
      "tmp_x.shape (5778, 4)\n",
      "tmp_y.shape (5778, 4)\n",
      "\n",
      "Initialize weights and biases\n",
      "tmp_W1.shape (50, 5778)\n",
      "tmp_W2.shape (5778, 50)\n",
      "tmp_b1.shape (50, 1)\n",
      "tmp_b2.shape (5778, 1)\n",
      "\n",
      "Forwad prop to get z and h\n",
      "tmp_z.shape: (5778, 4)\n",
      "tmp_h.shape: (50, 4)\n",
      "\n",
      "Get yhat by calling softmax\n",
      "tmp_yhat.shape: (5778, 4)\n",
      "\n",
      "call back_prop\n",
      "tmp_grad_W1.shape (50, 5778)\n",
      "tmp_grad_W2.shape (5778, 50)\n",
      "tmp_grad_b1.shape (50, 1)\n",
      "tmp_grad_b2.shape (5778, 1)\n"
     ]
    }
   ],
   "source": [
    "# Test the function\n",
    "tmp_C = 2\n",
    "tmp_N = 50\n",
    "tmp_batch_size = 4\n",
    "tmp_word2Ind, tmp_Ind2word = get_dict(data)\n",
    "tmp_V = len(word2Ind)\n",
    "\n",
    "# get a batch of data\n",
    "tmp_x, tmp_y = next(get_batches(data, tmp_word2Ind, tmp_V,tmp_C, tmp_batch_size))\n",
    "\n",
    "print(\"get a batch of data\")\n",
    "print(f\"tmp_x.shape {tmp_x.shape}\")\n",
    "print(f\"tmp_y.shape {tmp_y.shape}\")\n",
    "\n",
    "print()\n",
    "print(\"Initialize weights and biases\")\n",
    "tmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(tmp_N,tmp_V)\n",
    "\n",
    "print(f\"tmp_W1.shape {tmp_W1.shape}\")\n",
    "print(f\"tmp_W2.shape {tmp_W2.shape}\")\n",
    "print(f\"tmp_b1.shape {tmp_b1.shape}\")\n",
    "print(f\"tmp_b2.shape {tmp_b2.shape}\")\n",
    "\n",
    "print()\n",
    "print(\"Forwad prop to get z and h\")\n",
    "tmp_z, tmp_h = forward_prop(tmp_x, tmp_W1, tmp_W2, tmp_b1, tmp_b2)\n",
    "print(f\"tmp_z.shape: {tmp_z.shape}\")\n",
    "print(f\"tmp_h.shape: {tmp_h.shape}\")\n",
    "\n",
    "print()\n",
    "print(\"Get yhat by calling softmax\")\n",
    "tmp_yhat = softmax(tmp_z)\n",
    "print(f\"tmp_yhat.shape: {tmp_yhat.shape}\")\n",
    "\n",
    "tmp_m = (2*tmp_C)\n",
    "tmp_grad_W1, tmp_grad_W2, tmp_grad_b1, tmp_grad_b2 = back_prop(tmp_x, tmp_yhat, tmp_y, tmp_h, tmp_W1, tmp_W2, tmp_b1, tmp_b2, tmp_batch_size)\n",
    "\n",
    "print()\n",
    "print(\"call back_prop\")\n",
    "print(f\"tmp_grad_W1.shape {tmp_grad_W1.shape}\")\n",
    "print(f\"tmp_grad_W2.shape {tmp_grad_W2.shape}\")\n",
    "print(f\"tmp_grad_b1.shape {tmp_grad_b1.shape}\")\n",
    "print(f\"tmp_grad_b2.shape {tmp_grad_b2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(data, word2Ind, N, V, num_iters, alpha=0.03):\n",
    "    \n",
    "    '''\n",
    "    This is the gradient_descent function\n",
    "    \n",
    "      Inputs: \n",
    "        data:      text\n",
    "        word2Ind:  words to Indices\n",
    "        N:         dimension of hidden vector  \n",
    "        V:         dimension of vocabulary \n",
    "        num_iters: number of iterations  \n",
    "     Outputs: \n",
    "        W1, W2, b1, b2:  updated matrices and biases   \n",
    "\n",
    "    '''\n",
    "    W1, W2, b1, b2 = initialize_model(N,V, random_seed=282)\n",
    "    batch_size = 128\n",
    "    iters = 0\n",
    "    C = 2\n",
    "    for x, y in get_batches(data, word2Ind, V, C, batch_size):\n",
    "        ### START CODE HERE (Replace instances of 'None' with your own code) ###\n",
    "        # Get z and h\n",
    "        z, h = forward_prop(x, W1, W2, b1, b2)\n",
    "        # Get yhat\n",
    "        yhat = softmax(z)\n",
    "        # Get cost\n",
    "        cost = compute_cost(y, yhat, batch_size)\n",
    "        if ( (iters+1) % 10 == 0):\n",
    "            print(f\"iters: {iters + 1} cost: {cost:.6f}\")\n",
    "        # Get gradients\n",
    "        grad_W1, grad_W2, grad_b1, grad_b2 = back_prop(x, yhat, y, h, W1, W2, b1, b2, batch_size)\n",
    "        \n",
    "        # Update weights and biases\n",
    "        W1 -= alpha*grad_W1 \n",
    "        W2 -= alpha*grad_W2\n",
    "        b1 -= alpha*grad_b1\n",
    "        b2 -= alpha*grad_b2\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        iters += 1 \n",
    "        if iters == num_iters: \n",
    "            break\n",
    "        if iters % 100 == 0:\n",
    "            alpha *= 0.66\n",
    "            \n",
    "    return W1, W2, b1, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call gradient_descent\n",
      "iters: 10 cost: 0.107177\n",
      "iters: 20 cost: 0.038644\n",
      "iters: 30 cost: 0.023685\n",
      "iters: 40 cost: 0.017095\n",
      "iters: 50 cost: 0.013380\n",
      "iters: 60 cost: 0.010995\n",
      "iters: 70 cost: 0.009333\n",
      "iters: 80 cost: 0.008108\n",
      "iters: 90 cost: 0.007168\n",
      "iters: 100 cost: 0.006423\n",
      "iters: 110 cost: 0.005992\n",
      "iters: 120 cost: 0.005633\n",
      "iters: 130 cost: 0.005314\n",
      "iters: 140 cost: 0.005030\n",
      "iters: 150 cost: 0.004774\n"
     ]
    }
   ],
   "source": [
    "# test your function\n",
    "C = 2\n",
    "N = 50\n",
    "word2Ind, Ind2word = get_dict(data)\n",
    "V = len(word2Ind)\n",
    "num_iters = 150\n",
    "print(\"Call gradient_descent\")\n",
    "W1, W2, b1, b2 = gradient_descent(data, word2Ind, N, V, num_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualizing the word vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 50) [2745, 3951, 2961, 3023, 5675, 1452, 2472, 4191, 3730, 4278, 2316]\n"
     ]
    }
   ],
   "source": [
    "# visualizing the word vectors here\n",
    "from matplotlib import pyplot\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "words = ['king', 'queen','lord','man', 'woman','dog','horse',\n",
    "         'rich','poor','sad', 'happy']\n",
    "\n",
    "embs = (W1.T + W2)/2.0\n",
    " \n",
    "# given a list of words and the embeddings, it returns a matrix with all the embeddings\n",
    "idx = [word2Ind[word] for word in words]\n",
    "X = embs[idx, :]\n",
    "print(X.shape, idx)  # X.shape:  Number of words of dimension N each "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Created with matplotlib (https://matplotlib.org/) -->\n",
       "<svg height=\"248.611178pt\" version=\"1.1\" viewBox=\"0 0 396.345881 248.611178\" width=\"396.345881pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2021-06-22T14:08:00.835504</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.3.4, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M -0 248.611178 \n",
       "L 396.345881 248.611178 \n",
       "L 396.345881 0 \n",
       "L -0 0 \n",
       "z\n",
       "\" style=\"fill:none;\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 38.482813 224.733053 \n",
       "L 373.282813 224.733053 \n",
       "L 373.282813 7.293053 \n",
       "L 38.482813 7.293053 \n",
       "z\n",
       "\" style=\"fill:#ffffff;\"/>\n",
       "   </g>\n",
       "   <g id=\"PathCollection_1\">\n",
       "    <defs>\n",
       "     <path d=\"M 0 3 \n",
       "C 0.795609 3 1.55874 2.683901 2.12132 2.12132 \n",
       "C 2.683901 1.55874 3 0.795609 3 0 \n",
       "C 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \n",
       "C 1.55874 -2.683901 0.795609 -3 0 -3 \n",
       "C -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \n",
       "C -2.683901 -1.55874 -3 -0.795609 -3 0 \n",
       "C -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \n",
       "C -1.55874 2.683901 -0.795609 3 0 3 \n",
       "z\n",
       "\" id=\"m80ca921832\" style=\"stroke:#1f77b4;\"/>\n",
       "    </defs>\n",
       "    <g clip-path=\"url(#p154f87226a)\">\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"84.589681\" xlink:href=\"#m80ca921832\" y=\"87.517725\"/>\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"228.524318\" xlink:href=\"#m80ca921832\" y=\"45.135314\"/>\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"197.100505\" xlink:href=\"#m80ca921832\" y=\"68.197591\"/>\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"146.93431\" xlink:href=\"#m80ca921832\" y=\"85.703432\"/>\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"120.895331\" xlink:href=\"#m80ca921832\" y=\"214.849417\"/>\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"99.732716\" xlink:href=\"#m80ca921832\" y=\"155.443299\"/>\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"101.676147\" xlink:href=\"#m80ca921832\" y=\"98.60502\"/>\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"96.463031\" xlink:href=\"#m80ca921832\" y=\"17.17669\"/>\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"53.700994\" xlink:href=\"#m80ca921832\" y=\"73.87261\"/>\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"59.193441\" xlink:href=\"#m80ca921832\" y=\"176.195357\"/>\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"358.064631\" xlink:href=\"#m80ca921832\" y=\"149.504129\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" id=\"m392ce31192\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"69.093383\" xlink:href=\"#m392ce31192\" y=\"224.733053\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- −0.5 -->\n",
       "      <g transform=\"translate(56.951977 239.331491)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path d=\"M 10.59375 35.5 \n",
       "L 73.1875 35.5 \n",
       "L 73.1875 27.203125 \n",
       "L 10.59375 27.203125 \n",
       "z\n",
       "\" id=\"DejaVuSans-8722\"/>\n",
       "        <path d=\"M 31.78125 66.40625 \n",
       "Q 24.171875 66.40625 20.328125 58.90625 \n",
       "Q 16.5 51.421875 16.5 36.375 \n",
       "Q 16.5 21.390625 20.328125 13.890625 \n",
       "Q 24.171875 6.390625 31.78125 6.390625 \n",
       "Q 39.453125 6.390625 43.28125 13.890625 \n",
       "Q 47.125 21.390625 47.125 36.375 \n",
       "Q 47.125 51.421875 43.28125 58.90625 \n",
       "Q 39.453125 66.40625 31.78125 66.40625 \n",
       "z\n",
       "M 31.78125 74.21875 \n",
       "Q 44.046875 74.21875 50.515625 64.515625 \n",
       "Q 56.984375 54.828125 56.984375 36.375 \n",
       "Q 56.984375 17.96875 50.515625 8.265625 \n",
       "Q 44.046875 -1.421875 31.78125 -1.421875 \n",
       "Q 19.53125 -1.421875 13.0625 8.265625 \n",
       "Q 6.59375 17.96875 6.59375 36.375 \n",
       "Q 6.59375 54.828125 13.0625 64.515625 \n",
       "Q 19.53125 74.21875 31.78125 74.21875 \n",
       "z\n",
       "\" id=\"DejaVuSans-48\"/>\n",
       "        <path d=\"M 10.6875 12.40625 \n",
       "L 21 12.40625 \n",
       "L 21 0 \n",
       "L 10.6875 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-46\"/>\n",
       "        <path d=\"M 10.796875 72.90625 \n",
       "L 49.515625 72.90625 \n",
       "L 49.515625 64.59375 \n",
       "L 19.828125 64.59375 \n",
       "L 19.828125 46.734375 \n",
       "Q 21.96875 47.46875 24.109375 47.828125 \n",
       "Q 26.265625 48.1875 28.421875 48.1875 \n",
       "Q 40.625 48.1875 47.75 41.5 \n",
       "Q 54.890625 34.8125 54.890625 23.390625 \n",
       "Q 54.890625 11.625 47.5625 5.09375 \n",
       "Q 40.234375 -1.421875 26.90625 -1.421875 \n",
       "Q 22.3125 -1.421875 17.546875 -0.640625 \n",
       "Q 12.796875 0.140625 7.71875 1.703125 \n",
       "L 7.71875 11.625 \n",
       "Q 12.109375 9.234375 16.796875 8.0625 \n",
       "Q 21.484375 6.890625 26.703125 6.890625 \n",
       "Q 35.15625 6.890625 40.078125 11.328125 \n",
       "Q 45.015625 15.765625 45.015625 23.390625 \n",
       "Q 45.015625 31 40.078125 35.4375 \n",
       "Q 35.15625 39.890625 26.703125 39.890625 \n",
       "Q 22.75 39.890625 18.8125 39.015625 \n",
       "Q 14.890625 38.140625 10.796875 36.28125 \n",
       "z\n",
       "\" id=\"DejaVuSans-53\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-8722\"/>\n",
       "       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"140.62501\" xlink:href=\"#m392ce31192\" y=\"224.733053\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 0.0 -->\n",
       "      <g transform=\"translate(132.673447 239.331491)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"212.156636\" xlink:href=\"#m392ce31192\" y=\"224.733053\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 0.5 -->\n",
       "      <g transform=\"translate(204.205074 239.331491)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"283.688263\" xlink:href=\"#m392ce31192\" y=\"224.733053\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 1.0 -->\n",
       "      <g transform=\"translate(275.7367 239.331491)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path d=\"M 12.40625 8.296875 \n",
       "L 28.515625 8.296875 \n",
       "L 28.515625 63.921875 \n",
       "L 10.984375 60.40625 \n",
       "L 10.984375 69.390625 \n",
       "L 28.421875 72.90625 \n",
       "L 38.28125 72.90625 \n",
       "L 38.28125 8.296875 \n",
       "L 54.390625 8.296875 \n",
       "L 54.390625 0 \n",
       "L 12.40625 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-49\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"355.21989\" xlink:href=\"#m392ce31192\" y=\"224.733053\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 1.5 -->\n",
       "      <g transform=\"translate(347.268327 239.331491)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" id=\"mde98ee628e\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"38.482813\" xlink:href=\"#mde98ee628e\" y=\"202.12816\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- −1.0 -->\n",
       "      <g transform=\"translate(7.2 205.927379)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-8722\"/>\n",
       "       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"38.482813\" xlink:href=\"#mde98ee628e\" y=\"154.345925\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- −0.5 -->\n",
       "      <g transform=\"translate(7.2 158.145144)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-8722\"/>\n",
       "       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"38.482813\" xlink:href=\"#mde98ee628e\" y=\"106.563689\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 0.0 -->\n",
       "      <g transform=\"translate(15.579688 110.362908)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"38.482813\" xlink:href=\"#mde98ee628e\" y=\"58.781454\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 0.5 -->\n",
       "      <g transform=\"translate(15.579688 62.580673)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"38.482813\" xlink:href=\"#mde98ee628e\" y=\"10.999219\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 1.0 -->\n",
       "      <g transform=\"translate(15.579688 14.798437)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 38.482813 224.733053 \n",
       "L 38.482813 7.293053 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 373.282813 224.733053 \n",
       "L 373.282813 7.293053 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 38.482812 224.733053 \n",
       "L 373.282812 224.733053 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 38.482812 7.293053 \n",
       "L 373.282812 7.293053 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"text_11\">\n",
       "    <!-- king -->\n",
       "    <g transform=\"translate(84.589681 87.517725)scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path d=\"M 9.078125 75.984375 \n",
       "L 18.109375 75.984375 \n",
       "L 18.109375 31.109375 \n",
       "L 44.921875 54.6875 \n",
       "L 56.390625 54.6875 \n",
       "L 27.390625 29.109375 \n",
       "L 57.625 0 \n",
       "L 45.90625 0 \n",
       "L 18.109375 26.703125 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-107\"/>\n",
       "      <path d=\"M 9.421875 54.6875 \n",
       "L 18.40625 54.6875 \n",
       "L 18.40625 0 \n",
       "L 9.421875 0 \n",
       "z\n",
       "M 9.421875 75.984375 \n",
       "L 18.40625 75.984375 \n",
       "L 18.40625 64.59375 \n",
       "L 9.421875 64.59375 \n",
       "z\n",
       "\" id=\"DejaVuSans-105\"/>\n",
       "      <path d=\"M 54.890625 33.015625 \n",
       "L 54.890625 0 \n",
       "L 45.90625 0 \n",
       "L 45.90625 32.71875 \n",
       "Q 45.90625 40.484375 42.875 44.328125 \n",
       "Q 39.84375 48.1875 33.796875 48.1875 \n",
       "Q 26.515625 48.1875 22.3125 43.546875 \n",
       "Q 18.109375 38.921875 18.109375 30.90625 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "L 9.078125 54.6875 \n",
       "L 18.109375 54.6875 \n",
       "L 18.109375 46.1875 \n",
       "Q 21.34375 51.125 25.703125 53.5625 \n",
       "Q 30.078125 56 35.796875 56 \n",
       "Q 45.21875 56 50.046875 50.171875 \n",
       "Q 54.890625 44.34375 54.890625 33.015625 \n",
       "z\n",
       "\" id=\"DejaVuSans-110\"/>\n",
       "      <path d=\"M 45.40625 27.984375 \n",
       "Q 45.40625 37.75 41.375 43.109375 \n",
       "Q 37.359375 48.484375 30.078125 48.484375 \n",
       "Q 22.859375 48.484375 18.828125 43.109375 \n",
       "Q 14.796875 37.75 14.796875 27.984375 \n",
       "Q 14.796875 18.265625 18.828125 12.890625 \n",
       "Q 22.859375 7.515625 30.078125 7.515625 \n",
       "Q 37.359375 7.515625 41.375 12.890625 \n",
       "Q 45.40625 18.265625 45.40625 27.984375 \n",
       "z\n",
       "M 54.390625 6.78125 \n",
       "Q 54.390625 -7.171875 48.1875 -13.984375 \n",
       "Q 42 -20.796875 29.203125 -20.796875 \n",
       "Q 24.46875 -20.796875 20.265625 -20.09375 \n",
       "Q 16.0625 -19.390625 12.109375 -17.921875 \n",
       "L 12.109375 -9.1875 \n",
       "Q 16.0625 -11.328125 19.921875 -12.34375 \n",
       "Q 23.78125 -13.375 27.78125 -13.375 \n",
       "Q 36.625 -13.375 41.015625 -8.765625 \n",
       "Q 45.40625 -4.15625 45.40625 5.171875 \n",
       "L 45.40625 9.625 \n",
       "Q 42.625 4.78125 38.28125 2.390625 \n",
       "Q 33.9375 0 27.875 0 \n",
       "Q 17.828125 0 11.671875 7.65625 \n",
       "Q 5.515625 15.328125 5.515625 27.984375 \n",
       "Q 5.515625 40.671875 11.671875 48.328125 \n",
       "Q 17.828125 56 27.875 56 \n",
       "Q 33.9375 56 38.28125 53.609375 \n",
       "Q 42.625 51.21875 45.40625 46.390625 \n",
       "L 45.40625 54.6875 \n",
       "L 54.390625 54.6875 \n",
       "z\n",
       "\" id=\"DejaVuSans-103\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-107\"/>\n",
       "     <use x=\"57.910156\" xlink:href=\"#DejaVuSans-105\"/>\n",
       "     <use x=\"85.693359\" xlink:href=\"#DejaVuSans-110\"/>\n",
       "     <use x=\"149.072266\" xlink:href=\"#DejaVuSans-103\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_12\">\n",
       "    <!-- queen -->\n",
       "    <g transform=\"translate(228.524318 45.135314)scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path d=\"M 14.796875 27.296875 \n",
       "Q 14.796875 17.390625 18.875 11.75 \n",
       "Q 22.953125 6.109375 30.078125 6.109375 \n",
       "Q 37.203125 6.109375 41.296875 11.75 \n",
       "Q 45.40625 17.390625 45.40625 27.296875 \n",
       "Q 45.40625 37.203125 41.296875 42.84375 \n",
       "Q 37.203125 48.484375 30.078125 48.484375 \n",
       "Q 22.953125 48.484375 18.875 42.84375 \n",
       "Q 14.796875 37.203125 14.796875 27.296875 \n",
       "z\n",
       "M 45.40625 8.203125 \n",
       "Q 42.578125 3.328125 38.25 0.953125 \n",
       "Q 33.9375 -1.421875 27.875 -1.421875 \n",
       "Q 17.96875 -1.421875 11.734375 6.484375 \n",
       "Q 5.515625 14.40625 5.515625 27.296875 \n",
       "Q 5.515625 40.1875 11.734375 48.09375 \n",
       "Q 17.96875 56 27.875 56 \n",
       "Q 33.9375 56 38.25 53.625 \n",
       "Q 42.578125 51.265625 45.40625 46.390625 \n",
       "L 45.40625 54.6875 \n",
       "L 54.390625 54.6875 \n",
       "L 54.390625 -20.796875 \n",
       "L 45.40625 -20.796875 \n",
       "z\n",
       "\" id=\"DejaVuSans-113\"/>\n",
       "      <path d=\"M 8.5 21.578125 \n",
       "L 8.5 54.6875 \n",
       "L 17.484375 54.6875 \n",
       "L 17.484375 21.921875 \n",
       "Q 17.484375 14.15625 20.5 10.265625 \n",
       "Q 23.53125 6.390625 29.59375 6.390625 \n",
       "Q 36.859375 6.390625 41.078125 11.03125 \n",
       "Q 45.3125 15.671875 45.3125 23.6875 \n",
       "L 45.3125 54.6875 \n",
       "L 54.296875 54.6875 \n",
       "L 54.296875 0 \n",
       "L 45.3125 0 \n",
       "L 45.3125 8.40625 \n",
       "Q 42.046875 3.421875 37.71875 1 \n",
       "Q 33.40625 -1.421875 27.6875 -1.421875 \n",
       "Q 18.265625 -1.421875 13.375 4.4375 \n",
       "Q 8.5 10.296875 8.5 21.578125 \n",
       "z\n",
       "M 31.109375 56 \n",
       "z\n",
       "\" id=\"DejaVuSans-117\"/>\n",
       "      <path d=\"M 56.203125 29.59375 \n",
       "L 56.203125 25.203125 \n",
       "L 14.890625 25.203125 \n",
       "Q 15.484375 15.921875 20.484375 11.0625 \n",
       "Q 25.484375 6.203125 34.421875 6.203125 \n",
       "Q 39.59375 6.203125 44.453125 7.46875 \n",
       "Q 49.3125 8.734375 54.109375 11.28125 \n",
       "L 54.109375 2.78125 \n",
       "Q 49.265625 0.734375 44.1875 -0.34375 \n",
       "Q 39.109375 -1.421875 33.890625 -1.421875 \n",
       "Q 20.796875 -1.421875 13.15625 6.1875 \n",
       "Q 5.515625 13.8125 5.515625 26.8125 \n",
       "Q 5.515625 40.234375 12.765625 48.109375 \n",
       "Q 20.015625 56 32.328125 56 \n",
       "Q 43.359375 56 49.78125 48.890625 \n",
       "Q 56.203125 41.796875 56.203125 29.59375 \n",
       "z\n",
       "M 47.21875 32.234375 \n",
       "Q 47.125 39.59375 43.09375 43.984375 \n",
       "Q 39.0625 48.390625 32.421875 48.390625 \n",
       "Q 24.90625 48.390625 20.390625 44.140625 \n",
       "Q 15.875 39.890625 15.1875 32.171875 \n",
       "z\n",
       "\" id=\"DejaVuSans-101\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-113\"/>\n",
       "     <use x=\"63.476562\" xlink:href=\"#DejaVuSans-117\"/>\n",
       "     <use x=\"126.855469\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "     <use x=\"188.378906\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "     <use x=\"249.902344\" xlink:href=\"#DejaVuSans-110\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_13\">\n",
       "    <!-- lord -->\n",
       "    <g transform=\"translate(197.100505 68.197591)scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path d=\"M 9.421875 75.984375 \n",
       "L 18.40625 75.984375 \n",
       "L 18.40625 0 \n",
       "L 9.421875 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-108\"/>\n",
       "      <path d=\"M 30.609375 48.390625 \n",
       "Q 23.390625 48.390625 19.1875 42.75 \n",
       "Q 14.984375 37.109375 14.984375 27.296875 \n",
       "Q 14.984375 17.484375 19.15625 11.84375 \n",
       "Q 23.34375 6.203125 30.609375 6.203125 \n",
       "Q 37.796875 6.203125 41.984375 11.859375 \n",
       "Q 46.1875 17.53125 46.1875 27.296875 \n",
       "Q 46.1875 37.015625 41.984375 42.703125 \n",
       "Q 37.796875 48.390625 30.609375 48.390625 \n",
       "z\n",
       "M 30.609375 56 \n",
       "Q 42.328125 56 49.015625 48.375 \n",
       "Q 55.71875 40.765625 55.71875 27.296875 \n",
       "Q 55.71875 13.875 49.015625 6.21875 \n",
       "Q 42.328125 -1.421875 30.609375 -1.421875 \n",
       "Q 18.84375 -1.421875 12.171875 6.21875 \n",
       "Q 5.515625 13.875 5.515625 27.296875 \n",
       "Q 5.515625 40.765625 12.171875 48.375 \n",
       "Q 18.84375 56 30.609375 56 \n",
       "z\n",
       "\" id=\"DejaVuSans-111\"/>\n",
       "      <path d=\"M 41.109375 46.296875 \n",
       "Q 39.59375 47.171875 37.8125 47.578125 \n",
       "Q 36.03125 48 33.890625 48 \n",
       "Q 26.265625 48 22.1875 43.046875 \n",
       "Q 18.109375 38.09375 18.109375 28.8125 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "L 9.078125 54.6875 \n",
       "L 18.109375 54.6875 \n",
       "L 18.109375 46.1875 \n",
       "Q 20.953125 51.171875 25.484375 53.578125 \n",
       "Q 30.03125 56 36.53125 56 \n",
       "Q 37.453125 56 38.578125 55.875 \n",
       "Q 39.703125 55.765625 41.0625 55.515625 \n",
       "z\n",
       "\" id=\"DejaVuSans-114\"/>\n",
       "      <path d=\"M 45.40625 46.390625 \n",
       "L 45.40625 75.984375 \n",
       "L 54.390625 75.984375 \n",
       "L 54.390625 0 \n",
       "L 45.40625 0 \n",
       "L 45.40625 8.203125 \n",
       "Q 42.578125 3.328125 38.25 0.953125 \n",
       "Q 33.9375 -1.421875 27.875 -1.421875 \n",
       "Q 17.96875 -1.421875 11.734375 6.484375 \n",
       "Q 5.515625 14.40625 5.515625 27.296875 \n",
       "Q 5.515625 40.1875 11.734375 48.09375 \n",
       "Q 17.96875 56 27.875 56 \n",
       "Q 33.9375 56 38.25 53.625 \n",
       "Q 42.578125 51.265625 45.40625 46.390625 \n",
       "z\n",
       "M 14.796875 27.296875 \n",
       "Q 14.796875 17.390625 18.875 11.75 \n",
       "Q 22.953125 6.109375 30.078125 6.109375 \n",
       "Q 37.203125 6.109375 41.296875 11.75 \n",
       "Q 45.40625 17.390625 45.40625 27.296875 \n",
       "Q 45.40625 37.203125 41.296875 42.84375 \n",
       "Q 37.203125 48.484375 30.078125 48.484375 \n",
       "Q 22.953125 48.484375 18.875 42.84375 \n",
       "Q 14.796875 37.203125 14.796875 27.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-100\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-108\"/>\n",
       "     <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "     <use x=\"88.964844\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "     <use x=\"128.328125\" xlink:href=\"#DejaVuSans-100\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_14\">\n",
       "    <!-- man -->\n",
       "    <g transform=\"translate(146.93431 85.703432)scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path d=\"M 52 44.1875 \n",
       "Q 55.375 50.25 60.0625 53.125 \n",
       "Q 64.75 56 71.09375 56 \n",
       "Q 79.640625 56 84.28125 50.015625 \n",
       "Q 88.921875 44.046875 88.921875 33.015625 \n",
       "L 88.921875 0 \n",
       "L 79.890625 0 \n",
       "L 79.890625 32.71875 \n",
       "Q 79.890625 40.578125 77.09375 44.375 \n",
       "Q 74.3125 48.1875 68.609375 48.1875 \n",
       "Q 61.625 48.1875 57.5625 43.546875 \n",
       "Q 53.515625 38.921875 53.515625 30.90625 \n",
       "L 53.515625 0 \n",
       "L 44.484375 0 \n",
       "L 44.484375 32.71875 \n",
       "Q 44.484375 40.625 41.703125 44.40625 \n",
       "Q 38.921875 48.1875 33.109375 48.1875 \n",
       "Q 26.21875 48.1875 22.15625 43.53125 \n",
       "Q 18.109375 38.875 18.109375 30.90625 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "L 9.078125 54.6875 \n",
       "L 18.109375 54.6875 \n",
       "L 18.109375 46.1875 \n",
       "Q 21.1875 51.21875 25.484375 53.609375 \n",
       "Q 29.78125 56 35.6875 56 \n",
       "Q 41.65625 56 45.828125 52.96875 \n",
       "Q 50 49.953125 52 44.1875 \n",
       "z\n",
       "\" id=\"DejaVuSans-109\"/>\n",
       "      <path d=\"M 34.28125 27.484375 \n",
       "Q 23.390625 27.484375 19.1875 25 \n",
       "Q 14.984375 22.515625 14.984375 16.5 \n",
       "Q 14.984375 11.71875 18.140625 8.90625 \n",
       "Q 21.296875 6.109375 26.703125 6.109375 \n",
       "Q 34.1875 6.109375 38.703125 11.40625 \n",
       "Q 43.21875 16.703125 43.21875 25.484375 \n",
       "L 43.21875 27.484375 \n",
       "z\n",
       "M 52.203125 31.203125 \n",
       "L 52.203125 0 \n",
       "L 43.21875 0 \n",
       "L 43.21875 8.296875 \n",
       "Q 40.140625 3.328125 35.546875 0.953125 \n",
       "Q 30.953125 -1.421875 24.3125 -1.421875 \n",
       "Q 15.921875 -1.421875 10.953125 3.296875 \n",
       "Q 6 8.015625 6 15.921875 \n",
       "Q 6 25.140625 12.171875 29.828125 \n",
       "Q 18.359375 34.515625 30.609375 34.515625 \n",
       "L 43.21875 34.515625 \n",
       "L 43.21875 35.40625 \n",
       "Q 43.21875 41.609375 39.140625 45 \n",
       "Q 35.0625 48.390625 27.6875 48.390625 \n",
       "Q 23 48.390625 18.546875 47.265625 \n",
       "Q 14.109375 46.140625 10.015625 43.890625 \n",
       "L 10.015625 52.203125 \n",
       "Q 14.9375 54.109375 19.578125 55.046875 \n",
       "Q 24.21875 56 28.609375 56 \n",
       "Q 40.484375 56 46.34375 49.84375 \n",
       "Q 52.203125 43.703125 52.203125 31.203125 \n",
       "z\n",
       "\" id=\"DejaVuSans-97\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-109\"/>\n",
       "     <use x=\"97.412109\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "     <use x=\"158.691406\" xlink:href=\"#DejaVuSans-110\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_15\">\n",
       "    <!-- woman -->\n",
       "    <g transform=\"translate(120.895331 214.849417)scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path d=\"M 4.203125 54.6875 \n",
       "L 13.1875 54.6875 \n",
       "L 24.421875 12.015625 \n",
       "L 35.59375 54.6875 \n",
       "L 46.1875 54.6875 \n",
       "L 57.421875 12.015625 \n",
       "L 68.609375 54.6875 \n",
       "L 77.59375 54.6875 \n",
       "L 63.28125 0 \n",
       "L 52.6875 0 \n",
       "L 40.921875 44.828125 \n",
       "L 29.109375 0 \n",
       "L 18.5 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-119\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-119\"/>\n",
       "     <use x=\"81.787109\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "     <use x=\"142.96875\" xlink:href=\"#DejaVuSans-109\"/>\n",
       "     <use x=\"240.380859\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "     <use x=\"301.660156\" xlink:href=\"#DejaVuSans-110\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_16\">\n",
       "    <!-- dog -->\n",
       "    <g transform=\"translate(99.732716 155.443299)scale(0.1 -0.1)\">\n",
       "     <use xlink:href=\"#DejaVuSans-100\"/>\n",
       "     <use x=\"63.476562\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "     <use x=\"124.658203\" xlink:href=\"#DejaVuSans-103\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_17\">\n",
       "    <!-- horse -->\n",
       "    <g transform=\"translate(101.676147 98.60502)scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path d=\"M 54.890625 33.015625 \n",
       "L 54.890625 0 \n",
       "L 45.90625 0 \n",
       "L 45.90625 32.71875 \n",
       "Q 45.90625 40.484375 42.875 44.328125 \n",
       "Q 39.84375 48.1875 33.796875 48.1875 \n",
       "Q 26.515625 48.1875 22.3125 43.546875 \n",
       "Q 18.109375 38.921875 18.109375 30.90625 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "L 9.078125 75.984375 \n",
       "L 18.109375 75.984375 \n",
       "L 18.109375 46.1875 \n",
       "Q 21.34375 51.125 25.703125 53.5625 \n",
       "Q 30.078125 56 35.796875 56 \n",
       "Q 45.21875 56 50.046875 50.171875 \n",
       "Q 54.890625 44.34375 54.890625 33.015625 \n",
       "z\n",
       "\" id=\"DejaVuSans-104\"/>\n",
       "      <path d=\"M 44.28125 53.078125 \n",
       "L 44.28125 44.578125 \n",
       "Q 40.484375 46.53125 36.375 47.5 \n",
       "Q 32.28125 48.484375 27.875 48.484375 \n",
       "Q 21.1875 48.484375 17.84375 46.4375 \n",
       "Q 14.5 44.390625 14.5 40.28125 \n",
       "Q 14.5 37.15625 16.890625 35.375 \n",
       "Q 19.28125 33.59375 26.515625 31.984375 \n",
       "L 29.59375 31.296875 \n",
       "Q 39.15625 29.25 43.1875 25.515625 \n",
       "Q 47.21875 21.78125 47.21875 15.09375 \n",
       "Q 47.21875 7.46875 41.1875 3.015625 \n",
       "Q 35.15625 -1.421875 24.609375 -1.421875 \n",
       "Q 20.21875 -1.421875 15.453125 -0.5625 \n",
       "Q 10.6875 0.296875 5.421875 2 \n",
       "L 5.421875 11.28125 \n",
       "Q 10.40625 8.6875 15.234375 7.390625 \n",
       "Q 20.0625 6.109375 24.8125 6.109375 \n",
       "Q 31.15625 6.109375 34.5625 8.28125 \n",
       "Q 37.984375 10.453125 37.984375 14.40625 \n",
       "Q 37.984375 18.0625 35.515625 20.015625 \n",
       "Q 33.0625 21.96875 24.703125 23.78125 \n",
       "L 21.578125 24.515625 \n",
       "Q 13.234375 26.265625 9.515625 29.90625 \n",
       "Q 5.8125 33.546875 5.8125 39.890625 \n",
       "Q 5.8125 47.609375 11.28125 51.796875 \n",
       "Q 16.75 56 26.8125 56 \n",
       "Q 31.78125 56 36.171875 55.265625 \n",
       "Q 40.578125 54.546875 44.28125 53.078125 \n",
       "z\n",
       "\" id=\"DejaVuSans-115\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-104\"/>\n",
       "     <use x=\"63.378906\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "     <use x=\"124.560547\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "     <use x=\"165.673828\" xlink:href=\"#DejaVuSans-115\"/>\n",
       "     <use x=\"217.773438\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_18\">\n",
       "    <!-- rich -->\n",
       "    <g transform=\"translate(96.463031 17.17669)scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path d=\"M 48.78125 52.59375 \n",
       "L 48.78125 44.1875 \n",
       "Q 44.96875 46.296875 41.140625 47.34375 \n",
       "Q 37.3125 48.390625 33.40625 48.390625 \n",
       "Q 24.65625 48.390625 19.8125 42.84375 \n",
       "Q 14.984375 37.3125 14.984375 27.296875 \n",
       "Q 14.984375 17.28125 19.8125 11.734375 \n",
       "Q 24.65625 6.203125 33.40625 6.203125 \n",
       "Q 37.3125 6.203125 41.140625 7.25 \n",
       "Q 44.96875 8.296875 48.78125 10.40625 \n",
       "L 48.78125 2.09375 \n",
       "Q 45.015625 0.34375 40.984375 -0.53125 \n",
       "Q 36.96875 -1.421875 32.421875 -1.421875 \n",
       "Q 20.0625 -1.421875 12.78125 6.34375 \n",
       "Q 5.515625 14.109375 5.515625 27.296875 \n",
       "Q 5.515625 40.671875 12.859375 48.328125 \n",
       "Q 20.21875 56 33.015625 56 \n",
       "Q 37.15625 56 41.109375 55.140625 \n",
       "Q 45.0625 54.296875 48.78125 52.59375 \n",
       "z\n",
       "\" id=\"DejaVuSans-99\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-114\"/>\n",
       "     <use x=\"41.113281\" xlink:href=\"#DejaVuSans-105\"/>\n",
       "     <use x=\"68.896484\" xlink:href=\"#DejaVuSans-99\"/>\n",
       "     <use x=\"123.876953\" xlink:href=\"#DejaVuSans-104\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_19\">\n",
       "    <!-- poor -->\n",
       "    <g transform=\"translate(53.700994 73.87261)scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path d=\"M 18.109375 8.203125 \n",
       "L 18.109375 -20.796875 \n",
       "L 9.078125 -20.796875 \n",
       "L 9.078125 54.6875 \n",
       "L 18.109375 54.6875 \n",
       "L 18.109375 46.390625 \n",
       "Q 20.953125 51.265625 25.265625 53.625 \n",
       "Q 29.59375 56 35.59375 56 \n",
       "Q 45.5625 56 51.78125 48.09375 \n",
       "Q 58.015625 40.1875 58.015625 27.296875 \n",
       "Q 58.015625 14.40625 51.78125 6.484375 \n",
       "Q 45.5625 -1.421875 35.59375 -1.421875 \n",
       "Q 29.59375 -1.421875 25.265625 0.953125 \n",
       "Q 20.953125 3.328125 18.109375 8.203125 \n",
       "z\n",
       "M 48.6875 27.296875 \n",
       "Q 48.6875 37.203125 44.609375 42.84375 \n",
       "Q 40.53125 48.484375 33.40625 48.484375 \n",
       "Q 26.265625 48.484375 22.1875 42.84375 \n",
       "Q 18.109375 37.203125 18.109375 27.296875 \n",
       "Q 18.109375 17.390625 22.1875 11.75 \n",
       "Q 26.265625 6.109375 33.40625 6.109375 \n",
       "Q 40.53125 6.109375 44.609375 11.75 \n",
       "Q 48.6875 17.390625 48.6875 27.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-112\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-112\"/>\n",
       "     <use x=\"63.476562\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "     <use x=\"124.658203\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "     <use x=\"185.839844\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_20\">\n",
       "    <!-- sad -->\n",
       "    <g transform=\"translate(59.193441 176.195357)scale(0.1 -0.1)\">\n",
       "     <use xlink:href=\"#DejaVuSans-115\"/>\n",
       "     <use x=\"52.099609\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "     <use x=\"113.378906\" xlink:href=\"#DejaVuSans-100\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_21\">\n",
       "    <!-- happy -->\n",
       "    <g transform=\"translate(358.064631 149.504129)scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path d=\"M 32.171875 -5.078125 \n",
       "Q 28.375 -14.84375 24.75 -17.8125 \n",
       "Q 21.140625 -20.796875 15.09375 -20.796875 \n",
       "L 7.90625 -20.796875 \n",
       "L 7.90625 -13.28125 \n",
       "L 13.1875 -13.28125 \n",
       "Q 16.890625 -13.28125 18.9375 -11.515625 \n",
       "Q 21 -9.765625 23.484375 -3.21875 \n",
       "L 25.09375 0.875 \n",
       "L 2.984375 54.6875 \n",
       "L 12.5 54.6875 \n",
       "L 29.59375 11.921875 \n",
       "L 46.6875 54.6875 \n",
       "L 56.203125 54.6875 \n",
       "z\n",
       "\" id=\"DejaVuSans-121\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-104\"/>\n",
       "     <use x=\"63.378906\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "     <use x=\"124.658203\" xlink:href=\"#DejaVuSans-112\"/>\n",
       "     <use x=\"188.134766\" xlink:href=\"#DejaVuSans-112\"/>\n",
       "     <use x=\"251.611328\" xlink:href=\"#DejaVuSans-121\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p154f87226a\">\n",
       "   <rect height=\"217.44\" width=\"334.8\" x=\"38.482813\" y=\"7.293053\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result= compute_pca(X, 2)\n",
    "pyplot.scatter(result[:, 0], result[:, 1])\n",
    "for i, word in enumerate(words):\n",
    "    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Created with matplotlib (https://matplotlib.org/) -->\n",
       "<svg height=\"248.611178pt\" version=\"1.1\" viewBox=\"0 0 384.080256 248.611178\" width=\"384.080256pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2021-06-22T14:08:44.595963</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.3.4, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M -0 248.611178 \n",
       "L 384.080256 248.611178 \n",
       "L 384.080256 0 \n",
       "L -0 0 \n",
       "z\n",
       "\" style=\"fill:none;\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 38.482813 224.733053 \n",
       "L 373.282813 224.733053 \n",
       "L 373.282813 7.293053 \n",
       "L 38.482813 7.293053 \n",
       "z\n",
       "\" style=\"fill:#ffffff;\"/>\n",
       "   </g>\n",
       "   <g id=\"PathCollection_1\">\n",
       "    <defs>\n",
       "     <path d=\"M 0 3 \n",
       "C 0.795609 3 1.55874 2.683901 2.12132 2.12132 \n",
       "C 2.683901 1.55874 3 0.795609 3 0 \n",
       "C 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \n",
       "C 1.55874 -2.683901 0.795609 -3 0 -3 \n",
       "C -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \n",
       "C -2.683901 -1.55874 -3 -0.795609 -3 0 \n",
       "C -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \n",
       "C -1.55874 2.683901 -0.795609 3 0 3 \n",
       "z\n",
       "\" id=\"m5769bf0d5a\" style=\"stroke:#1f77b4;\"/>\n",
       "    </defs>\n",
       "    <g clip-path=\"url(#p48fb6647b3)\">\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"53.700994\" xlink:href=\"#m5769bf0d5a\" y=\"87.517725\"/>\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"267.839165\" xlink:href=\"#m5769bf0d5a\" y=\"45.135314\"/>\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"232.753713\" xlink:href=\"#m5769bf0d5a\" y=\"68.197591\"/>\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"286.569304\" xlink:href=\"#m5769bf0d5a\" y=\"85.703432\"/>\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"226.290119\" xlink:href=\"#m5769bf0d5a\" y=\"214.849417\"/>\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"358.064631\" xlink:href=\"#m5769bf0d5a\" y=\"155.443299\"/>\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"245.512419\" xlink:href=\"#m5769bf0d5a\" y=\"98.60502\"/>\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"280.07222\" xlink:href=\"#m5769bf0d5a\" y=\"17.17669\"/>\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"219.184994\" xlink:href=\"#m5769bf0d5a\" y=\"73.87261\"/>\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"229.520048\" xlink:href=\"#m5769bf0d5a\" y=\"176.195357\"/>\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"199.106527\" xlink:href=\"#m5769bf0d5a\" y=\"149.504129\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" id=\"mba2741db8f\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"48.946366\" xlink:href=\"#mba2741db8f\" y=\"224.733053\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- −1.25 -->\n",
       "      <g transform=\"translate(33.62371 239.331491)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path d=\"M 10.59375 35.5 \n",
       "L 73.1875 35.5 \n",
       "L 73.1875 27.203125 \n",
       "L 10.59375 27.203125 \n",
       "z\n",
       "\" id=\"DejaVuSans-8722\"/>\n",
       "        <path d=\"M 12.40625 8.296875 \n",
       "L 28.515625 8.296875 \n",
       "L 28.515625 63.921875 \n",
       "L 10.984375 60.40625 \n",
       "L 10.984375 69.390625 \n",
       "L 28.421875 72.90625 \n",
       "L 38.28125 72.90625 \n",
       "L 38.28125 8.296875 \n",
       "L 54.390625 8.296875 \n",
       "L 54.390625 0 \n",
       "L 12.40625 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-49\"/>\n",
       "        <path d=\"M 10.6875 12.40625 \n",
       "L 21 12.40625 \n",
       "L 21 0 \n",
       "L 10.6875 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-46\"/>\n",
       "        <path d=\"M 19.1875 8.296875 \n",
       "L 53.609375 8.296875 \n",
       "L 53.609375 0 \n",
       "L 7.328125 0 \n",
       "L 7.328125 8.296875 \n",
       "Q 12.9375 14.109375 22.625 23.890625 \n",
       "Q 32.328125 33.6875 34.8125 36.53125 \n",
       "Q 39.546875 41.84375 41.421875 45.53125 \n",
       "Q 43.3125 49.21875 43.3125 52.78125 \n",
       "Q 43.3125 58.59375 39.234375 62.25 \n",
       "Q 35.15625 65.921875 28.609375 65.921875 \n",
       "Q 23.96875 65.921875 18.8125 64.3125 \n",
       "Q 13.671875 62.703125 7.8125 59.421875 \n",
       "L 7.8125 69.390625 \n",
       "Q 13.765625 71.78125 18.9375 73 \n",
       "Q 24.125 74.21875 28.421875 74.21875 \n",
       "Q 39.75 74.21875 46.484375 68.546875 \n",
       "Q 53.21875 62.890625 53.21875 53.421875 \n",
       "Q 53.21875 48.921875 51.53125 44.890625 \n",
       "Q 49.859375 40.875 45.40625 35.40625 \n",
       "Q 44.1875 33.984375 37.640625 27.21875 \n",
       "Q 31.109375 20.453125 19.1875 8.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-50\"/>\n",
       "        <path d=\"M 10.796875 72.90625 \n",
       "L 49.515625 72.90625 \n",
       "L 49.515625 64.59375 \n",
       "L 19.828125 64.59375 \n",
       "L 19.828125 46.734375 \n",
       "Q 21.96875 47.46875 24.109375 47.828125 \n",
       "Q 26.265625 48.1875 28.421875 48.1875 \n",
       "Q 40.625 48.1875 47.75 41.5 \n",
       "Q 54.890625 34.8125 54.890625 23.390625 \n",
       "Q 54.890625 11.625 47.5625 5.09375 \n",
       "Q 40.234375 -1.421875 26.90625 -1.421875 \n",
       "Q 22.3125 -1.421875 17.546875 -0.640625 \n",
       "Q 12.796875 0.140625 7.71875 1.703125 \n",
       "L 7.71875 11.625 \n",
       "Q 12.109375 9.234375 16.796875 8.0625 \n",
       "Q 21.484375 6.890625 26.703125 6.890625 \n",
       "Q 35.15625 6.890625 40.078125 11.328125 \n",
       "Q 45.015625 15.765625 45.015625 23.390625 \n",
       "Q 45.015625 31 40.078125 35.4375 \n",
       "Q 35.15625 39.890625 26.703125 39.890625 \n",
       "Q 22.75 39.890625 18.8125 39.015625 \n",
       "Q 14.890625 38.140625 10.796875 36.28125 \n",
       "z\n",
       "\" id=\"DejaVuSans-53\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-8722\"/>\n",
       "       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-50\"/>\n",
       "       <use x=\"242.822266\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"86.404623\" xlink:href=\"#mba2741db8f\" y=\"224.733053\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- −1.00 -->\n",
       "      <g transform=\"translate(71.081967 239.331491)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path d=\"M 31.78125 66.40625 \n",
       "Q 24.171875 66.40625 20.328125 58.90625 \n",
       "Q 16.5 51.421875 16.5 36.375 \n",
       "Q 16.5 21.390625 20.328125 13.890625 \n",
       "Q 24.171875 6.390625 31.78125 6.390625 \n",
       "Q 39.453125 6.390625 43.28125 13.890625 \n",
       "Q 47.125 21.390625 47.125 36.375 \n",
       "Q 47.125 51.421875 43.28125 58.90625 \n",
       "Q 39.453125 66.40625 31.78125 66.40625 \n",
       "z\n",
       "M 31.78125 74.21875 \n",
       "Q 44.046875 74.21875 50.515625 64.515625 \n",
       "Q 56.984375 54.828125 56.984375 36.375 \n",
       "Q 56.984375 17.96875 50.515625 8.265625 \n",
       "Q 44.046875 -1.421875 31.78125 -1.421875 \n",
       "Q 19.53125 -1.421875 13.0625 8.265625 \n",
       "Q 6.59375 17.96875 6.59375 36.375 \n",
       "Q 6.59375 54.828125 13.0625 64.515625 \n",
       "Q 19.53125 74.21875 31.78125 74.21875 \n",
       "z\n",
       "\" id=\"DejaVuSans-48\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-8722\"/>\n",
       "       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"242.822266\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"123.862879\" xlink:href=\"#mba2741db8f\" y=\"224.733053\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- −0.75 -->\n",
       "      <g transform=\"translate(108.540223 239.331491)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path d=\"M 8.203125 72.90625 \n",
       "L 55.078125 72.90625 \n",
       "L 55.078125 68.703125 \n",
       "L 28.609375 0 \n",
       "L 18.3125 0 \n",
       "L 43.21875 64.59375 \n",
       "L 8.203125 64.59375 \n",
       "z\n",
       "\" id=\"DejaVuSans-55\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-8722\"/>\n",
       "       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-55\"/>\n",
       "       <use x=\"242.822266\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"161.321136\" xlink:href=\"#mba2741db8f\" y=\"224.733053\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- −0.50 -->\n",
       "      <g transform=\"translate(145.99848 239.331491)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-8722\"/>\n",
       "       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "       <use x=\"242.822266\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"198.779392\" xlink:href=\"#mba2741db8f\" y=\"224.733053\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- −0.25 -->\n",
       "      <g transform=\"translate(183.456736 239.331491)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-8722\"/>\n",
       "       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-50\"/>\n",
       "       <use x=\"242.822266\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"236.237649\" xlink:href=\"#mba2741db8f\" y=\"224.733053\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 0.00 -->\n",
       "      <g transform=\"translate(225.104836 239.331491)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_7\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"273.695905\" xlink:href=\"#mba2741db8f\" y=\"224.733053\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 0.25 -->\n",
       "      <g transform=\"translate(262.563093 239.331491)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n",
       "       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_8\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"311.154162\" xlink:href=\"#mba2741db8f\" y=\"224.733053\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 0.50 -->\n",
       "      <g transform=\"translate(300.021349 239.331491)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_9\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"348.612418\" xlink:href=\"#mba2741db8f\" y=\"224.733053\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 0.75 -->\n",
       "      <g transform=\"translate(337.479605 239.331491)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\n",
       "       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" id=\"mf15a6e4230\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"38.482813\" xlink:href=\"#mf15a6e4230\" y=\"202.12816\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- −1.0 -->\n",
       "      <g transform=\"translate(7.2 205.927379)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-8722\"/>\n",
       "       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"38.482813\" xlink:href=\"#mf15a6e4230\" y=\"154.345925\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- −0.5 -->\n",
       "      <g transform=\"translate(7.2 158.145144)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-8722\"/>\n",
       "       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"38.482813\" xlink:href=\"#mf15a6e4230\" y=\"106.563689\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 0.0 -->\n",
       "      <g transform=\"translate(15.579688 110.362908)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"38.482813\" xlink:href=\"#mf15a6e4230\" y=\"58.781454\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 0.5 -->\n",
       "      <g transform=\"translate(15.579688 62.580673)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_14\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"38.482813\" xlink:href=\"#mf15a6e4230\" y=\"10.999219\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_14\">\n",
       "      <!-- 1.0 -->\n",
       "      <g transform=\"translate(15.579688 14.798437)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 38.482813 224.733053 \n",
       "L 38.482813 7.293053 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 373.282813 224.733053 \n",
       "L 373.282813 7.293053 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 38.482813 224.733053 \n",
       "L 373.282812 224.733053 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 38.482813 7.293053 \n",
       "L 373.282812 7.293053 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"text_15\">\n",
       "    <!-- king -->\n",
       "    <g transform=\"translate(53.700994 87.517725)scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path d=\"M 9.078125 75.984375 \n",
       "L 18.109375 75.984375 \n",
       "L 18.109375 31.109375 \n",
       "L 44.921875 54.6875 \n",
       "L 56.390625 54.6875 \n",
       "L 27.390625 29.109375 \n",
       "L 57.625 0 \n",
       "L 45.90625 0 \n",
       "L 18.109375 26.703125 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-107\"/>\n",
       "      <path d=\"M 9.421875 54.6875 \n",
       "L 18.40625 54.6875 \n",
       "L 18.40625 0 \n",
       "L 9.421875 0 \n",
       "z\n",
       "M 9.421875 75.984375 \n",
       "L 18.40625 75.984375 \n",
       "L 18.40625 64.59375 \n",
       "L 9.421875 64.59375 \n",
       "z\n",
       "\" id=\"DejaVuSans-105\"/>\n",
       "      <path d=\"M 54.890625 33.015625 \n",
       "L 54.890625 0 \n",
       "L 45.90625 0 \n",
       "L 45.90625 32.71875 \n",
       "Q 45.90625 40.484375 42.875 44.328125 \n",
       "Q 39.84375 48.1875 33.796875 48.1875 \n",
       "Q 26.515625 48.1875 22.3125 43.546875 \n",
       "Q 18.109375 38.921875 18.109375 30.90625 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "L 9.078125 54.6875 \n",
       "L 18.109375 54.6875 \n",
       "L 18.109375 46.1875 \n",
       "Q 21.34375 51.125 25.703125 53.5625 \n",
       "Q 30.078125 56 35.796875 56 \n",
       "Q 45.21875 56 50.046875 50.171875 \n",
       "Q 54.890625 44.34375 54.890625 33.015625 \n",
       "z\n",
       "\" id=\"DejaVuSans-110\"/>\n",
       "      <path d=\"M 45.40625 27.984375 \n",
       "Q 45.40625 37.75 41.375 43.109375 \n",
       "Q 37.359375 48.484375 30.078125 48.484375 \n",
       "Q 22.859375 48.484375 18.828125 43.109375 \n",
       "Q 14.796875 37.75 14.796875 27.984375 \n",
       "Q 14.796875 18.265625 18.828125 12.890625 \n",
       "Q 22.859375 7.515625 30.078125 7.515625 \n",
       "Q 37.359375 7.515625 41.375 12.890625 \n",
       "Q 45.40625 18.265625 45.40625 27.984375 \n",
       "z\n",
       "M 54.390625 6.78125 \n",
       "Q 54.390625 -7.171875 48.1875 -13.984375 \n",
       "Q 42 -20.796875 29.203125 -20.796875 \n",
       "Q 24.46875 -20.796875 20.265625 -20.09375 \n",
       "Q 16.0625 -19.390625 12.109375 -17.921875 \n",
       "L 12.109375 -9.1875 \n",
       "Q 16.0625 -11.328125 19.921875 -12.34375 \n",
       "Q 23.78125 -13.375 27.78125 -13.375 \n",
       "Q 36.625 -13.375 41.015625 -8.765625 \n",
       "Q 45.40625 -4.15625 45.40625 5.171875 \n",
       "L 45.40625 9.625 \n",
       "Q 42.625 4.78125 38.28125 2.390625 \n",
       "Q 33.9375 0 27.875 0 \n",
       "Q 17.828125 0 11.671875 7.65625 \n",
       "Q 5.515625 15.328125 5.515625 27.984375 \n",
       "Q 5.515625 40.671875 11.671875 48.328125 \n",
       "Q 17.828125 56 27.875 56 \n",
       "Q 33.9375 56 38.28125 53.609375 \n",
       "Q 42.625 51.21875 45.40625 46.390625 \n",
       "L 45.40625 54.6875 \n",
       "L 54.390625 54.6875 \n",
       "z\n",
       "\" id=\"DejaVuSans-103\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-107\"/>\n",
       "     <use x=\"57.910156\" xlink:href=\"#DejaVuSans-105\"/>\n",
       "     <use x=\"85.693359\" xlink:href=\"#DejaVuSans-110\"/>\n",
       "     <use x=\"149.072266\" xlink:href=\"#DejaVuSans-103\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_16\">\n",
       "    <!-- queen -->\n",
       "    <g transform=\"translate(267.839165 45.135314)scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path d=\"M 14.796875 27.296875 \n",
       "Q 14.796875 17.390625 18.875 11.75 \n",
       "Q 22.953125 6.109375 30.078125 6.109375 \n",
       "Q 37.203125 6.109375 41.296875 11.75 \n",
       "Q 45.40625 17.390625 45.40625 27.296875 \n",
       "Q 45.40625 37.203125 41.296875 42.84375 \n",
       "Q 37.203125 48.484375 30.078125 48.484375 \n",
       "Q 22.953125 48.484375 18.875 42.84375 \n",
       "Q 14.796875 37.203125 14.796875 27.296875 \n",
       "z\n",
       "M 45.40625 8.203125 \n",
       "Q 42.578125 3.328125 38.25 0.953125 \n",
       "Q 33.9375 -1.421875 27.875 -1.421875 \n",
       "Q 17.96875 -1.421875 11.734375 6.484375 \n",
       "Q 5.515625 14.40625 5.515625 27.296875 \n",
       "Q 5.515625 40.1875 11.734375 48.09375 \n",
       "Q 17.96875 56 27.875 56 \n",
       "Q 33.9375 56 38.25 53.625 \n",
       "Q 42.578125 51.265625 45.40625 46.390625 \n",
       "L 45.40625 54.6875 \n",
       "L 54.390625 54.6875 \n",
       "L 54.390625 -20.796875 \n",
       "L 45.40625 -20.796875 \n",
       "z\n",
       "\" id=\"DejaVuSans-113\"/>\n",
       "      <path d=\"M 8.5 21.578125 \n",
       "L 8.5 54.6875 \n",
       "L 17.484375 54.6875 \n",
       "L 17.484375 21.921875 \n",
       "Q 17.484375 14.15625 20.5 10.265625 \n",
       "Q 23.53125 6.390625 29.59375 6.390625 \n",
       "Q 36.859375 6.390625 41.078125 11.03125 \n",
       "Q 45.3125 15.671875 45.3125 23.6875 \n",
       "L 45.3125 54.6875 \n",
       "L 54.296875 54.6875 \n",
       "L 54.296875 0 \n",
       "L 45.3125 0 \n",
       "L 45.3125 8.40625 \n",
       "Q 42.046875 3.421875 37.71875 1 \n",
       "Q 33.40625 -1.421875 27.6875 -1.421875 \n",
       "Q 18.265625 -1.421875 13.375 4.4375 \n",
       "Q 8.5 10.296875 8.5 21.578125 \n",
       "z\n",
       "M 31.109375 56 \n",
       "z\n",
       "\" id=\"DejaVuSans-117\"/>\n",
       "      <path d=\"M 56.203125 29.59375 \n",
       "L 56.203125 25.203125 \n",
       "L 14.890625 25.203125 \n",
       "Q 15.484375 15.921875 20.484375 11.0625 \n",
       "Q 25.484375 6.203125 34.421875 6.203125 \n",
       "Q 39.59375 6.203125 44.453125 7.46875 \n",
       "Q 49.3125 8.734375 54.109375 11.28125 \n",
       "L 54.109375 2.78125 \n",
       "Q 49.265625 0.734375 44.1875 -0.34375 \n",
       "Q 39.109375 -1.421875 33.890625 -1.421875 \n",
       "Q 20.796875 -1.421875 13.15625 6.1875 \n",
       "Q 5.515625 13.8125 5.515625 26.8125 \n",
       "Q 5.515625 40.234375 12.765625 48.109375 \n",
       "Q 20.015625 56 32.328125 56 \n",
       "Q 43.359375 56 49.78125 48.890625 \n",
       "Q 56.203125 41.796875 56.203125 29.59375 \n",
       "z\n",
       "M 47.21875 32.234375 \n",
       "Q 47.125 39.59375 43.09375 43.984375 \n",
       "Q 39.0625 48.390625 32.421875 48.390625 \n",
       "Q 24.90625 48.390625 20.390625 44.140625 \n",
       "Q 15.875 39.890625 15.1875 32.171875 \n",
       "z\n",
       "\" id=\"DejaVuSans-101\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-113\"/>\n",
       "     <use x=\"63.476562\" xlink:href=\"#DejaVuSans-117\"/>\n",
       "     <use x=\"126.855469\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "     <use x=\"188.378906\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "     <use x=\"249.902344\" xlink:href=\"#DejaVuSans-110\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_17\">\n",
       "    <!-- lord -->\n",
       "    <g transform=\"translate(232.753713 68.197591)scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path d=\"M 9.421875 75.984375 \n",
       "L 18.40625 75.984375 \n",
       "L 18.40625 0 \n",
       "L 9.421875 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-108\"/>\n",
       "      <path d=\"M 30.609375 48.390625 \n",
       "Q 23.390625 48.390625 19.1875 42.75 \n",
       "Q 14.984375 37.109375 14.984375 27.296875 \n",
       "Q 14.984375 17.484375 19.15625 11.84375 \n",
       "Q 23.34375 6.203125 30.609375 6.203125 \n",
       "Q 37.796875 6.203125 41.984375 11.859375 \n",
       "Q 46.1875 17.53125 46.1875 27.296875 \n",
       "Q 46.1875 37.015625 41.984375 42.703125 \n",
       "Q 37.796875 48.390625 30.609375 48.390625 \n",
       "z\n",
       "M 30.609375 56 \n",
       "Q 42.328125 56 49.015625 48.375 \n",
       "Q 55.71875 40.765625 55.71875 27.296875 \n",
       "Q 55.71875 13.875 49.015625 6.21875 \n",
       "Q 42.328125 -1.421875 30.609375 -1.421875 \n",
       "Q 18.84375 -1.421875 12.171875 6.21875 \n",
       "Q 5.515625 13.875 5.515625 27.296875 \n",
       "Q 5.515625 40.765625 12.171875 48.375 \n",
       "Q 18.84375 56 30.609375 56 \n",
       "z\n",
       "\" id=\"DejaVuSans-111\"/>\n",
       "      <path d=\"M 41.109375 46.296875 \n",
       "Q 39.59375 47.171875 37.8125 47.578125 \n",
       "Q 36.03125 48 33.890625 48 \n",
       "Q 26.265625 48 22.1875 43.046875 \n",
       "Q 18.109375 38.09375 18.109375 28.8125 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "L 9.078125 54.6875 \n",
       "L 18.109375 54.6875 \n",
       "L 18.109375 46.1875 \n",
       "Q 20.953125 51.171875 25.484375 53.578125 \n",
       "Q 30.03125 56 36.53125 56 \n",
       "Q 37.453125 56 38.578125 55.875 \n",
       "Q 39.703125 55.765625 41.0625 55.515625 \n",
       "z\n",
       "\" id=\"DejaVuSans-114\"/>\n",
       "      <path d=\"M 45.40625 46.390625 \n",
       "L 45.40625 75.984375 \n",
       "L 54.390625 75.984375 \n",
       "L 54.390625 0 \n",
       "L 45.40625 0 \n",
       "L 45.40625 8.203125 \n",
       "Q 42.578125 3.328125 38.25 0.953125 \n",
       "Q 33.9375 -1.421875 27.875 -1.421875 \n",
       "Q 17.96875 -1.421875 11.734375 6.484375 \n",
       "Q 5.515625 14.40625 5.515625 27.296875 \n",
       "Q 5.515625 40.1875 11.734375 48.09375 \n",
       "Q 17.96875 56 27.875 56 \n",
       "Q 33.9375 56 38.25 53.625 \n",
       "Q 42.578125 51.265625 45.40625 46.390625 \n",
       "z\n",
       "M 14.796875 27.296875 \n",
       "Q 14.796875 17.390625 18.875 11.75 \n",
       "Q 22.953125 6.109375 30.078125 6.109375 \n",
       "Q 37.203125 6.109375 41.296875 11.75 \n",
       "Q 45.40625 17.390625 45.40625 27.296875 \n",
       "Q 45.40625 37.203125 41.296875 42.84375 \n",
       "Q 37.203125 48.484375 30.078125 48.484375 \n",
       "Q 22.953125 48.484375 18.875 42.84375 \n",
       "Q 14.796875 37.203125 14.796875 27.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-100\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-108\"/>\n",
       "     <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "     <use x=\"88.964844\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "     <use x=\"128.328125\" xlink:href=\"#DejaVuSans-100\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_18\">\n",
       "    <!-- man -->\n",
       "    <g transform=\"translate(286.569304 85.703432)scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path d=\"M 52 44.1875 \n",
       "Q 55.375 50.25 60.0625 53.125 \n",
       "Q 64.75 56 71.09375 56 \n",
       "Q 79.640625 56 84.28125 50.015625 \n",
       "Q 88.921875 44.046875 88.921875 33.015625 \n",
       "L 88.921875 0 \n",
       "L 79.890625 0 \n",
       "L 79.890625 32.71875 \n",
       "Q 79.890625 40.578125 77.09375 44.375 \n",
       "Q 74.3125 48.1875 68.609375 48.1875 \n",
       "Q 61.625 48.1875 57.5625 43.546875 \n",
       "Q 53.515625 38.921875 53.515625 30.90625 \n",
       "L 53.515625 0 \n",
       "L 44.484375 0 \n",
       "L 44.484375 32.71875 \n",
       "Q 44.484375 40.625 41.703125 44.40625 \n",
       "Q 38.921875 48.1875 33.109375 48.1875 \n",
       "Q 26.21875 48.1875 22.15625 43.53125 \n",
       "Q 18.109375 38.875 18.109375 30.90625 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "L 9.078125 54.6875 \n",
       "L 18.109375 54.6875 \n",
       "L 18.109375 46.1875 \n",
       "Q 21.1875 51.21875 25.484375 53.609375 \n",
       "Q 29.78125 56 35.6875 56 \n",
       "Q 41.65625 56 45.828125 52.96875 \n",
       "Q 50 49.953125 52 44.1875 \n",
       "z\n",
       "\" id=\"DejaVuSans-109\"/>\n",
       "      <path d=\"M 34.28125 27.484375 \n",
       "Q 23.390625 27.484375 19.1875 25 \n",
       "Q 14.984375 22.515625 14.984375 16.5 \n",
       "Q 14.984375 11.71875 18.140625 8.90625 \n",
       "Q 21.296875 6.109375 26.703125 6.109375 \n",
       "Q 34.1875 6.109375 38.703125 11.40625 \n",
       "Q 43.21875 16.703125 43.21875 25.484375 \n",
       "L 43.21875 27.484375 \n",
       "z\n",
       "M 52.203125 31.203125 \n",
       "L 52.203125 0 \n",
       "L 43.21875 0 \n",
       "L 43.21875 8.296875 \n",
       "Q 40.140625 3.328125 35.546875 0.953125 \n",
       "Q 30.953125 -1.421875 24.3125 -1.421875 \n",
       "Q 15.921875 -1.421875 10.953125 3.296875 \n",
       "Q 6 8.015625 6 15.921875 \n",
       "Q 6 25.140625 12.171875 29.828125 \n",
       "Q 18.359375 34.515625 30.609375 34.515625 \n",
       "L 43.21875 34.515625 \n",
       "L 43.21875 35.40625 \n",
       "Q 43.21875 41.609375 39.140625 45 \n",
       "Q 35.0625 48.390625 27.6875 48.390625 \n",
       "Q 23 48.390625 18.546875 47.265625 \n",
       "Q 14.109375 46.140625 10.015625 43.890625 \n",
       "L 10.015625 52.203125 \n",
       "Q 14.9375 54.109375 19.578125 55.046875 \n",
       "Q 24.21875 56 28.609375 56 \n",
       "Q 40.484375 56 46.34375 49.84375 \n",
       "Q 52.203125 43.703125 52.203125 31.203125 \n",
       "z\n",
       "\" id=\"DejaVuSans-97\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-109\"/>\n",
       "     <use x=\"97.412109\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "     <use x=\"158.691406\" xlink:href=\"#DejaVuSans-110\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_19\">\n",
       "    <!-- woman -->\n",
       "    <g transform=\"translate(226.290119 214.849417)scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path d=\"M 4.203125 54.6875 \n",
       "L 13.1875 54.6875 \n",
       "L 24.421875 12.015625 \n",
       "L 35.59375 54.6875 \n",
       "L 46.1875 54.6875 \n",
       "L 57.421875 12.015625 \n",
       "L 68.609375 54.6875 \n",
       "L 77.59375 54.6875 \n",
       "L 63.28125 0 \n",
       "L 52.6875 0 \n",
       "L 40.921875 44.828125 \n",
       "L 29.109375 0 \n",
       "L 18.5 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-119\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-119\"/>\n",
       "     <use x=\"81.787109\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "     <use x=\"142.96875\" xlink:href=\"#DejaVuSans-109\"/>\n",
       "     <use x=\"240.380859\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "     <use x=\"301.660156\" xlink:href=\"#DejaVuSans-110\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_20\">\n",
       "    <!-- dog -->\n",
       "    <g transform=\"translate(358.064631 155.443299)scale(0.1 -0.1)\">\n",
       "     <use xlink:href=\"#DejaVuSans-100\"/>\n",
       "     <use x=\"63.476562\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "     <use x=\"124.658203\" xlink:href=\"#DejaVuSans-103\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_21\">\n",
       "    <!-- horse -->\n",
       "    <g transform=\"translate(245.512419 98.60502)scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path d=\"M 54.890625 33.015625 \n",
       "L 54.890625 0 \n",
       "L 45.90625 0 \n",
       "L 45.90625 32.71875 \n",
       "Q 45.90625 40.484375 42.875 44.328125 \n",
       "Q 39.84375 48.1875 33.796875 48.1875 \n",
       "Q 26.515625 48.1875 22.3125 43.546875 \n",
       "Q 18.109375 38.921875 18.109375 30.90625 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "L 9.078125 75.984375 \n",
       "L 18.109375 75.984375 \n",
       "L 18.109375 46.1875 \n",
       "Q 21.34375 51.125 25.703125 53.5625 \n",
       "Q 30.078125 56 35.796875 56 \n",
       "Q 45.21875 56 50.046875 50.171875 \n",
       "Q 54.890625 44.34375 54.890625 33.015625 \n",
       "z\n",
       "\" id=\"DejaVuSans-104\"/>\n",
       "      <path d=\"M 44.28125 53.078125 \n",
       "L 44.28125 44.578125 \n",
       "Q 40.484375 46.53125 36.375 47.5 \n",
       "Q 32.28125 48.484375 27.875 48.484375 \n",
       "Q 21.1875 48.484375 17.84375 46.4375 \n",
       "Q 14.5 44.390625 14.5 40.28125 \n",
       "Q 14.5 37.15625 16.890625 35.375 \n",
       "Q 19.28125 33.59375 26.515625 31.984375 \n",
       "L 29.59375 31.296875 \n",
       "Q 39.15625 29.25 43.1875 25.515625 \n",
       "Q 47.21875 21.78125 47.21875 15.09375 \n",
       "Q 47.21875 7.46875 41.1875 3.015625 \n",
       "Q 35.15625 -1.421875 24.609375 -1.421875 \n",
       "Q 20.21875 -1.421875 15.453125 -0.5625 \n",
       "Q 10.6875 0.296875 5.421875 2 \n",
       "L 5.421875 11.28125 \n",
       "Q 10.40625 8.6875 15.234375 7.390625 \n",
       "Q 20.0625 6.109375 24.8125 6.109375 \n",
       "Q 31.15625 6.109375 34.5625 8.28125 \n",
       "Q 37.984375 10.453125 37.984375 14.40625 \n",
       "Q 37.984375 18.0625 35.515625 20.015625 \n",
       "Q 33.0625 21.96875 24.703125 23.78125 \n",
       "L 21.578125 24.515625 \n",
       "Q 13.234375 26.265625 9.515625 29.90625 \n",
       "Q 5.8125 33.546875 5.8125 39.890625 \n",
       "Q 5.8125 47.609375 11.28125 51.796875 \n",
       "Q 16.75 56 26.8125 56 \n",
       "Q 31.78125 56 36.171875 55.265625 \n",
       "Q 40.578125 54.546875 44.28125 53.078125 \n",
       "z\n",
       "\" id=\"DejaVuSans-115\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-104\"/>\n",
       "     <use x=\"63.378906\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "     <use x=\"124.560547\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "     <use x=\"165.673828\" xlink:href=\"#DejaVuSans-115\"/>\n",
       "     <use x=\"217.773438\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_22\">\n",
       "    <!-- rich -->\n",
       "    <g transform=\"translate(280.07222 17.17669)scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path d=\"M 48.78125 52.59375 \n",
       "L 48.78125 44.1875 \n",
       "Q 44.96875 46.296875 41.140625 47.34375 \n",
       "Q 37.3125 48.390625 33.40625 48.390625 \n",
       "Q 24.65625 48.390625 19.8125 42.84375 \n",
       "Q 14.984375 37.3125 14.984375 27.296875 \n",
       "Q 14.984375 17.28125 19.8125 11.734375 \n",
       "Q 24.65625 6.203125 33.40625 6.203125 \n",
       "Q 37.3125 6.203125 41.140625 7.25 \n",
       "Q 44.96875 8.296875 48.78125 10.40625 \n",
       "L 48.78125 2.09375 \n",
       "Q 45.015625 0.34375 40.984375 -0.53125 \n",
       "Q 36.96875 -1.421875 32.421875 -1.421875 \n",
       "Q 20.0625 -1.421875 12.78125 6.34375 \n",
       "Q 5.515625 14.109375 5.515625 27.296875 \n",
       "Q 5.515625 40.671875 12.859375 48.328125 \n",
       "Q 20.21875 56 33.015625 56 \n",
       "Q 37.15625 56 41.109375 55.140625 \n",
       "Q 45.0625 54.296875 48.78125 52.59375 \n",
       "z\n",
       "\" id=\"DejaVuSans-99\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-114\"/>\n",
       "     <use x=\"41.113281\" xlink:href=\"#DejaVuSans-105\"/>\n",
       "     <use x=\"68.896484\" xlink:href=\"#DejaVuSans-99\"/>\n",
       "     <use x=\"123.876953\" xlink:href=\"#DejaVuSans-104\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_23\">\n",
       "    <!-- poor -->\n",
       "    <g transform=\"translate(219.184994 73.87261)scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path d=\"M 18.109375 8.203125 \n",
       "L 18.109375 -20.796875 \n",
       "L 9.078125 -20.796875 \n",
       "L 9.078125 54.6875 \n",
       "L 18.109375 54.6875 \n",
       "L 18.109375 46.390625 \n",
       "Q 20.953125 51.265625 25.265625 53.625 \n",
       "Q 29.59375 56 35.59375 56 \n",
       "Q 45.5625 56 51.78125 48.09375 \n",
       "Q 58.015625 40.1875 58.015625 27.296875 \n",
       "Q 58.015625 14.40625 51.78125 6.484375 \n",
       "Q 45.5625 -1.421875 35.59375 -1.421875 \n",
       "Q 29.59375 -1.421875 25.265625 0.953125 \n",
       "Q 20.953125 3.328125 18.109375 8.203125 \n",
       "z\n",
       "M 48.6875 27.296875 \n",
       "Q 48.6875 37.203125 44.609375 42.84375 \n",
       "Q 40.53125 48.484375 33.40625 48.484375 \n",
       "Q 26.265625 48.484375 22.1875 42.84375 \n",
       "Q 18.109375 37.203125 18.109375 27.296875 \n",
       "Q 18.109375 17.390625 22.1875 11.75 \n",
       "Q 26.265625 6.109375 33.40625 6.109375 \n",
       "Q 40.53125 6.109375 44.609375 11.75 \n",
       "Q 48.6875 17.390625 48.6875 27.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-112\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-112\"/>\n",
       "     <use x=\"63.476562\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "     <use x=\"124.658203\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "     <use x=\"185.839844\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_24\">\n",
       "    <!-- sad -->\n",
       "    <g transform=\"translate(229.520048 176.195357)scale(0.1 -0.1)\">\n",
       "     <use xlink:href=\"#DejaVuSans-115\"/>\n",
       "     <use x=\"52.099609\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "     <use x=\"113.378906\" xlink:href=\"#DejaVuSans-100\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_25\">\n",
       "    <!-- happy -->\n",
       "    <g transform=\"translate(199.106527 149.504129)scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path d=\"M 32.171875 -5.078125 \n",
       "Q 28.375 -14.84375 24.75 -17.8125 \n",
       "Q 21.140625 -20.796875 15.09375 -20.796875 \n",
       "L 7.90625 -20.796875 \n",
       "L 7.90625 -13.28125 \n",
       "L 13.1875 -13.28125 \n",
       "Q 16.890625 -13.28125 18.9375 -11.515625 \n",
       "Q 21 -9.765625 23.484375 -3.21875 \n",
       "L 25.09375 0.875 \n",
       "L 2.984375 54.6875 \n",
       "L 12.5 54.6875 \n",
       "L 29.59375 11.921875 \n",
       "L 46.6875 54.6875 \n",
       "L 56.203125 54.6875 \n",
       "z\n",
       "\" id=\"DejaVuSans-121\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-104\"/>\n",
       "     <use x=\"63.378906\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "     <use x=\"124.658203\" xlink:href=\"#DejaVuSans-112\"/>\n",
       "     <use x=\"188.134766\" xlink:href=\"#DejaVuSans-112\"/>\n",
       "     <use x=\"251.611328\" xlink:href=\"#DejaVuSans-121\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p48fb6647b3\">\n",
       "   <rect height=\"217.44\" width=\"334.8\" x=\"38.482813\" y=\"7.293053\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result= compute_pca(X, 4)\n",
    "pyplot.scatter(result[:, 3], result[:, 1])\n",
    "for i, word in enumerate(words):\n",
    "    pyplot.annotate(word, xy=(result[i, 3], result[i, 1]))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
