{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#C3W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import random as rnd\n",
    "\n",
    "import trax\n",
    "\n",
    "trax.supervised.training.Loop\n",
    "\n",
    "import trax.fastmath.numpy as np\n",
    "\n",
    "from trax import layers as tl\n",
    "\n",
    "from utilsc3w1 import Layer, load_tweets, process_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2. Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2.1. Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of positive tweets: 5000\n",
      "The number of negative tweets: 5000\n",
      "length of train_x 8000\n",
      "length of val_x 2000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "all_positive_tweets, all_negative_tweets = load_tweets()\n",
    "\n",
    "print(f\"The number of positive tweets: {len(all_positive_tweets)}\")\n",
    "print(f\"The number of negative tweets: {len(all_negative_tweets)}\")\n",
    "\n",
    "val_pos   = all_positive_tweets[4000:]\n",
    "train_pos  = all_positive_tweets[:4000]\n",
    "\n",
    "val_neg   = all_negative_tweets[4000:]\n",
    "train_neg  = all_negative_tweets[:4000]\n",
    "\n",
    "train_x = train_pos + train_neg \n",
    "\n",
    "val_x  = val_pos + val_neg\n",
    "\n",
    "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
    "\n",
    "val_y  = np.append(np.ones(len(val_pos)), np.zeros(len(val_neg)))\n",
    "\n",
    "print(f\"length of train_x {len(train_x)}\")\n",
    "print(f\"length of val_x {len(val_x)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original tweet at training position 0\n",
      "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "Tweet at training position 0 after processing:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"original tweet at training position 0\")\n",
    "print(train_pos[0])\n",
    "\n",
    "print(\"Tweet at training position 0 after processing:\")\n",
    "process_tweet(train_pos[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2.2. Building the Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in vocab are 9088\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'__PAD__': 0,\n",
       " '__</e>__': 1,\n",
       " '__UNK__': 2,\n",
       " 'followfriday': 3,\n",
       " 'top': 4,\n",
       " 'engag': 5,\n",
       " 'member': 6,\n",
       " 'commun': 7,\n",
       " 'week': 8,\n",
       " ':)': 9,\n",
       " 'hey': 10,\n",
       " 'jame': 11,\n",
       " 'odd': 12,\n",
       " ':/': 13,\n",
       " 'pleas': 14,\n",
       " 'call': 15,\n",
       " 'contact': 16,\n",
       " 'centr': 17,\n",
       " '02392441234': 18,\n",
       " 'abl': 19,\n",
       " 'assist': 20,\n",
       " 'mani': 21,\n",
       " 'thank': 22,\n",
       " 'listen': 23,\n",
       " 'last': 24,\n",
       " 'night': 25,\n",
       " 'bleed': 26,\n",
       " 'amaz': 27,\n",
       " 'track': 28,\n",
       " 'scotland': 29,\n",
       " 'congrat': 30,\n",
       " 'yeaaah': 31,\n",
       " 'yipppi': 32,\n",
       " 'accnt': 33,\n",
       " 'verifi': 34,\n",
       " 'rqst': 35,\n",
       " 'succeed': 36,\n",
       " 'got': 37,\n",
       " 'blue': 38,\n",
       " 'tick': 39,\n",
       " 'mark': 40,\n",
       " 'fb': 41,\n",
       " 'profil': 42,\n",
       " '15': 43,\n",
       " 'day': 44,\n",
       " 'one': 45,\n",
       " 'irresist': 46,\n",
       " 'flipkartfashionfriday': 47,\n",
       " 'like': 48,\n",
       " 'keep': 49,\n",
       " 'love': 50,\n",
       " 'custom': 51,\n",
       " 'wait': 52,\n",
       " 'long': 53,\n",
       " 'hope': 54,\n",
       " 'enjoy': 55,\n",
       " 'happi': 56,\n",
       " 'friday': 57,\n",
       " 'lwwf': 58,\n",
       " 'second': 59,\n",
       " 'thought': 60,\n",
       " '‚Äô': 61,\n",
       " 'enough': 62,\n",
       " 'time': 63,\n",
       " 'dd': 64,\n",
       " 'new': 65,\n",
       " 'short': 66,\n",
       " 'enter': 67,\n",
       " 'system': 68,\n",
       " 'sheep': 69,\n",
       " 'must': 70,\n",
       " 'buy': 71,\n",
       " 'jgh': 72,\n",
       " 'go': 73,\n",
       " 'bayan': 74,\n",
       " ':D': 75,\n",
       " 'bye': 76,\n",
       " 'act': 77,\n",
       " 'mischiev': 78,\n",
       " 'etl': 79,\n",
       " 'layer': 80,\n",
       " 'in-hous': 81,\n",
       " 'wareh': 82,\n",
       " 'app': 83,\n",
       " 'katamari': 84,\n",
       " 'well': 85,\n",
       " '‚Ä¶': 86,\n",
       " 'name': 87,\n",
       " 'impli': 88,\n",
       " ':p': 89,\n",
       " 'influenc': 90,\n",
       " 'big': 91,\n",
       " '...': 92,\n",
       " 'juici': 93,\n",
       " 'selfi': 94,\n",
       " 'follow': 95,\n",
       " 'perfect': 96,\n",
       " 'alreadi': 97,\n",
       " 'know': 98,\n",
       " \"what'\": 99,\n",
       " 'great': 100,\n",
       " 'opportun': 101,\n",
       " 'junior': 102,\n",
       " 'triathlet': 103,\n",
       " 'age': 104,\n",
       " '12': 105,\n",
       " '13': 106,\n",
       " 'gatorad': 107,\n",
       " 'seri': 108,\n",
       " 'get': 109,\n",
       " 'entri': 110,\n",
       " 'lay': 111,\n",
       " 'greet': 112,\n",
       " 'card': 113,\n",
       " 'rang': 114,\n",
       " 'print': 115,\n",
       " 'today': 116,\n",
       " 'job': 117,\n",
       " ':-)': 118,\n",
       " \"friend'\": 119,\n",
       " 'lunch': 120,\n",
       " 'yummm': 121,\n",
       " 'nostalgia': 122,\n",
       " 'tb': 123,\n",
       " 'ku': 124,\n",
       " 'id': 125,\n",
       " 'conflict': 126,\n",
       " 'help': 127,\n",
       " \"here'\": 128,\n",
       " 'screenshot': 129,\n",
       " 'work': 130,\n",
       " 'hi': 131,\n",
       " 'liv': 132,\n",
       " 'hello': 133,\n",
       " 'need': 134,\n",
       " 'someth': 135,\n",
       " 'u': 136,\n",
       " 'fm': 137,\n",
       " 'twitter': 138,\n",
       " '‚Äî': 139,\n",
       " 'sure': 140,\n",
       " 'thing': 141,\n",
       " 'dm': 142,\n",
       " 'x': 143,\n",
       " \"i'v\": 144,\n",
       " 'heard': 145,\n",
       " 'four': 146,\n",
       " 'season': 147,\n",
       " 'pretti': 148,\n",
       " 'dope': 149,\n",
       " 'penthous': 150,\n",
       " 'obv': 151,\n",
       " 'gobigorgohom': 152,\n",
       " 'fun': 153,\n",
       " \"y'all\": 154,\n",
       " 'yeah': 155,\n",
       " 'suppos': 156,\n",
       " 'lol': 157,\n",
       " 'chat': 158,\n",
       " 'bit': 159,\n",
       " 'youth': 160,\n",
       " 'üíÖ': 161,\n",
       " 'üèΩ': 162,\n",
       " 'üíã': 163,\n",
       " 'seen': 164,\n",
       " 'year': 165,\n",
       " 'rest': 166,\n",
       " 'goe': 167,\n",
       " 'quickli': 168,\n",
       " 'bed': 169,\n",
       " 'music': 170,\n",
       " 'fix': 171,\n",
       " 'dream': 172,\n",
       " 'spiritu': 173,\n",
       " 'ritual': 174,\n",
       " 'festiv': 175,\n",
       " 'n√©pal': 176,\n",
       " 'begin': 177,\n",
       " 'line-up': 178,\n",
       " 'left': 179,\n",
       " 'see': 180,\n",
       " 'sarah': 181,\n",
       " 'send': 182,\n",
       " 'us': 183,\n",
       " 'email': 184,\n",
       " 'bitsy@bitdefender.com': 185,\n",
       " \"we'll\": 186,\n",
       " 'asap': 187,\n",
       " 'kik': 188,\n",
       " 'hatessuc': 189,\n",
       " '32429': 190,\n",
       " 'kikm': 191,\n",
       " 'lgbt': 192,\n",
       " 'tinder': 193,\n",
       " 'nsfw': 194,\n",
       " 'akua': 195,\n",
       " 'cumshot': 196,\n",
       " 'come': 197,\n",
       " 'hous': 198,\n",
       " 'nsn_supplement': 199,\n",
       " 'effect': 200,\n",
       " 'press': 201,\n",
       " 'releas': 202,\n",
       " 'distribut': 203,\n",
       " 'result': 204,\n",
       " 'link': 205,\n",
       " 'remov': 206,\n",
       " 'pressreleas': 207,\n",
       " 'newsdistribut': 208,\n",
       " 'bam': 209,\n",
       " 'bestfriend': 210,\n",
       " 'lot': 211,\n",
       " 'warsaw': 212,\n",
       " '<3': 213,\n",
       " 'x46': 214,\n",
       " 'everyon': 215,\n",
       " 'watch': 216,\n",
       " 'documentari': 217,\n",
       " 'earthl': 218,\n",
       " 'youtub': 219,\n",
       " 'support': 220,\n",
       " 'buuut': 221,\n",
       " 'oh': 222,\n",
       " 'look': 223,\n",
       " 'forward': 224,\n",
       " 'visit': 225,\n",
       " 'next': 226,\n",
       " 'letsgetmessi': 227,\n",
       " 'jo': 228,\n",
       " 'make': 229,\n",
       " 'feel': 230,\n",
       " 'better': 231,\n",
       " 'never': 232,\n",
       " 'anyon': 233,\n",
       " 'kpop': 234,\n",
       " 'flesh': 235,\n",
       " 'good': 236,\n",
       " 'girl': 237,\n",
       " 'best': 238,\n",
       " 'wish': 239,\n",
       " 'reason': 240,\n",
       " 'epic': 241,\n",
       " 'soundtrack': 242,\n",
       " 'shout': 243,\n",
       " 'ad': 244,\n",
       " 'video': 245,\n",
       " 'playlist': 246,\n",
       " 'would': 247,\n",
       " 'dear': 248,\n",
       " 'jordan': 249,\n",
       " 'okay': 250,\n",
       " 'fake': 251,\n",
       " 'gameplay': 252,\n",
       " ';)': 253,\n",
       " 'haha': 254,\n",
       " 'im': 255,\n",
       " 'kid': 256,\n",
       " 'stuff': 257,\n",
       " 'exactli': 258,\n",
       " 'product': 259,\n",
       " 'line': 260,\n",
       " 'etsi': 261,\n",
       " 'shop': 262,\n",
       " 'check': 263,\n",
       " 'vacat': 264,\n",
       " 'recharg': 265,\n",
       " 'normal': 266,\n",
       " 'charger': 267,\n",
       " 'asleep': 268,\n",
       " 'talk': 269,\n",
       " 'sooo': 270,\n",
       " 'someon': 271,\n",
       " 'text': 272,\n",
       " 'ye': 273,\n",
       " 'bet': 274,\n",
       " \"he'll\": 275,\n",
       " 'fit': 276,\n",
       " 'hear': 277,\n",
       " 'speech': 278,\n",
       " 'piti': 279,\n",
       " 'green': 280,\n",
       " 'garden': 281,\n",
       " 'midnight': 282,\n",
       " 'sun': 283,\n",
       " 'beauti': 284,\n",
       " 'canal': 285,\n",
       " 'dasvidaniya': 286,\n",
       " 'till': 287,\n",
       " 'scout': 288,\n",
       " 'sg': 289,\n",
       " 'futur': 290,\n",
       " 'wlan': 291,\n",
       " 'pro': 292,\n",
       " 'confer': 293,\n",
       " 'asia': 294,\n",
       " 'chang': 295,\n",
       " 'lollipop': 296,\n",
       " 'üç≠': 297,\n",
       " 'nez': 298,\n",
       " 'agnezmo': 299,\n",
       " 'oley': 300,\n",
       " 'mama': 301,\n",
       " 'stand': 302,\n",
       " 'stronger': 303,\n",
       " 'god': 304,\n",
       " 'misti': 305,\n",
       " 'babi': 306,\n",
       " 'cute': 307,\n",
       " 'woohoo': 308,\n",
       " \"can't\": 309,\n",
       " 'sign': 310,\n",
       " 'yet': 311,\n",
       " 'still': 312,\n",
       " 'think': 313,\n",
       " 'mka': 314,\n",
       " 'liam': 315,\n",
       " 'access': 316,\n",
       " 'welcom': 317,\n",
       " 'stat': 318,\n",
       " 'arriv': 319,\n",
       " '1': 320,\n",
       " 'unfollow': 321,\n",
       " 'via': 322,\n",
       " 'surpris': 323,\n",
       " 'figur': 324,\n",
       " 'happybirthdayemilybett': 325,\n",
       " 'sweet': 326,\n",
       " 'talent': 327,\n",
       " '2': 328,\n",
       " 'plan': 329,\n",
       " 'drain': 330,\n",
       " 'gotta': 331,\n",
       " 'timezon': 332,\n",
       " 'parent': 333,\n",
       " 'proud': 334,\n",
       " 'least': 335,\n",
       " 'mayb': 336,\n",
       " 'sometim': 337,\n",
       " 'grade': 338,\n",
       " 'al': 339,\n",
       " 'grand': 340,\n",
       " 'manila_bro': 341,\n",
       " 'chosen': 342,\n",
       " 'let': 343,\n",
       " 'around': 344,\n",
       " '..': 345,\n",
       " 'side': 346,\n",
       " 'world': 347,\n",
       " 'eh': 348,\n",
       " 'take': 349,\n",
       " 'care': 350,\n",
       " 'final': 351,\n",
       " 'fuck': 352,\n",
       " 'weekend': 353,\n",
       " 'real': 354,\n",
       " 'x45': 355,\n",
       " 'join': 356,\n",
       " 'hushedcallwithfraydo': 357,\n",
       " 'gift': 358,\n",
       " 'yeahhh': 359,\n",
       " 'hushedpinwithsammi': 360,\n",
       " 'event': 361,\n",
       " 'might': 362,\n",
       " 'luv': 363,\n",
       " 'realli': 364,\n",
       " 'appreci': 365,\n",
       " 'share': 366,\n",
       " 'wow': 367,\n",
       " 'tom': 368,\n",
       " 'gym': 369,\n",
       " 'monday': 370,\n",
       " 'invit': 371,\n",
       " 'scope': 372,\n",
       " 'friend': 373,\n",
       " 'nude': 374,\n",
       " 'sleep': 375,\n",
       " 'birthday': 376,\n",
       " 'want': 377,\n",
       " 't-shirt': 378,\n",
       " 'cool': 379,\n",
       " 'haw': 380,\n",
       " 'phela': 381,\n",
       " 'mom': 382,\n",
       " 'obvious': 383,\n",
       " 'princ': 384,\n",
       " 'charm': 385,\n",
       " 'stage': 386,\n",
       " 'luck': 387,\n",
       " 'tyler': 388,\n",
       " 'hipster': 389,\n",
       " 'glass': 390,\n",
       " 'marti': 391,\n",
       " 'glad': 392,\n",
       " 'done': 393,\n",
       " 'afternoon': 394,\n",
       " 'read': 395,\n",
       " 'kahfi': 396,\n",
       " 'finish': 397,\n",
       " 'ohmyg': 398,\n",
       " 'yaya': 399,\n",
       " 'dub': 400,\n",
       " 'stalk': 401,\n",
       " 'ig': 402,\n",
       " 'gondooo': 403,\n",
       " 'moo': 404,\n",
       " 'tologooo': 405,\n",
       " 'becom': 406,\n",
       " 'detail': 407,\n",
       " 'zzz': 408,\n",
       " 'xx': 409,\n",
       " 'physiotherapi': 410,\n",
       " 'hashtag': 411,\n",
       " 'üí™': 412,\n",
       " 'monica': 413,\n",
       " 'miss': 414,\n",
       " 'sound': 415,\n",
       " 'morn': 416,\n",
       " \"that'\": 417,\n",
       " 'x43': 418,\n",
       " 'definit': 419,\n",
       " 'tri': 420,\n",
       " 'tonight': 421,\n",
       " 'took': 422,\n",
       " 'advic': 423,\n",
       " 'treviso': 424,\n",
       " 'concert': 425,\n",
       " 'citi': 426,\n",
       " 'countri': 427,\n",
       " \"i'll\": 428,\n",
       " 'start': 429,\n",
       " 'fine': 430,\n",
       " 'gorgeou': 431,\n",
       " 'xo': 432,\n",
       " 'oven': 433,\n",
       " 'roast': 434,\n",
       " 'garlic': 435,\n",
       " 'oliv': 436,\n",
       " 'oil': 437,\n",
       " 'dri': 438,\n",
       " 'tomato': 439,\n",
       " 'basil': 440,\n",
       " 'centuri': 441,\n",
       " 'tuna': 442,\n",
       " 'right': 443,\n",
       " 'back': 444,\n",
       " 'atchya': 445,\n",
       " 'even': 446,\n",
       " 'almost': 447,\n",
       " 'chanc': 448,\n",
       " 'cheer': 449,\n",
       " 'po': 450,\n",
       " 'ice': 451,\n",
       " 'cream': 452,\n",
       " 'agre': 453,\n",
       " '100': 454,\n",
       " 'heheheh': 455,\n",
       " 'that': 456,\n",
       " 'point': 457,\n",
       " 'stay': 458,\n",
       " 'home': 459,\n",
       " 'soon': 460,\n",
       " 'promis': 461,\n",
       " 'web': 462,\n",
       " 'whatsapp': 463,\n",
       " 'volta': 464,\n",
       " 'funcionar': 465,\n",
       " 'com': 466,\n",
       " 'iphon': 467,\n",
       " 'jailbroken': 468,\n",
       " 'later': 469,\n",
       " '34': 470,\n",
       " 'min': 471,\n",
       " 'leia': 472,\n",
       " 'appear': 473,\n",
       " 'hologram': 474,\n",
       " 'r2d2': 475,\n",
       " 'w': 476,\n",
       " 'messag': 477,\n",
       " 'obi': 478,\n",
       " 'wan': 479,\n",
       " 'sit': 480,\n",
       " 'luke': 481,\n",
       " 'inter': 482,\n",
       " '3': 483,\n",
       " 'ucl': 484,\n",
       " 'arsen': 485,\n",
       " 'small': 486,\n",
       " 'team': 487,\n",
       " 'pass': 488,\n",
       " 'üöÇ': 489,\n",
       " 'dewsburi': 490,\n",
       " 'railway': 491,\n",
       " 'station': 492,\n",
       " 'dew': 493,\n",
       " 'west': 494,\n",
       " 'yorkshir': 495,\n",
       " '430': 496,\n",
       " 'smh': 497,\n",
       " '9:25': 498,\n",
       " 'live': 499,\n",
       " 'strang': 500,\n",
       " 'imagin': 501,\n",
       " 'megan': 502,\n",
       " 'masaantoday': 503,\n",
       " 'a4': 504,\n",
       " 'shweta': 505,\n",
       " 'tripathi': 506,\n",
       " '5': 507,\n",
       " '20': 508,\n",
       " 'kurta': 509,\n",
       " 'half': 510,\n",
       " 'number': 511,\n",
       " 'wsalelov': 512,\n",
       " 'ah': 513,\n",
       " 'larri': 514,\n",
       " 'anyway': 515,\n",
       " 'kinda': 516,\n",
       " 'goood': 517,\n",
       " 'life': 518,\n",
       " 'enn': 519,\n",
       " 'could': 520,\n",
       " 'warmup': 521,\n",
       " '15th': 522,\n",
       " 'bath': 523,\n",
       " 'dum': 524,\n",
       " 'andar': 525,\n",
       " 'ram': 526,\n",
       " 'sampath': 527,\n",
       " 'sona': 528,\n",
       " 'mohapatra': 529,\n",
       " 'samantha': 530,\n",
       " 'edward': 531,\n",
       " 'mein': 532,\n",
       " 'tulan': 533,\n",
       " 'razi': 534,\n",
       " 'wah': 535,\n",
       " 'josh': 536,\n",
       " 'alway': 537,\n",
       " 'smile': 538,\n",
       " 'pictur': 539,\n",
       " '16.20': 540,\n",
       " 'giveitup': 541,\n",
       " 'given': 542,\n",
       " 'ga': 543,\n",
       " 'subsidi': 544,\n",
       " 'initi': 545,\n",
       " 'propos': 546,\n",
       " 'delight': 547,\n",
       " 'yesterday': 548,\n",
       " 'x42': 549,\n",
       " 'lmaoo': 550,\n",
       " 'song': 551,\n",
       " 'ever': 552,\n",
       " 'shall': 553,\n",
       " 'littl': 554,\n",
       " 'throwback': 555,\n",
       " 'outli': 556,\n",
       " 'island': 557,\n",
       " 'cheung': 558,\n",
       " 'chau': 559,\n",
       " 'mui': 560,\n",
       " 'wo': 561,\n",
       " 'total': 562,\n",
       " 'differ': 563,\n",
       " 'kfckitchentour': 564,\n",
       " 'kitchen': 565,\n",
       " 'clean': 566,\n",
       " \"i'm\": 567,\n",
       " 'cusp': 568,\n",
       " 'test': 569,\n",
       " 'water': 570,\n",
       " 'reward': 571,\n",
       " 'arummzz': 572,\n",
       " \"let'\": 573,\n",
       " 'drive': 574,\n",
       " 'travel': 575,\n",
       " 'yogyakarta': 576,\n",
       " 'jeep': 577,\n",
       " 'indonesia': 578,\n",
       " 'instamood': 579,\n",
       " 'wanna': 580,\n",
       " 'skype': 581,\n",
       " 'may': 582,\n",
       " 'nice': 583,\n",
       " 'friendli': 584,\n",
       " 'pretend': 585,\n",
       " 'film': 586,\n",
       " 'congratul': 587,\n",
       " 'winner': 588,\n",
       " 'cheesydelight': 589,\n",
       " 'contest': 590,\n",
       " 'address': 591,\n",
       " 'guy': 592,\n",
       " 'market': 593,\n",
       " '24/7': 594,\n",
       " '14': 595,\n",
       " 'hour': 596,\n",
       " 'leav': 597,\n",
       " 'without': 598,\n",
       " 'delay': 599,\n",
       " 'actual': 600,\n",
       " 'easi': 601,\n",
       " 'guess': 602,\n",
       " 'train': 603,\n",
       " 'wd': 604,\n",
       " 'shift': 605,\n",
       " 'engin': 606,\n",
       " 'etc': 607,\n",
       " 'sunburn': 608,\n",
       " 'peel': 609,\n",
       " 'blog': 610,\n",
       " 'huge': 611,\n",
       " 'warm': 612,\n",
       " '‚òÜ': 613,\n",
       " 'complet': 614,\n",
       " 'triangl': 615,\n",
       " 'northern': 616,\n",
       " 'ireland': 617,\n",
       " 'sight': 618,\n",
       " 'smthng': 619,\n",
       " 'fr': 620,\n",
       " 'hug': 621,\n",
       " 'xoxo': 622,\n",
       " 'uu': 623,\n",
       " 'jaann': 624,\n",
       " 'topnewfollow': 625,\n",
       " 'connect': 626,\n",
       " 'wonder': 627,\n",
       " 'made': 628,\n",
       " 'fluffi': 629,\n",
       " 'insid': 630,\n",
       " 'pirouett': 631,\n",
       " 'moos': 632,\n",
       " 'trip': 633,\n",
       " 'philli': 634,\n",
       " 'decemb': 635,\n",
       " \"i'd\": 636,\n",
       " 'dude': 637,\n",
       " 'x41': 638,\n",
       " 'question': 639,\n",
       " 'flaw': 640,\n",
       " 'pain': 641,\n",
       " 'negat': 642,\n",
       " 'strength': 643,\n",
       " 'went': 644,\n",
       " 'solo': 645,\n",
       " 'move': 646,\n",
       " 'fav': 647,\n",
       " 'nirvana': 648,\n",
       " 'smell': 649,\n",
       " 'teen': 650,\n",
       " 'spirit': 651,\n",
       " 'rip': 652,\n",
       " 'ami': 653,\n",
       " 'winehous': 654,\n",
       " 'coupl': 655,\n",
       " 'tomhiddleston': 656,\n",
       " 'elizabetholsen': 657,\n",
       " 'yaytheylookgreat': 658,\n",
       " 'goodnight': 659,\n",
       " 'vid': 660,\n",
       " 'wake': 661,\n",
       " 'gonna': 662,\n",
       " 'shoot': 663,\n",
       " 'itti': 664,\n",
       " 'bitti': 665,\n",
       " 'teeni': 666,\n",
       " 'bikini': 667,\n",
       " 'much': 668,\n",
       " '4th': 669,\n",
       " 'togeth': 670,\n",
       " 'end': 671,\n",
       " 'xfile': 672,\n",
       " 'content': 673,\n",
       " 'rain': 674,\n",
       " 'fabul': 675,\n",
       " 'fantast': 676,\n",
       " '‚ô°': 677,\n",
       " 'jb': 678,\n",
       " 'forev': 679,\n",
       " 'belieb': 680,\n",
       " 'nighti': 681,\n",
       " 'bug': 682,\n",
       " 'bite': 683,\n",
       " 'bracelet': 684,\n",
       " 'idea': 685,\n",
       " 'foundri': 686,\n",
       " 'game': 687,\n",
       " 'sens': 688,\n",
       " 'pic': 689,\n",
       " 'ef': 690,\n",
       " 'phone': 691,\n",
       " 'woot': 692,\n",
       " 'derek': 693,\n",
       " 'use': 694,\n",
       " 'parkshar': 695,\n",
       " 'gloucestershir': 696,\n",
       " 'aaaahhh': 697,\n",
       " 'man': 698,\n",
       " 'traffic': 699,\n",
       " 'stress': 700,\n",
       " 'reliev': 701,\n",
       " \"how'r\": 702,\n",
       " 'arbeloa': 703,\n",
       " 'turn': 704,\n",
       " '17': 705,\n",
       " 'omg': 706,\n",
       " 'say': 707,\n",
       " 'europ': 708,\n",
       " 'rise': 709,\n",
       " 'find': 710,\n",
       " 'hard': 711,\n",
       " 'believ': 712,\n",
       " 'uncount': 713,\n",
       " 'coz': 714,\n",
       " 'unlimit': 715,\n",
       " 'cours': 716,\n",
       " 'teamposit': 717,\n",
       " 'aldub': 718,\n",
       " '‚òï': 719,\n",
       " 'rita': 720,\n",
       " 'info': 721,\n",
       " \"we'd\": 722,\n",
       " 'way': 723,\n",
       " 'boy': 724,\n",
       " 'x40': 725,\n",
       " 'true': 726,\n",
       " 'sethi': 727,\n",
       " 'high': 728,\n",
       " 'exe': 729,\n",
       " 'skeem': 730,\n",
       " 'saam': 731,\n",
       " 'peopl': 732,\n",
       " 'polit': 733,\n",
       " 'izzat': 734,\n",
       " 'wese': 735,\n",
       " 'trust': 736,\n",
       " 'khawateen': 737,\n",
       " 'k': 738,\n",
       " 'sath': 739,\n",
       " 'mana': 740,\n",
       " 'kar': 741,\n",
       " 'deya': 742,\n",
       " 'sort': 743,\n",
       " 'smart': 744,\n",
       " 'hair': 745,\n",
       " 'tbh': 746,\n",
       " 'jacob': 747,\n",
       " 'g': 748,\n",
       " 'upgrad': 749,\n",
       " 'tee': 750,\n",
       " 'famili': 751,\n",
       " 'person': 752,\n",
       " 'two': 753,\n",
       " 'convers': 754,\n",
       " 'onlin': 755,\n",
       " 'mclaren': 756,\n",
       " 'fridayfeel': 757,\n",
       " 'tgif': 758,\n",
       " 'squar': 759,\n",
       " 'enix': 760,\n",
       " 'bissmillah': 761,\n",
       " 'ya': 762,\n",
       " 'allah': 763,\n",
       " \"we'r\": 764,\n",
       " 'socent': 765,\n",
       " 'startup': 766,\n",
       " 'drop': 767,\n",
       " 'your': 768,\n",
       " 'arnd': 769,\n",
       " 'town': 770,\n",
       " 'basic': 771,\n",
       " 'piss': 772,\n",
       " 'cup': 773,\n",
       " 'also': 774,\n",
       " 'terribl': 775,\n",
       " 'complic': 776,\n",
       " 'discuss': 777,\n",
       " 'snapchat': 778,\n",
       " 'lynettelow': 779,\n",
       " 'kikmenow': 780,\n",
       " 'snapm': 781,\n",
       " 'hot': 782,\n",
       " 'amazon': 783,\n",
       " 'kikmeguy': 784,\n",
       " 'defin': 785,\n",
       " 'grow': 786,\n",
       " 'sport': 787,\n",
       " 'rt': 788,\n",
       " 'rakyat': 789,\n",
       " 'write': 790,\n",
       " 'sinc': 791,\n",
       " 'mention': 792,\n",
       " 'fli': 793,\n",
       " 'fish': 794,\n",
       " 'promot': 795,\n",
       " 'post': 796,\n",
       " 'cyber': 797,\n",
       " 'ourdaughtersourprid': 798,\n",
       " 'mypapamyprid': 799,\n",
       " 'papa': 800,\n",
       " 'coach': 801,\n",
       " 'posit': 802,\n",
       " 'kha': 803,\n",
       " 'atleast': 804,\n",
       " 'x39': 805,\n",
       " 'mango': 806,\n",
       " \"lassi'\": 807,\n",
       " \"monty'\": 808,\n",
       " 'marvel': 809,\n",
       " 'though': 810,\n",
       " 'suspect': 811,\n",
       " 'meant': 812,\n",
       " '24': 813,\n",
       " 'hr': 814,\n",
       " 'touch': 815,\n",
       " 'kepler': 816,\n",
       " '452b': 817,\n",
       " 'chalna': 818,\n",
       " 'hai': 819,\n",
       " 'thankyou': 820,\n",
       " 'hazel': 821,\n",
       " 'food': 822,\n",
       " 'brooklyn': 823,\n",
       " 'pta': 824,\n",
       " 'awak': 825,\n",
       " 'okayi': 826,\n",
       " 'awww': 827,\n",
       " 'ha': 828,\n",
       " 'doc': 829,\n",
       " 'splendid': 830,\n",
       " 'spam': 831,\n",
       " 'folder': 832,\n",
       " 'amount': 833,\n",
       " 'nigeria': 834,\n",
       " 'claim': 835,\n",
       " 'rted': 836,\n",
       " 'leg': 837,\n",
       " 'hurt': 838,\n",
       " 'bad': 839,\n",
       " 'mine': 840,\n",
       " 'saturday': 841,\n",
       " 'thaaank': 842,\n",
       " 'puhon': 843,\n",
       " 'happinesss': 844,\n",
       " 'tnc': 845,\n",
       " 'prior': 846,\n",
       " 'notif': 847,\n",
       " 'fat': 848,\n",
       " 'co': 849,\n",
       " 'probabl': 850,\n",
       " 'ate': 851,\n",
       " 'yuna': 852,\n",
       " 'tamesid': 853,\n",
       " '¬¥': 854,\n",
       " 'googl': 855,\n",
       " 'account': 856,\n",
       " 'scouser': 857,\n",
       " 'everyth': 858,\n",
       " 'zoe': 859,\n",
       " 'mate': 860,\n",
       " 'liter': 861,\n",
       " \"they'r\": 862,\n",
       " 'samee': 863,\n",
       " 'edgar': 864,\n",
       " 'updat': 865,\n",
       " 'log': 866,\n",
       " 'bring': 867,\n",
       " 'abe': 868,\n",
       " 'meet': 869,\n",
       " 'x38': 870,\n",
       " 'sigh': 871,\n",
       " 'dreamili': 872,\n",
       " 'pout': 873,\n",
       " 'eye': 874,\n",
       " 'quacketyquack': 875,\n",
       " 'funni': 876,\n",
       " 'happen': 877,\n",
       " 'phil': 878,\n",
       " 'em': 879,\n",
       " 'del': 880,\n",
       " 'rodder': 881,\n",
       " 'els': 882,\n",
       " 'play': 883,\n",
       " 'newest': 884,\n",
       " 'gamejam': 885,\n",
       " 'irish': 886,\n",
       " 'literatur': 887,\n",
       " 'inaccess': 888,\n",
       " \"kareena'\": 889,\n",
       " 'fan': 890,\n",
       " 'brain': 891,\n",
       " 'dot': 892,\n",
       " 'braindot': 893,\n",
       " 'fair': 894,\n",
       " 'rush': 895,\n",
       " 'either': 896,\n",
       " 'brandi': 897,\n",
       " '18': 898,\n",
       " 'carniv': 899,\n",
       " 'men': 900,\n",
       " 'put': 901,\n",
       " 'mask': 902,\n",
       " 'xavier': 903,\n",
       " 'forneret': 904,\n",
       " 'jennif': 905,\n",
       " 'site': 906,\n",
       " 'free': 907,\n",
       " '50.000': 908,\n",
       " '8': 909,\n",
       " 'ball': 910,\n",
       " 'pool': 911,\n",
       " 'coin': 912,\n",
       " 'edit': 913,\n",
       " 'trish': 914,\n",
       " '‚ô•': 915,\n",
       " 'grate': 916,\n",
       " 'three': 917,\n",
       " 'comment': 918,\n",
       " 'wakeup': 919,\n",
       " 'besid': 920,\n",
       " 'dirti': 921,\n",
       " 'sex': 922,\n",
       " 'lmaooo': 923,\n",
       " 'üò§': 924,\n",
       " 'loui': 925,\n",
       " \"he'\": 926,\n",
       " 'throw': 927,\n",
       " 'caus': 928,\n",
       " 'inspir': 929,\n",
       " 'ff': 930,\n",
       " 'twoof': 931,\n",
       " 'gr8': 932,\n",
       " 'wkend': 933,\n",
       " 'kind': 934,\n",
       " 'exhaust': 935,\n",
       " 'word': 936,\n",
       " 'cheltenham': 937,\n",
       " 'area': 938,\n",
       " 'kale': 939,\n",
       " 'crisp': 940,\n",
       " 'ruin': 941,\n",
       " 'x37': 942,\n",
       " 'open': 943,\n",
       " 'worldwid': 944,\n",
       " 'outta': 945,\n",
       " 'sfvbeta': 946,\n",
       " 'vantast': 947,\n",
       " 'xcylin': 948,\n",
       " 'bundl': 949,\n",
       " 'show': 950,\n",
       " 'internet': 951,\n",
       " 'price': 952,\n",
       " 'realisticli': 953,\n",
       " 'pay': 954,\n",
       " 'net': 955,\n",
       " 'educ': 956,\n",
       " 'power': 957,\n",
       " 'weapon': 958,\n",
       " 'nelson': 959,\n",
       " 'mandela': 960,\n",
       " 'recent': 961,\n",
       " 'j': 962,\n",
       " 'chenab': 963,\n",
       " 'flow': 964,\n",
       " 'pakistan': 965,\n",
       " 'incredibleindia': 966,\n",
       " 'teenchoic': 967,\n",
       " 'choiceinternationalartist': 968,\n",
       " 'superjunior': 969,\n",
       " 'caught': 970,\n",
       " 'first': 971,\n",
       " 'salmon': 972,\n",
       " 'super-blend': 973,\n",
       " 'project': 974,\n",
       " 'youth@bipolaruk.org.uk': 975,\n",
       " 'awesom': 976,\n",
       " 'stream': 977,\n",
       " 'alma': 978,\n",
       " 'mater': 979,\n",
       " 'highschoolday': 980,\n",
       " 'clientvisit': 981,\n",
       " 'faith': 982,\n",
       " 'christian': 983,\n",
       " 'school': 984,\n",
       " 'lizaminnelli': 985,\n",
       " 'upcom': 986,\n",
       " 'uk': 987,\n",
       " 'üòÑ': 988,\n",
       " 'singl': 989,\n",
       " 'hill': 990,\n",
       " 'everi': 991,\n",
       " 'beat': 992,\n",
       " 'wrong': 993,\n",
       " 'readi': 994,\n",
       " 'natur': 995,\n",
       " 'pefumeri': 996,\n",
       " 'workshop': 997,\n",
       " 'neal': 998,\n",
       " 'yard': 999,\n",
       " ...}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Vocab = {'__PAD__': 0, '__</e>__': 1, '__UNK__': 2} \n",
    "\n",
    "for tweet in train_x: \n",
    "    processed_tweet = process_tweet(tweet)\n",
    "    for word in processed_tweet:\n",
    "        if word not in Vocab: \n",
    "            Vocab[word] = len(Vocab)\n",
    "    \n",
    "print(\"Total words in vocab are\",len(Vocab))\n",
    "display(Vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2.3. Converting a tweet to a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tweet_to_tensor(tweet, vocab_dict, unk_token='__UNK__', verbose=False):\n",
    "    word_l = process_tweet(tweet)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"List of words from the processed tweet:\")\n",
    "        print(word_l)\n",
    "        \n",
    "    tensor_l = []\n",
    "    \n",
    "    unk_ID = vocab_dict[unk_token]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"The unique integer ID for the unk_token is {unk_ID}\")\n",
    "        \n",
    "    for word in word_l:\n",
    "        word_ID = vocab_dict[word] if word in vocab_dict else unk_ID\n",
    "        \n",
    "        tensor_l.append(word_ID) \n",
    "    \n",
    "    return tensor_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual tweet is\n",
      " Bro:U wan cut hair anot,ur hair long Liao bo\n",
      "Me:since ord liao,take it easy lor treat as save $ leave it longer :)\n",
      "Bro:LOL Sibei xialan\n",
      "\n",
      "Tensor of tweet:\n",
      " [1065, 136, 479, 2351, 745, 8148, 1123, 745, 53, 2, 2672, 791, 2, 2, 349, 601, 2, 3489, 1017, 597, 4559, 9, 1065, 157, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "print(\"Actual tweet is\\n\", val_pos[0])\n",
    "print(\"\\nTensor of tweet:\\n\", tweet_to_tensor(val_pos[0], vocab_dict=Vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# test tweet_to_tensor\n",
    "\n",
    "def test_tweet_to_tensor():\n",
    "    test_cases = [\n",
    "        \n",
    "        {\n",
    "            \"name\":\"simple_test_check\",\n",
    "            \"input\": [val_pos[1], Vocab],\n",
    "            \"expected\":[444, 2, 304, 567, 56, 9],\n",
    "            \"error\":\"The function gives bad output for val_pos[1]. Test failed\"\n",
    "        },\n",
    "        {\n",
    "            \"name\":\"datatype_check\",\n",
    "            \"input\":[val_pos[1], Vocab],\n",
    "            \"expected\":type([]),\n",
    "            \"error\":\"Datatype mismatch. Need only list not np.array\"\n",
    "        },\n",
    "        {\n",
    "            \"name\":\"without_unk_check\",\n",
    "            \"input\":[val_pos[1], Vocab],\n",
    "            \"expected\":6,\n",
    "            \"error\":\"Unk word check not done- Please check if you included mapping for unknown word\"\n",
    "        }\n",
    "    ]\n",
    "    count = 0\n",
    "    for test_case in test_cases:\n",
    "        \n",
    "        try:\n",
    "            if test_case['name'] == \"simple_test_check\":\n",
    "                assert test_case[\"expected\"] == tweet_to_tensor(*test_case['input'])\n",
    "                count += 1\n",
    "            if test_case['name'] == \"datatype_check\":\n",
    "                assert isinstance(tweet_to_tensor(*test_case['input']), test_case[\"expected\"])\n",
    "                count += 1\n",
    "            if test_case['name'] == \"without_unk_check\":\n",
    "                assert None not in tweet_to_tensor(*test_case['input'])\n",
    "                count += 1\n",
    "                \n",
    "            \n",
    "            \n",
    "        except:\n",
    "            print(test_case['error'])\n",
    "    if count == 3:\n",
    "        print(\"\\033[92m All tests passed\")\n",
    "    else:\n",
    "        print(count,\" Tests passed out of 3\")\n",
    "test_tweet_to_tensor()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2.4. Creating a Batch Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_generator(data_pos, data_neg, batch_size, loop, vocab_dict, shuffle=False):\n",
    "\n",
    "    assert batch_size % 2 == 0\n",
    "\n",
    "    n_to_take = batch_size // 2\n",
    "\n",
    "    pos_index = 0\n",
    "    neg_index = 0\n",
    "    \n",
    "    len_data_pos = len(data_pos)\n",
    "    len_data_neg = len(data_neg)\n",
    "    \n",
    "    pos_index_lines = list(range(len_data_pos))\n",
    "    neg_index_lines = list(range(len_data_neg))\n",
    "    \n",
    "    if shuffle:\n",
    "        rnd.shuffle(pos_index_lines)\n",
    "        rnd.shuffle(neg_index_lines)\n",
    "        \n",
    "    stop = False\n",
    "    \n",
    "    while not stop:  \n",
    "        \n",
    "        batch = []\n",
    "\n",
    "        for i in range(n_to_take):\n",
    "                    \n",
    "            if pos_index >= len_data_pos: \n",
    "                \n",
    "                if not loop:\n",
    "                    stop = True;\n",
    "                    break;\n",
    "                \n",
    "                pos_index = 0\n",
    "                \n",
    "                if shuffle:\n",
    "                    rnd.shuffle(pos_index_lines)\n",
    "                    \n",
    "            tweet = data_pos[pos_index_lines[pos_index]]\n",
    "            \n",
    "            tensor = tweet_to_tensor(tweet, vocab_dict)\n",
    "            \n",
    "            batch.append(tensor)\n",
    "            \n",
    "            pos_index = pos_index + 1\n",
    "\n",
    "        for i in range(n_to_take):            \n",
    "            if neg_index >= len_data_neg:                \n",
    "                if not loop:\n",
    "                    stop = True;\n",
    "                    break;                    \n",
    "                neg_index = 0\n",
    "                \n",
    "                if shuffle:\n",
    "                    rnd.shuffle(neg_index_lines)\n",
    "            tweet = data_neg[neg_index_lines[neg_index]]            \n",
    "            tensor = tweet_to_tensor(tweet, vocab_dict)\n",
    "            \n",
    "            batch.append(tensor)\n",
    "            \n",
    "            neg_index += 1\n",
    "\n",
    "        if stop:\n",
    "            break;\n",
    "\n",
    "        pos_index += n_to_take\n",
    "\n",
    "        neg_index += n_to_take\n",
    "\n",
    "        max_len = max([len(t) for t in batch]) \n",
    "\n",
    "        tensor_pad_l = []\n",
    "        for tensor in batch:\n",
    "\n",
    "            n_pad = max_len - len(tensor)\n",
    "            \n",
    "            pad_l = [0]*n_pad\n",
    "            \n",
    "            tensor_pad = tensor + pad_l\n",
    "            \n",
    "            tensor_pad_l.append(tensor_pad)\n",
    "\n",
    "        inputs = np.array(tensor_pad_l)\n",
    "\n",
    "        target_pos = [1]*n_to_take\n",
    "        target_neg = [0]*n_to_take\n",
    "        \n",
    "        target_l = target_pos + target_neg\n",
    "        targets = np.array(target_l)\n",
    "\n",
    "        example_weights = np.ones_like(targets)\n",
    "\n",
    "        yield inputs, targets, example_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: [[2005 4451 3201    9    0    0    0    0    0    0    0]\n",
      " [4954  567 2000 1454 5174 3499  141 3499  130  459    9]\n",
      " [3761  109  136  583 2930 3969    0    0    0    0    0]\n",
      " [ 250 3761    0    0    0    0    0    0    0    0    0]]\n",
      "Targets: [1 1 0 0]\n",
      "Example Weights: [1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Set the random number generator for the shuffle procedure\n",
    "rnd.seed(30) \n",
    "\n",
    "def train_generator(batch_size, shuffle = False):\n",
    "    return data_generator(train_pos, train_neg, batch_size, True, Vocab, shuffle)\n",
    "\n",
    "def val_generator(batch_size, shuffle = False):\n",
    "    return data_generator(val_pos, val_neg, batch_size, True, Vocab, shuffle)\n",
    "\n",
    "def test_generator(batch_size, shuffle = False):\n",
    "    return data_generator(val_pos, val_neg, batch_size, False, Vocab, shuffle)\n",
    "\n",
    "inputs, targets, example_weights = next(train_generator(4, shuffle=True))\n",
    "\n",
    "print(f'Inputs: {inputs}')\n",
    "print(f'Targets: {targets}')\n",
    "print(f'Example Weights: {example_weights}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The inputs shape is (4, 14)\n",
      "The targets shape is (4,)\n",
      "The example weights shape is (4,)\n",
      "input tensor: [3 4 5 6 7 8 9 0 0 0 0 0 0 0]; target 1; example weights 1\n",
      "input tensor: [10 11 12 13 14 15 16 17 18 19 20  9 21 22]; target 1; example weights 1\n",
      "input tensor: [5738 2901 3761    0    0    0    0    0    0    0    0    0    0    0]; target 0; example weights 1\n",
      "input tensor: [ 858  256 3652 5739  307 4458  567 1230 2767  328 1202 3761    0    0]; target 0; example weights 1\n"
     ]
    }
   ],
   "source": [
    "# Test the train_generator\n",
    "\n",
    "tmp_data_gen = train_generator(batch_size = 4)\n",
    "\n",
    "tmp_inputs, tmp_targets, tmp_example_weights = next(tmp_data_gen)\n",
    "\n",
    "print(f\"The inputs shape is {tmp_inputs.shape}\")\n",
    "print(f\"The targets shape is {tmp_targets.shape}\")\n",
    "print(f\"The example weights shape is {tmp_example_weights.shape}\")\n",
    "\n",
    "for i,t in enumerate(tmp_inputs):\n",
    "    print(f\"input tensor: {t}; target {tmp_targets[i]}; example weights {tmp_example_weights[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3. Defining Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3.1. Relu Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class Relu(Layer):\n",
    "    def forward(self, x):\n",
    "        \n",
    "        activation = np.maximum(x,0)\n",
    "        \n",
    "        return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data is:\n",
      "[[-2. -1.  0.]\n",
      " [ 0.  1.  2.]]\n",
      "Output of Relu is:\n",
      "[[0. 0. 0.]\n",
      " [0. 1. 2.]]\n"
     ]
    }
   ],
   "source": [
    "# Test your relu function\n",
    "x = np.array([[-2.0, -1.0, 0.0], [0.0, 1.0, 2.0]], dtype=float)\n",
    "relu_layer = Relu()\n",
    "print(\"Test data is:\")\n",
    "print(x)\n",
    "print(\"Output of Relu is:\")\n",
    "print(relu_layer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3.2. Dense Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from trax import fastmath\n",
    "\n",
    "np = fastmath.numpy\n",
    "random = fastmath.random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The random seed generated by random.get_prng\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Buffer([0, 1], dtype=uint32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "choose a matrix with 2 rows and 3 columns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight matrix generated with a normal distribution with mean 0 and stdev of 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Buffer([[ 0.957307  , -0.9699291 ,  1.0070664 ],\n",
       "        [ 0.36619025,  0.17294823,  0.29092228]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# See how the fastmath.trax.random.normal function works\n",
    "tmp_key = random.get_prng(seed=1)\n",
    "print(\"The random seed generated by random.get_prng\")\n",
    "display(tmp_key)\n",
    "\n",
    "print(\"choose a matrix with 2 rows and 3 columns\")\n",
    "tmp_shape=(2,3)\n",
    "display(tmp_shape)\n",
    "\n",
    "tmp_weight = trax.fastmath.random.normal(key=tmp_key, shape=tmp_shape)\n",
    "\n",
    "print(\"Weight matrix generated with a normal distribution with mean 0 and stdev of 1\")\n",
    "display(tmp_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "\n",
    "    def __init__(self, n_units, init_stdev=0.1):        \n",
    "        self._n_units = n_units\n",
    "        self._init_stdev = 0.1\n",
    "\n",
    "    def forward(self, x):\n",
    "        dense = np.dot(x, self.weights) \n",
    "        return dense\n",
    "\n",
    "    # init_weights\n",
    "    def init_weights_and_state(self, input_signature, random_key):\n",
    "        input_shape = input_signature.shape      \n",
    "        w = self._init_stdev * random.normal(\n",
    "            key = random_key, shape = (input_shape[-1], self._n_units))\n",
    "           \n",
    "        self.weights = w\n",
    "        return self.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights are\n",
      "  [[-0.02837108  0.09368161 -0.10050074  0.14165013  0.105433    0.09108126\n",
      "  -0.04265672  0.0986188  -0.05575325  0.00153249]\n",
      " [-0.20785688  0.0554837   0.09142365  0.05744595  0.07227863  0.01210617\n",
      "  -0.03237354  0.16234994  0.02450038 -0.13809782]\n",
      " [-0.06111238  0.01403724  0.08410042 -0.1094358  -0.10775021 -0.11396459\n",
      "  -0.05933381 -0.01557652 -0.03832145 -0.11144515]]\n",
      "Foward function output is  [[-3.0395498   0.92668015  2.5414746  -2.050473   -1.976939   -2.582209\n",
      "  -1.7952734   0.9442741  -0.8980402  -3.7497485 ]]\n"
     ]
    }
   ],
   "source": [
    "# Testing your Dense layer \n",
    "dense_layer = Dense(n_units=10)  \n",
    "random_key = random.get_prng(seed=0)\n",
    "z = np.array([[2.0, 7.0, 25.0]])\n",
    "\n",
    "dense_layer.init(z, random_key)\n",
    "print(\"Weights are\\n \",dense_layer.weights)\n",
    "print(\"Foward function output is \", dense_layer(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3.3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding_3_2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tmp_embed = tl.Embedding(vocab_size=3, d_feature=2)\n",
    "display(tmp_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean along axis 0 creates a vector whose length equals the vocabulary size\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([2.5, 3.5, 4.5], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean along axis 1 creates a vector whose length equals the number of elements in a word embedding\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([2., 5.], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tmp_embed = np.array([[1,2,3,],\n",
    "                    [4,5,6]\n",
    "                   ])\n",
    "\n",
    "print(\"The mean along axis 0 creates a vector whose length equals the vocabulary size\")\n",
    "display(np.mean(tmp_embed,axis=0))\n",
    "\n",
    "print(\"The mean along axis 1 creates a vector whose length equals the number of elements in a word embedding\")\n",
    "display(np.mean(tmp_embed,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classifier(vocab_size=len(Vocab), embedding_dim=256, output_dim=2, mode='train'):\n",
    "\n",
    "    embed_layer = tl.Embedding(\n",
    "        vocab_size=vocab_size, \n",
    "        d_feature=embedding_dim)\n",
    "    \n",
    "    mean_layer = tl.Mean(axis=1)\n",
    "    \n",
    "    dense_output_layer = tl.Dense(n_units = output_dim)\n",
    "\n",
    "    \n",
    "    log_softmax_layer = tl.LogSoftmax()\n",
    "\n",
    "    model = tl.Serial(\n",
    "      embed_layer,\n",
    "      mean_layer,\n",
    "      dense_output_layer,\n",
    "      log_softmax_layer\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp_model = classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'trax.layers.combinators.Serial'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Serial[\n",
       "  Embedding_9088_256\n",
       "  Mean\n",
       "  Dense_2\n",
       "  LogSoftmax\n",
       "]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(type(tmp_model))\n",
    "display(tmp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 4.1. Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from trax.supervised import training\n",
    "\n",
    "batch_size = 16\n",
    "rnd.seed(271)\n",
    "\n",
    "train_task = training.TrainTask(\n",
    "    labeled_data=train_generator(batch_size=batch_size, shuffle=True),\n",
    "    loss_layer=tl.CrossEntropyLoss(),\n",
    "    optimizer=trax.optimizers.Adam(0.01),\n",
    "    n_steps_per_checkpoint=10,\n",
    ")\n",
    "\n",
    "eval_task = training.EvalTask(\n",
    "    labeled_data=val_generator(batch_size=batch_size, shuffle=True),\n",
    "    metrics=[tl.CrossEntropyLoss(), tl.Accuracy()],\n",
    ")\n",
    "\n",
    "model = classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/diegobatalhacunhadasilva/model/\n"
     ]
    }
   ],
   "source": [
    "output_dir = '~/model/'\n",
    "output_dir_expand = os.path.expanduser(output_dir)\n",
    "print(output_dir_expand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(classifier, train_task, eval_task, n_steps, output_dir):\n",
    "\n",
    "    training_loop = training.Loop(\n",
    "                                classifier,\n",
    "                                train_task,\n",
    "                                eval_tasks = eval_task,\n",
    "                                output_dir = output_dir)\n",
    "\n",
    "    training_loop.run(n_steps = n_steps)\n",
    "\n",
    "    return training_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step    310: Ran 10 train steps in 12.07 secs\n",
      "Step    310: train CrossEntropyLoss |  0.00130165\n",
      "Step    310: eval  CrossEntropyLoss |  0.00011680\n",
      "Step    310: eval          Accuracy |  1.00000000\n",
      "\n",
      "Step    320: Ran 10 train steps in 6.48 secs\n",
      "Step    320: train CrossEntropyLoss |  0.00039270\n",
      "Step    320: eval  CrossEntropyLoss |  0.01050072\n",
      "Step    320: eval          Accuracy |  1.00000000\n",
      "\n",
      "Step    330: Ran 10 train steps in 4.30 secs\n",
      "Step    330: train CrossEntropyLoss |  0.00074696\n",
      "Step    330: eval  CrossEntropyLoss |  0.00194571\n",
      "Step    330: eval          Accuracy |  1.00000000\n",
      "\n",
      "Step    340: Ran 10 train steps in 2.56 secs\n",
      "Step    340: train CrossEntropyLoss |  0.01558403\n",
      "Step    340: eval  CrossEntropyLoss |  0.00391711\n",
      "Step    340: eval          Accuracy |  1.00000000\n",
      "\n",
      "Step    350: Ran 10 train steps in 1.20 secs\n",
      "Step    350: train CrossEntropyLoss |  0.00877521\n",
      "Step    350: eval  CrossEntropyLoss |  0.01291416\n",
      "Step    350: eval          Accuracy |  1.00000000\n",
      "\n",
      "Step    360: Ran 10 train steps in 2.46 secs\n",
      "Step    360: train CrossEntropyLoss |  0.01898986\n",
      "Step    360: eval  CrossEntropyLoss |  0.08106923\n",
      "Step    360: eval          Accuracy |  0.93750000\n",
      "\n",
      "Step    370: Ran 10 train steps in 3.54 secs\n",
      "Step    370: train CrossEntropyLoss |  0.02607264\n",
      "Step    370: eval  CrossEntropyLoss |  0.00029345\n",
      "Step    370: eval          Accuracy |  1.00000000\n",
      "\n",
      "Step    380: Ran 10 train steps in 1.40 secs\n",
      "Step    380: train CrossEntropyLoss |  0.00241261\n",
      "Step    380: eval  CrossEntropyLoss |  0.00029743\n",
      "Step    380: eval          Accuracy |  1.00000000\n",
      "\n",
      "Step    390: Ran 10 train steps in 1.29 secs\n",
      "Step    390: train CrossEntropyLoss |  0.00033606\n",
      "Step    390: eval  CrossEntropyLoss |  0.00005293\n",
      "Step    390: eval          Accuracy |  1.00000000\n",
      "\n",
      "Step    400: Ran 10 train steps in 1.33 secs\n",
      "Step    400: train CrossEntropyLoss |  0.00072635\n",
      "Step    400: eval  CrossEntropyLoss |  0.00005402\n",
      "Step    400: eval          Accuracy |  1.00000000\n"
     ]
    }
   ],
   "source": [
    "training_loop = train_model(model, train_task, eval_task, 100, output_dir_expand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 4.2. Practice Making a Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch is a tuple of length 3 because position 0 contains the tweets, and position 1 contains the targets.\n",
      "The shape of the tweet tensors is (16, 15) (num of examples, length of tweet tensors)\n",
      "The shape of the labels is (16,), which is the batch size.\n",
      "The shape of the example_weights is (16,), which is the same as inputs/targets size.\n"
     ]
    }
   ],
   "source": [
    "tmp_train_generator = train_generator(16)\n",
    "\n",
    "tmp_batch = next(tmp_train_generator)\n",
    "\n",
    "tmp_inputs, tmp_targets, tmp_example_weights = tmp_batch\n",
    "\n",
    "print(f\"The batch is a tuple of length {len(tmp_batch)} because position 0 contains the tweets, and position 1 contains the targets.\") \n",
    "print(f\"The shape of the tweet tensors is {tmp_inputs.shape} (num of examples, length of tweet tensors)\")\n",
    "print(f\"The shape of the labels is {tmp_targets.shape}, which is the batch size.\")\n",
    "print(f\"The shape of the example_weights is {tmp_example_weights.shape}, which is the same as inputs/targets size.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction shape is (16, 2), num of tensor_tweets as rows\n",
      "Column 0 is the probability of a negative sentiment (class 0)\n",
      "Column 1 is the probability of a positive sentiment (class 1)\n",
      "\n",
      "View the prediction array\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([[-1.0037046e+01, -4.3869019e-05],\n",
       "             [-7.4305410e+00, -5.9294701e-04],\n",
       "             [-1.0318024e+01, -3.2901764e-05],\n",
       "             [-1.0070904e+01, -4.2438507e-05],\n",
       "             [-1.0103464e+01, -4.1007996e-05],\n",
       "             [-9.8432789e+00, -5.2928925e-05],\n",
       "             [-1.2162621e+01, -5.2452087e-06],\n",
       "             [-5.3617773e+00, -4.7037601e-03],\n",
       "             [-9.5367432e-07, -1.3715170e+01],\n",
       "             [ 0.0000000e+00, -1.5701397e+01],\n",
       "             [ 0.0000000e+00, -1.5249322e+01],\n",
       "             [ 0.0000000e+00, -2.6646961e+01],\n",
       "             [-3.7455559e-04, -7.8900232e+00],\n",
       "             [-1.4305115e-06, -1.3485584e+01],\n",
       "             [-1.4305115e-06, -1.3555519e+01],\n",
       "             [-4.7683716e-07, -1.4546761e+01]], dtype=float32)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_pred = training_loop.eval_model(tmp_inputs)\n",
    "print(f\"The prediction shape is {tmp_pred.shape}, num of tensor_tweets as rows\")\n",
    "print(\"Column 0 is the probability of a negative sentiment (class 0)\")\n",
    "print(\"Column 1 is the probability of a positive sentiment (class 1)\")\n",
    "print()\n",
    "print(\"View the prediction array\")\n",
    "tmp_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neg log prob -10.0370\tPos log prob -0.0000\t is positive? True\t actual 1\n",
      "Neg log prob -7.4305\tPos log prob -0.0006\t is positive? True\t actual 1\n",
      "Neg log prob -10.3180\tPos log prob -0.0000\t is positive? True\t actual 1\n",
      "Neg log prob -10.0709\tPos log prob -0.0000\t is positive? True\t actual 1\n",
      "Neg log prob -10.1035\tPos log prob -0.0000\t is positive? True\t actual 1\n",
      "Neg log prob -9.8433\tPos log prob -0.0001\t is positive? True\t actual 1\n",
      "Neg log prob -12.1626\tPos log prob -0.0000\t is positive? True\t actual 1\n",
      "Neg log prob -5.3618\tPos log prob -0.0047\t is positive? True\t actual 1\n",
      "Neg log prob -0.0000\tPos log prob -13.7152\t is positive? False\t actual 0\n",
      "Neg log prob 0.0000\tPos log prob -15.7014\t is positive? False\t actual 0\n",
      "Neg log prob 0.0000\tPos log prob -15.2493\t is positive? False\t actual 0\n",
      "Neg log prob 0.0000\tPos log prob -26.6470\t is positive? False\t actual 0\n",
      "Neg log prob -0.0004\tPos log prob -7.8900\t is positive? False\t actual 0\n",
      "Neg log prob -0.0000\tPos log prob -13.4856\t is positive? False\t actual 0\n",
      "Neg log prob -0.0000\tPos log prob -13.5555\t is positive? False\t actual 0\n",
      "Neg log prob -0.0000\tPos log prob -14.5468\t is positive? False\t actual 0\n"
     ]
    }
   ],
   "source": [
    "tmp_is_positive = tmp_pred[:,1] > tmp_pred[:,0]\n",
    "for i, p in enumerate(tmp_is_positive):\n",
    "    print(f\"Neg log prob {tmp_pred[i,0]:.4f}\\tPos log prob {tmp_pred[i,1]:.4f}\\t is positive? {p}\\t actual {tmp_targets[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array of booleans\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([ True,  True,  True,  True,  True,  True,  True,  True,\n",
       "             False, False, False, False, False, False, False, False],            dtype=bool)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array of integers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array of floats\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "             0.], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# View the array of booleans\n",
    "print(\"Array of booleans\")\n",
    "display(tmp_is_positive)\n",
    "\n",
    "# convert boolean to type int32\n",
    "# True is converted to 1\n",
    "# False is converted to 0\n",
    "tmp_is_positive_int = tmp_is_positive.astype(np.int32)\n",
    "\n",
    "\n",
    "# View the array of integers\n",
    "print(\"Array of integers\")\n",
    "display(tmp_is_positive_int)\n",
    "\n",
    "# convert boolean to type float32\n",
    "tmp_is_positive_float = tmp_is_positive.astype(np.float32)\n",
    "\n",
    "# View the array of floats\n",
    "print(\"Array of floats\")\n",
    "display(tmp_is_positive_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 2)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True == 1: True\n",
      "True == 2: False\n",
      "False == 0: True\n",
      "False == 2: False\n"
     ]
    }
   ],
   "source": [
    "print(f\"True == 1: {True == 1}\")\n",
    "print(f\"True == 2: {True == 2}\")\n",
    "print(f\"False == 0: {False == 0}\")\n",
    "print(f\"False == 2: {False == 2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 5.1. Computing the accuracity of the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(preds, y, y_weights):\n",
    "\n",
    "    is_pos =  preds[:, 1] > preds[:, 0] \n",
    "\n",
    "    is_pos_int = is_pos.astype(np.int32)\n",
    "    \n",
    "    correct = is_pos_int == y \n",
    "\n",
    "    sum_weights = np.sum(y_weights)\n",
    "    \n",
    "    correct_float = correct.astype(np.float32)\n",
    "    \n",
    "    weighted_correct_float = correct_float * y_weights\n",
    "    weighted_num_correct = np.sum(weighted_correct_float)\n",
    " \n",
    "    accuracy = weighted_num_correct / sum_weights\n",
    "\n",
    "    return accuracy, weighted_num_correct, sum_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's prediction accuracy on a single training batch is: 100.0%\n",
      "Weighted number of correct predictions 64.0; weighted number of total observations predicted 64\n"
     ]
    }
   ],
   "source": [
    "# test your function\n",
    "tmp_val_generator = val_generator(64)\n",
    "\n",
    "# get one batch\n",
    "tmp_batch = next(tmp_val_generator)\n",
    "\n",
    "# Position 0 has the model inputs (tweets as tensors)\n",
    "# position 1 has the targets (the actual labels)\n",
    "tmp_inputs, tmp_targets, tmp_example_weights = tmp_batch\n",
    "\n",
    "# feed the tweet tensors into the model to get a prediction\n",
    "tmp_pred = training_loop.eval_model(tmp_inputs)\n",
    "\n",
    "tmp_acc, tmp_num_correct, tmp_num_predictions = compute_accuracy(preds=tmp_pred, y=tmp_targets, y_weights=tmp_example_weights)\n",
    "\n",
    "print(f\"Model's prediction accuracy on a single training batch is: {100 * tmp_acc}%\")\n",
    "print(f\"Weighted number of correct predictions {tmp_num_correct}; weighted number of total observations predicted {tmp_num_predictions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 5.2. Testing Your Model on Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def test_model(generator, model):\n",
    "    \n",
    "    accuracy = 0.\n",
    "    total_num_correct = 0\n",
    "    total_num_pred = 0\n",
    "    \n",
    "    for batch in generator: \n",
    "        \n",
    "        inputs = batch[0]\n",
    "        \n",
    "        targets = batch[1]\n",
    "        \n",
    "        example_weight = batch[2]\n",
    "\n",
    "        pred = model(inputs)\n",
    "        \n",
    "        batch_accuracy, batch_num_correct, batch_num_pred = compute_accuracy(pred, targets, example_weight) \n",
    "\n",
    "        total_num_correct += batch_num_correct\n",
    "\n",
    "        total_num_pred += batch_num_pred\n",
    "\n",
    "    accuracy = total_num_correct / total_num_pred\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of your model on the validation set is 0.9940\n"
     ]
    }
   ],
   "source": [
    "model = training_loop.eval_model\n",
    "accuracy = test_model(test_generator(16), model)\n",
    "\n",
    "print(f'The accuracy of your model on the validation set is {accuracy:.4f}', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 6. Testing with our own input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(sentence):\n",
    "    inputs = np.array(tweet_to_tensor(sentence, vocab_dict=Vocab))\n",
    "    \n",
    "    inputs = inputs[None, :]  \n",
    "    \n",
    "    preds_probs = model(inputs)\n",
    "    \n",
    "    preds = int(preds_probs[0, 1] > preds_probs[0, 0])\n",
    "    \n",
    "    sentiment = \"negative\"\n",
    "    if preds == 1:\n",
    "        sentiment = 'positive'\n",
    "\n",
    "    return preds, sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment of the sentence \n",
      "***\n",
      "\"It's such a nice day, think i'll be taking Sid to Ramsgate fish and chips for lunch at Peter's fish factory and then the beach maybe\"\n",
      "***\n",
      "is positive.\n",
      "\n",
      "The sentiment of the sentence \n",
      "***\n",
      "\"I hated my day, it was the worst, I'm so sad.\"\n",
      "***\n",
      "is negative.\n"
     ]
    }
   ],
   "source": [
    "sentence = \"It's such a nice day, think i'll be taking Sid to Ramsgate fish and chips for lunch at Peter's fish factory and then the beach maybe\"\n",
    "tmp_pred, tmp_sentiment = predict(sentence)\n",
    "print(f\"The sentiment of the sentence \\n***\\n\\\"{sentence}\\\"\\n***\\nis {tmp_sentiment}.\")\n",
    "\n",
    "print()\n",
    "sentence = \"I hated my day, it was the worst, I'm so sad.\"\n",
    "tmp_pred, tmp_sentiment = predict(sentence)\n",
    "print(f\"The sentiment of the sentence \\n***\\n\\\"{sentence}\\\"\\n***\\nis {tmp_sentiment}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
